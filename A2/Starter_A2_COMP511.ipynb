{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_l0gc5Kl8oF"
      },
      "source": [
        "# Assignment 2\n",
        "**COMP 511: Network Science**\n",
        "\n",
        "**Due on February 6th 2026**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdqAqjTGl8oI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import Counter, defaultdict\n",
        "import gdown\n",
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMGyX3jtl8oJ"
      },
      "source": [
        "## 1. Centrality Measures [20%]\n",
        "\n",
        "Select 3 (or more) centrality measures and find the top 5 most important nodes in the [Enron dataset](https://www.cs.cornell.edu/~arb/data/pvc-email-Enron/). Who are the top ranked people?\n",
        "\n",
        "*   aggregate all emails sent at different times into a static snapshot with an edge weight showing how many emails in total have been send from one node to the other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3PcjYf9l8oJ"
      },
      "outputs": [],
      "source": [
        "# google drive file ID\n",
        "file_id = \"1hbpdnqvTRqPZ4MvzxeKLLkifdXZhtQsn\"\n",
        "\n",
        "output_file = \"email-Enron.tar.gz\"\n",
        "\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file, quiet=False)\n",
        "\n",
        "# Extract the .tar.gz file\n",
        "try:\n",
        "    with tarfile.open(output_file, \"r:gz\") as tar:\n",
        "        tar.extractall()  # extract into \"email-Enron\" folder\n",
        "        print(\"Extraction completed successfully.\")\n",
        "except tarfile.ReadError as e:\n",
        "    print(\"Error reading the tar.gz file:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUwaUStsl8oJ"
      },
      "outputs": [],
      "source": [
        "data_folder = 'email-Enron'\n",
        "addresses_path = f\"{data_folder}/addresses-email-Enron.txt\"\n",
        "core_path = f\"{data_folder}/core-email-Enron.txt\"\n",
        "email_path = f\"{data_folder}/email-Enron.txt\"\n",
        "\n",
        "# read the addresses\n",
        "addresses_df = pd.read_csv(\n",
        "    addresses_path,\n",
        "    sep=r'\\s+',\n",
        "    header=None,\n",
        "    names=['ID', 'email_address'],\n",
        "    on_bad_lines='skip'\n",
        ")\n",
        "addresses_df.set_index('ID', inplace=True)\n",
        "\n",
        "# read core ids\n",
        "core_df = pd.read_csv(\n",
        "    core_path,\n",
        "    sep=r'\\s+',\n",
        "    header=None,\n",
        "    names=['ID']\n",
        ")\n",
        "\n",
        "# use set for efficiency\n",
        "core_ids = set(core_df['ID'].tolist())\n",
        "\n",
        "# read email edges\n",
        "edges_df = pd.read_csv(\n",
        "    email_path,\n",
        "    sep=r'\\s+',\n",
        "    header=None,\n",
        "    names=['sender', 'recipient', 'timestamp']\n",
        ")\n",
        "\n",
        "# we need to aggregate the edge weights to get a static snapshot that we can use\n",
        "agg_edges = edges_df.groupby(['sender', 'recipient']).size().reset_index(name='weight')\n",
        "\n",
        "# now lets make the static graph\n",
        "G_static = nx.DiGraph()\n",
        "\n",
        "# add weights between nodes based on agg_edges\n",
        "for row in agg_edges.itertuples(index=False):\n",
        "    G_static.add_edge(row.sender, row.recipient, weight=row.weight)\n",
        "\n",
        "# add the email as a node attribute so we can see important names later\n",
        "for node in G_static.nodes():\n",
        "  try:\n",
        "    G_static.nodes[node]['email_address'] = addresses_df.loc[node, 'email_address']\n",
        "  except KeyError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7plopTZ6l8oK"
      },
      "outputs": [],
      "source": [
        "# Write your code here to calculate centrality measures and find top 5 nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Z3IUyrl8oK"
      },
      "source": [
        "## 2. Graph Clustering [50%]\n",
        "\n",
        "Select two (or more) community detection or graph clustering algorithms, apply them on the following real world datasets and evaluate their performances:\n",
        "\n",
        "*   -- real-classic: [strike, karate, polblog, polbooks, football](http://www.reirab.com/Teaching/NS20/classic.tar.gz)\n",
        "*   -- real-node-label: [citeseer, cora, pubmed](https://github.com/tkipf/gcn/tree/master/gcn/data)\n",
        "\n",
        "Here, the goal is to use the classification labels as clustering labels, and see how well we can find those labels without using the feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQmrrCznl8oK"
      },
      "outputs": [],
      "source": [
        "# download classic graph data\n",
        "download_folder = \"/content/classic_graph_data/\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "file_ids = {\n",
        "    \"polbooks.gml\": \"1SzhpzmpEO9NSjEabjoZP16NzYN3s6RBn\",\n",
        "    \"polbooks.txt\": \"1yEzPMsGPUwrGepoxN4hlX3a54CvAPLj-\",\n",
        "    \"polblogs.gml\": \"10ptbBiMgm541aAr3Rd4svBFia6qjJOGV\",\n",
        "    \"polblogs.txt\": \"1un9r36q4qtuz9Kjv7MqVifMp0pH45Es2\",\n",
        "    \"karate.gml\": \"1zvpOcA_74qO0pp9o36xg2rdwx9pTkgW_\",\n",
        "    \"karate.txt\": \"1-i66II9DThEJruAWoslgb2Cc88R17jBK\",\n",
        "    \"strike.gml\": \"137lB6IHqkRzHYOeYBn-0n4pNIYN3tuvr\",\n",
        "    \"football.gml\": \"1ooEj5eBI7UM2ZtfexmaTxIFJt6qn43Uy\",\n",
        "    \"football.txt\": \"126uHBR218fWCBRJjORSmaaxoHUGfTbPp\",\n",
        "}\n",
        "\n",
        "for file_name, file_id in file_ids.items():\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    output_path = os.path.join(download_folder, file_name)\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "    print(f\"Downloaded: {file_name}\")\n",
        "\n",
        "# download labeled graph data\n",
        "download_folder = \"/content/labeled_graph_data/\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "file_ids = {\n",
        "    \"citeseer_node_labels\": \"1u51oSH8O46i3WnG_RqHrxcZnTURod20l\",\n",
        "    \"citeseer_edges\": \"1IceUHrm8fk51Fb_gLHmXg0kETcCZa2bw\",\n",
        "    \"cora_cites\": \"1AC_LOmycApD7_MtahW6WeaxSOzhPPp9J\",\n",
        "    \"cora_content\": \"1mQ9dWt_8jJr0pJFiyTsKS2hGEE85YPZQ\",\n",
        "    \"pubmed_node_labels\": \"1HUlVeZCjiO4GEQrQgH282VPC835mfPaO\",\n",
        "    \"pubmed_edges\": \"1lss1_snvRj8sx4Aoo-vS2bJt62j_3YPt\"\n",
        "}\n",
        "\n",
        "for file_name, file_id in file_ids.items():\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    output_path = os.path.join(download_folder, file_name)\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "    print(f\"Downloaded: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm57793pl8oK"
      },
      "outputs": [],
      "source": [
        "# nx.read_gml needs a simple graph so we have to manually remove duplicate edges\n",
        "def clean_gml_file(input_path, output_path, gml_filename):\n",
        "\n",
        "    with open(input_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    seen_edges = set()\n",
        "    new_lines = []\n",
        "    inside_edge_block = False\n",
        "\n",
        "    # edge block\n",
        "    buffer_block = []\n",
        "    edge_counter = 0\n",
        "\n",
        "    for line in lines:\n",
        "        stripped_line = line.rstrip(\"\\n\\r\")\n",
        "\n",
        "        # detect start of an edge block\n",
        "        if stripped_line.strip().startswith(\"edge [\"):\n",
        "            inside_edge_block = True\n",
        "            buffer_block = [line]  # start buffering this edge block\n",
        "            continue\n",
        "\n",
        "        if inside_edge_block:\n",
        "            buffer_block.append(line)\n",
        "\n",
        "            if stripped_line.strip().startswith(\"]\"):\n",
        "                # end of the edge block\n",
        "                inside_edge_block = False\n",
        "                edge_counter += 1\n",
        "\n",
        "                # combine block into one string for parsing\n",
        "                block_text = \"\".join(buffer_block)\n",
        "\n",
        "                #extract source & target from any line within the block\n",
        "                match_source = re.search(r'\\bsource\\s+(\\d+)', block_text)\n",
        "                match_target = re.search(r'\\btarget\\s+(\\d+)', block_text)\n",
        "\n",
        "                if match_source and match_target:\n",
        "                    source = int(match_source.group(1))\n",
        "                    target = int(match_target.group(1))\n",
        "\n",
        "                    if source != target:  # self-loops\n",
        "                        edge_tuple = tuple(sorted((source, target)))\n",
        "\n",
        "                        if edge_tuple not in seen_edges:\n",
        "                            seen_edges.add(edge_tuple)\n",
        "                            new_lines.extend(buffer_block)\n",
        "\n",
        "                # reset buffer\n",
        "                buffer_block = []\n",
        "\n",
        "        else:\n",
        "            # non-edge lines go directly to output\n",
        "            new_lines.append(line)\n",
        "\n",
        "    # save the cleaned gml file\n",
        "    with open(output_path, 'w') as out:\n",
        "        out.writelines(new_lines)\n",
        "\n",
        "    print(f\"[CLEAN] Removed duplicates. Cleaned file saved to: {output_path}\")\n",
        "\n",
        "classic_graph_folder = \"/content/classic_graph_data/\"\n",
        "classic_cleaned_folder = \"/content/classic_graph_data_cleaned/\"\n",
        "os.makedirs(classic_cleaned_folder, exist_ok=True)\n",
        "\n",
        "classic_gml_files = [f for f in os.listdir(classic_graph_folder) if f.endswith(\".gml\")]\n",
        "classic_graphs = {}\n",
        "\n",
        "print(\"Loading 'classic' graphs. Note: These are often referred to as unlabeled, but most (except strike) do have ground truth labels available.\")\n",
        "\n",
        "for gml_file in classic_gml_files:\n",
        "\n",
        "    #clean all the gml files first\n",
        "    input_path = os.path.join(classic_graph_folder, gml_file)\n",
        "    output_path = os.path.join(classic_cleaned_folder, gml_file)\n",
        "    clean_gml_file(input_path, output_path, gml_file)\n",
        "\n",
        "    # load and apply some processing\n",
        "    try:\n",
        "        G = nx.read_gml(output_path)\n",
        "\n",
        "        # undirected simple graph\n",
        "        if isinstance(G, nx.MultiDiGraph) or isinstance(G, nx.DiGraph):\n",
        "            G = nx.Graph(G)\n",
        "\n",
        "        # remove self-loops (double checking)\n",
        "        G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "        # Rename 'value' to 'label' if present\n",
        "        for node, data in G.nodes(data=True):\n",
        "            if 'value' in data:\n",
        "                G.nodes[node]['label'] = data['value']\n",
        "\n",
        "        classic_graphs[gml_file] = G\n",
        "        print(f\"[LOADED] {gml_file} -> {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
        "    except nx.NetworkXError as e:\n",
        "        print(f\"[ERROR] Could not load {gml_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCZRHqiSl8oK"
      },
      "outputs": [],
      "source": [
        "# now we load the labeled graphs\n",
        "labeled_graph_folder = \"/content/labeled_graph_data/\"\n",
        "labeled_cleaned_folder = \"/content/labeled_graph_data_cleaned/\"\n",
        "os.makedirs(labeled_cleaned_folder, exist_ok=True)\n",
        "\n",
        "graph_folder = \"labeled_graph_data\"\n",
        "edges_file = os.path.join(graph_folder, \"citeseer_edges\")\n",
        "labels_file = os.path.join(graph_folder, \"citeseer_node_labels\")\n",
        "\n",
        "def load_citeseer_graph(edges_file, labels_file):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    with open(edges_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            src, dst, *_ = line.strip().split(\",\")\n",
        "            G.add_edge(int(src), int(dst))\n",
        "\n",
        "    labels = {}\n",
        "    with open(labels_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            node, label = line.strip().split(\",\")\n",
        "            node, label = int(node), int(label)\n",
        "            G.nodes[node][\"label\"] = label\n",
        "            labels[node] = label\n",
        "\n",
        "    return G, labels\n",
        "\n",
        "G_citeseer, citeseer_labels = load_citeseer_graph(edges_file, labels_file)\n",
        "print(f\"Loaded CiteSeer graph with {G_citeseer.number_of_nodes()} nodes and {G_citeseer.number_of_edges()} edges.\")\n",
        "\n",
        "# Now we do the cora graph\n",
        "edges_file = os.path.join(graph_folder, \"cora_cites\")\n",
        "content_file = os.path.join(graph_folder, \"cora_content\")\n",
        "\n",
        "def load_cora_graph(edges_file, content_file):\n",
        "    G = nx.Graph()\n",
        "    node_features = {}\n",
        "    node_labels = {}\n",
        "\n",
        "    with open(content_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")  # tab separator\n",
        "            node_id = int(parts[0])  # first column is node ID\n",
        "            features = np.array([int(x) for x in parts[1:-1]])  # middle columns are features\n",
        "            label = parts[-1]  # last column is class label\n",
        "\n",
        "            G.add_node(node_id, features=features, label=label)\n",
        "            node_features[node_id] = features\n",
        "            node_labels[node_id] = label\n",
        "\n",
        "    with open(edges_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            src, dst = map(int, line.strip().split(\"\\t\"))\n",
        "            G.add_edge(src, dst)\n",
        "\n",
        "    return G, node_features, node_labels\n",
        "\n",
        "G_cora, cora_features, cora_labels = load_cora_graph(edges_file, content_file)\n",
        "print(f\"Loaded Cora graph with {G_cora.number_of_nodes()} nodes and {G_cora.number_of_edges()} edges.\")\n",
        "\n",
        "# Now pubmed\n",
        "edges_file = os.path.join(graph_folder, \"pubmed_edges\")\n",
        "labels_file = os.path.join(graph_folder, \"pubmed_node_labels\")\n",
        "\n",
        "def load_pubmed_graph(edges_file, labels_file):\n",
        "    G = nx.Graph()\n",
        "\n",
        "    with open(edges_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            src, dst = map(int, line.strip().split(\",\"))\n",
        "            G.add_edge(src, dst)\n",
        "\n",
        "    labels = {}\n",
        "    with open(labels_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            node, label = line.strip().split(\",\")\n",
        "            node, label = int(node), int(label)\n",
        "            G.nodes[node][\"label\"] = label\n",
        "            labels[node] = label\n",
        "\n",
        "    return G, labels\n",
        "\n",
        "G_pubmed, pubmed_labels = load_pubmed_graph(edges_file, labels_file)\n",
        "print(f\"Loaded PubMed graph with {G_pubmed.number_of_nodes()} nodes and {G_pubmed.number_of_edges()} edges.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVM073GUl8oL"
      },
      "source": [
        "### 2(a) Algorithmic Complexity [10%]\n",
        "Derive and report the complexity of the chosen algorithms (in your report, not here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cko9X8Xjl8oL"
      },
      "source": [
        "### 2(b) Qualitative Evaluation [20%]\n",
        "Visualize the obtained clusters using [Gephi](https://gephi.org) or any other graph visualization tool. Report the visualizations and comment your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wyBFvEgl8oL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My51aNIFl8oL"
      },
      "source": [
        "### 2(c) Quantitative Evaluation [20%]\n",
        "Evaluate the quality of the clusters using label independent (topology only) metrics for both sets of graphs and label dependent metrics for graphs with labels.\n",
        "\n",
        "*   \\tipo{topology based metrics: Modularity and Conductance}\n",
        "*   \\tipo{label dependent metrics: NMI and ARI (report for all datasets that have labels, this should be all but **strike**)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvDIcEnwl8oL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZh8ikmnl8oL"
      },
      "source": [
        "## 3. Evaluation using synthetic datasets: [30%]\n",
        "1. Create a set of synthetic dataset using [LFR](https://networkx.github.io/documentation/stable/reference/generated/networkx.generators.community.LFR_benchmark_graph.html).  [10%]\n",
        "    *   The common practice is to sample for varying values of $\\mu$ which controls how well separated are the communities, i.e. generating synthetic graphs with $\\mu=.1$ to $\\mu=.9$, reporting average performance for 10 realizations at each difficulty level (90 total), see https://arxiv.org/abs/0805.4770, Fig 5 for example. N = 1000, or 5000 are common settings. For this experiments, you can use $\\mu=.5$, n=1000, tau1 = 3, tau2 = 1.5, average degree=5, min community=20.\n",
        "2. Evaluate the chosen algorithms (quantitatively using ARI/NMI) in the previous questions on this synthetic datasets and report your results [10%]\n",
        "3. Compute the Average Modularity and Conductance measures for each of the following sets of datasets (i) Real-Classic, (ii) Real-Node-Label and (iii) Synthetic datasets, compare them and report your observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVEtkQwHl8oL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ondsa6JUl8oL"
      },
      "source": [
        "## 4. [Bonus] Compare the results with an algorithm published/proposed in the last 4 years and report your observations [10%]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcgmraMCl8oL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMA_EBGLl8oM"
      },
      "source": [
        "## 5. [Bonus] Compare the results on 3 more real world datasets not included in the assignment and report your observations [10%]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVlV5ihDl8oM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
