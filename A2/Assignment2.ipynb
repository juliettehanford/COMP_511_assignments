{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_l0gc5Kl8oF"
   },
   "source": [
    "# Assignment 2\n",
    "**COMP 511: Network Science**\n",
    "\n",
    "**Due on February 6th 2026**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wdqAqjTGl8oI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter, defaultdict\n",
    "import gdown\n",
    "import tarfile\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMGyX3jtl8oJ"
   },
   "source": [
    "## 1. Centrality Measures [20%]\n",
    "\n",
    "Select 3 (or more) centrality measures and find the top 5 most important nodes in the [Enron dataset](https://www.cs.cornell.edu/~arb/data/pvc-email-Enron/). Who are the top ranked people?\n",
    "\n",
    "*   aggregate all emails sent at different times into a static snapshot with an edge weight showing how many emails in total have been send from one node to the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i3PcjYf9l8oJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hbpdnqvTRqPZ4MvzxeKLLkifdXZhtQsn\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/email-Enron.tar.gz\n",
      "100%|██████████| 1.66M/1.66M [00:00<00:00, 11.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# google drive file ID\n",
    "file_id = \"1hbpdnqvTRqPZ4MvzxeKLLkifdXZhtQsn\"\n",
    "\n",
    "output_file = \"email-Enron.tar.gz\"\n",
    "\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file, quiet=False)\n",
    "\n",
    "# Extract the .tar.gz file\n",
    "try:\n",
    "    with tarfile.open(output_file, \"r:gz\") as tar:\n",
    "        tar.extractall()  # extract into \"email-Enron\" folder\n",
    "        print(\"Extraction completed successfully.\")\n",
    "except tarfile.ReadError as e:\n",
    "    print(\"Error reading the tar.gz file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lUwaUStsl8oJ"
   },
   "outputs": [],
   "source": [
    "data_folder = 'email-Enron'\n",
    "addresses_path = f\"{data_folder}/addresses-email-Enron.txt\"\n",
    "core_path = f\"{data_folder}/core-email-Enron.txt\"\n",
    "email_path = f\"{data_folder}/email-Enron.txt\"\n",
    "\n",
    "# read the addresses\n",
    "addresses_df = pd.read_csv(\n",
    "    addresses_path,\n",
    "    sep=r'\\s+',\n",
    "    header=None,\n",
    "    names=['ID', 'email_address'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "addresses_df.set_index('ID', inplace=True)\n",
    "\n",
    "# read core ids\n",
    "core_df = pd.read_csv(\n",
    "    core_path,\n",
    "    sep=r'\\s+',\n",
    "    header=None,\n",
    "    names=['ID']\n",
    ")\n",
    "\n",
    "# use set for efficiency\n",
    "core_ids = set(core_df['ID'].tolist())\n",
    "\n",
    "# read email edges\n",
    "edges_df = pd.read_csv(\n",
    "    email_path,\n",
    "    sep=r'\\s+',\n",
    "    header=None,\n",
    "    names=['sender', 'recipient', 'timestamp']\n",
    ")\n",
    "\n",
    "# we need to aggregate the edge weights to get a static snapshot that we can use\n",
    "agg_edges = edges_df.groupby(['sender', 'recipient']).size().reset_index(name='weight')\n",
    "\n",
    "# now lets make the static graph\n",
    "G_static = nx.DiGraph()\n",
    "\n",
    "# add weights between nodes based on agg_edges\n",
    "for row in agg_edges.itertuples(index=False):\n",
    "    G_static.add_edge(row.sender, row.recipient, weight=row.weight)\n",
    "\n",
    "# add the email as a node attribute so we can see important names later\n",
    "for node in G_static.nodes():\n",
    "  try:\n",
    "    G_static.nodes[node]['email_address'] = addresses_df.loc[node, 'email_address']\n",
    "  except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7plopTZ6l8oK"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m deg_cent = nx.degree_centrality(G_static)\n\u001b[32m      3\u001b[39m eig_cent = nx.eigenvector_centrality(G_static, max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m betw_cent = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbetweenness_centrality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# HITS: returns (hubs, authorities) dicts\u001b[39;00m\n\u001b[32m      7\u001b[39m hits_hubs, hits_auths = nx.hits(G_static, max_iter=\u001b[32m1000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 18:4\u001b[39m, in \u001b[36margmap_betweenness_centrality_14\u001b[39m\u001b[34m(G, k, normalized, weight, endpoints, seed, backend, **backend_kwargs)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/.venv/lib/python3.14/site-packages/networkx/utils/backends.py:551\u001b[39m, in \u001b[36m_dispatchable._call_if_no_backends_installed\u001b[39m\u001b[34m(self, backend, *args, **kwargs)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backends:\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    547\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not implemented by \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnetworkx\u001b[39m\u001b[33m'\u001b[39m\u001b[33m backend. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    548\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis function is included in NetworkX as an API to dispatch to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    549\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mother backends.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    550\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/.venv/lib/python3.14/site-packages/networkx/algorithms/centrality/betweenness.py:231\u001b[39m, in \u001b[36mbetweenness_centrality\u001b[39m\u001b[34m(G, k, normalized, weight, endpoints, seed)\u001b[39m\n\u001b[32m    229\u001b[39m         betweenness, _ = _accumulate_endpoints(betweenness, S, P, sigma, s)\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m         betweenness, _ = \u001b[43m_accumulate_basic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbetweenness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# rescaling\u001b[39;00m\n\u001b[32m    233\u001b[39m betweenness = _rescale(\n\u001b[32m    234\u001b[39m     betweenness,\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mlen\u001b[39m(G),\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m     sampled_nodes=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nodes,\n\u001b[32m    240\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/.venv/lib/python3.14/site-packages/networkx/algorithms/centrality/betweenness.py:464\u001b[39m, in \u001b[36m_accumulate_basic\u001b[39m\u001b[34m(betweenness, S, P, sigma, s)\u001b[39m\n\u001b[32m    462\u001b[39m         delta[v] += sigma[v] * coeff\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m w != s:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         betweenness[w] += delta[w]\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m betweenness, delta\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# compute centrality measures on the full directed graph\n",
    "deg_cent = nx.degree_centrality(G_static)\n",
    "eig_cent = nx.eigenvector_centrality(G_static, max_iter=1000)\n",
    "betw_cent = nx.betweenness_centrality(G_static)\n",
    "\n",
    "# HITS: returns (hubs, authorities) dicts\n",
    "hits_hubs, hits_auths = nx.hits(G_static, max_iter=1000)\n",
    "\n",
    "# helper to look up email from node id\n",
    "def node_to_email(nid):\n",
    "    if nid in addresses_df.index:\n",
    "        return addresses_df.loc[nid, 'email_address']\n",
    "    return f\"Unknown({nid})\"\n",
    "\n",
    "# display top 5 for each measure\n",
    "measures = {\n",
    "    'Degree': deg_cent,\n",
    "    'Eigenvector': eig_cent,\n",
    "    'Betweenness': betw_cent,\n",
    "    'HITS Hub': hits_hubs,\n",
    "    'HITS Authority': hits_auths,\n",
    "}\n",
    "\n",
    "for name, scores in measures.items():\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"\\nTop 5 by {name} Centrality:\")\n",
    "    for i, (nid, val) in enumerate(ranked, 1):\n",
    "        print(f\"  {i}. {node_to_email(nid)} — {val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0Z3IUyrl8oK"
   },
   "source": [
    "## 2. Graph Clustering [50%]\n",
    "\n",
    "Select two (or more) community detection or graph clustering algorithms, apply them on the following real world datasets and evaluate their performances:\n",
    "\n",
    "*   -- real-classic: [strike, karate, polblog, polbooks, football](http://www.reirab.com/Teaching/NS20/classic.tar.gz)\n",
    "*   -- real-node-label: [citeseer, cora, pubmed](https://github.com/tkipf/gcn/tree/master/gcn/data)\n",
    "\n",
    "Here, the goal is to use the classification labels as clustering labels, and see how well we can find those labels without using the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQmrrCznl8oK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SzhpzmpEO9NSjEabjoZP16NzYN3s6RBn\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/polbooks.gml\n",
      "100%|██████████| 26.3k/26.3k [00:00<00:00, 18.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: polbooks.gml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1yEzPMsGPUwrGepoxN4hlX3a54CvAPLj-\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/polbooks.txt\n",
      "100%|██████████| 638/638 [00:00<00:00, 1.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: polbooks.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10ptbBiMgm541aAr3Rd4svBFia6qjJOGV\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/polblogs.gml\n",
      "100%|██████████| 978k/978k [00:00<00:00, 6.89MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: polblogs.gml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1un9r36q4qtuz9Kjv7MqVifMp0pH45Es2\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/polblogs.txt\n",
      "100%|██████████| 763/763 [00:00<00:00, 1.04MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: polblogs.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zvpOcA_74qO0pp9o36xg2rdwx9pTkgW_\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/karate.gml\n",
      "100%|██████████| 4.31k/4.31k [00:00<00:00, 7.82MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: karate.gml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-i66II9DThEJruAWoslgb2Cc88R17jBK\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/karate.txt\n",
      "100%|██████████| 343/343 [00:00<00:00, 379kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: karate.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=137lB6IHqkRzHYOeYBn-0n4pNIYN3tuvr\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/strike.gml\n",
      "100%|██████████| 2.22k/2.22k [00:00<00:00, 3.28MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: strike.gml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ooEj5eBI7UM2ZtfexmaTxIFJt6qn43Uy\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/football.gml\n",
      "100%|██████████| 33.4k/33.4k [00:00<00:00, 4.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: football.gml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=126uHBR218fWCBRJjORSmaaxoHUGfTbPp\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/classic_graph_data/football.txt\n",
      "100%|██████████| 672/672 [00:00<00:00, 857kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: football.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1u51oSH8O46i3WnG_RqHrxcZnTURod20l\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/labeled_graph_data/citeseer_node_labels\n",
      "100%|██████████| 21.7k/21.7k [00:00<00:00, 3.45MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: citeseer_node_labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1IceUHrm8fk51Fb_gLHmXg0kETcCZa2bw\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/labeled_graph_data/citeseer_edges\n",
      "100%|██████████| 51.1k/51.1k [00:00<00:00, 139kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: citeseer_edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1AC_LOmycApD7_MtahW6WeaxSOzhPPp9J\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/labeled_graph_data/cora_cites\n",
      "100%|██████████| 69.9k/69.9k [00:00<00:00, 544kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: cora_cites\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mQ9dWt_8jJr0pJFiyTsKS2hGEE85YPZQ\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/labeled_graph_data/cora_content\n",
      "100%|██████████| 7.82M/7.82M [00:00<00:00, 13.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: cora_content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HUlVeZCjiO4GEQrQgH282VPC835mfPaO\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/labeled_graph_data/pubmed_node_labels\n",
      "100%|██████████| 208k/208k [00:00<00:00, 2.90MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: pubmed_node_labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lss1_snvRj8sx4Aoo-vS2bJt62j_3YPt\n",
      "To: /Users/barthelemyderivieres/Desktop/McGill/U4 sem 2/COMP511/A1/assignments/A2/labeled_graph_data/pubmed_edges\n",
      "100%|██████████| 761k/761k [00:00<00:00, 5.00MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: pubmed_edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_folder = \"classic_graph_data/\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "file_ids = {\n",
    "    \"polbooks.gml\": \"1SzhpzmpEO9NSjEabjoZP16NzYN3s6RBn\",\n",
    "    \"polbooks.txt\": \"1yEzPMsGPUwrGepoxN4hlX3a54CvAPLj-\",\n",
    "    \"polblogs.gml\": \"10ptbBiMgm541aAr3Rd4svBFia6qjJOGV\",\n",
    "    \"polblogs.txt\": \"1un9r36q4qtuz9Kjv7MqVifMp0pH45Es2\",\n",
    "    \"karate.gml\": \"1zvpOcA_74qO0pp9o36xg2rdwx9pTkgW_\",\n",
    "    \"karate.txt\": \"1-i66II9DThEJruAWoslgb2Cc88R17jBK\",\n",
    "    \"strike.gml\": \"137lB6IHqkRzHYOeYBn-0n4pNIYN3tuvr\",\n",
    "    \"football.gml\": \"1ooEj5eBI7UM2ZtfexmaTxIFJt6qn43Uy\",\n",
    "    \"football.txt\": \"126uHBR218fWCBRJjORSmaaxoHUGfTbPp\",\n",
    "}\n",
    "\n",
    "for file_name, file_id in file_ids.items():\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    output_path = os.path.join(download_folder, file_name)\n",
    "    gdown.download(url, output_path, quiet=False)\n",
    "    print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "download_folder = \"labeled_graph_data/\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "file_ids = {\n",
    "    \"citeseer_node_labels\": \"1u51oSH8O46i3WnG_RqHrxcZnTURod20l\",\n",
    "    \"citeseer_edges\": \"1IceUHrm8fk51Fb_gLHmXg0kETcCZa2bw\",\n",
    "    \"cora_cites\": \"1AC_LOmycApD7_MtahW6WeaxSOzhPPp9J\",\n",
    "    \"cora_content\": \"1mQ9dWt_8jJr0pJFiyTsKS2hGEE85YPZQ\",\n",
    "    \"pubmed_node_labels\": \"1HUlVeZCjiO4GEQrQgH282VPC835mfPaO\",\n",
    "    \"pubmed_edges\": \"1lss1_snvRj8sx4Aoo-vS2bJt62j_3YPt\"\n",
    "}\n",
    "\n",
    "for file_name, file_id in file_ids.items():\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    output_path = os.path.join(download_folder, file_name)\n",
    "    gdown.download(url, output_path, quiet=False)\n",
    "    print(f\"Downloaded: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vm57793pl8oK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'classic' graphs. Note: These are often referred to as unlabeled, but most (except strike) do have ground truth labels available.\n",
      "[CLEAN] Removed duplicates. Cleaned file saved to: classic_graph_data_cleaned/polblogs.gml\n",
      "[LOADED] polblogs.gml -> 1490 nodes, 16715 edges.\n",
      "[CLEAN] Removed duplicates. Cleaned file saved to: classic_graph_data_cleaned/strike.gml\n",
      "[LOADED] strike.gml -> 24 nodes, 38 edges.\n",
      "[CLEAN] Removed duplicates. Cleaned file saved to: classic_graph_data_cleaned/karate.gml\n",
      "[LOADED] karate.gml -> 34 nodes, 78 edges.\n",
      "[CLEAN] Removed duplicates. Cleaned file saved to: classic_graph_data_cleaned/football.gml\n",
      "[LOADED] football.gml -> 115 nodes, 613 edges.\n",
      "[CLEAN] Removed duplicates. Cleaned file saved to: classic_graph_data_cleaned/polbooks.gml\n",
      "[LOADED] polbooks.gml -> 105 nodes, 441 edges.\n"
     ]
    }
   ],
   "source": [
    "def clean_gml_file(input_path, output_path, gml_filename):\n",
    "\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    seen_edges = set()\n",
    "    new_lines = []\n",
    "    inside_edge_block = False\n",
    "    buffer_block = []\n",
    "    edge_counter = 0\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.rstrip(\"\\n\\r\")\n",
    "\n",
    "        if stripped_line.strip().startswith(\"edge [\"):\n",
    "            inside_edge_block = True\n",
    "            buffer_block = [line]  # start buffering this edge block\n",
    "            continue\n",
    "\n",
    "        if inside_edge_block:\n",
    "            buffer_block.append(line)\n",
    "\n",
    "            if stripped_line.strip().startswith(\"]\"):\n",
    "                inside_edge_block = False\n",
    "                edge_counter += 1\n",
    "\n",
    "                block_text = \"\".join(buffer_block)\n",
    "                match_source = re.search(r'\\bsource\\s+(\\d+)', block_text)\n",
    "                match_target = re.search(r'\\btarget\\s+(\\d+)', block_text)\n",
    "\n",
    "                if match_source and match_target:\n",
    "                    source = int(match_source.group(1))\n",
    "                    target = int(match_target.group(1))\n",
    "\n",
    "                    if source != target:\n",
    "                        edge_tuple = tuple(sorted((source, target)))\n",
    "\n",
    "                        if edge_tuple not in seen_edges:\n",
    "                            seen_edges.add(edge_tuple)\n",
    "                            new_lines.extend(buffer_block)\n",
    "                buffer_block = []\n",
    "\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "    with open(output_path, 'w') as out:\n",
    "        out.writelines(new_lines)\n",
    "\n",
    "classic_graph_folder = \"classic_graph_data/\"\n",
    "classic_cleaned_folder = \"classic_graph_data_cleaned/\"\n",
    "os.makedirs(classic_cleaned_folder, exist_ok=True)\n",
    "\n",
    "classic_gml_files = [f for f in os.listdir(classic_graph_folder) if f.endswith(\".gml\")]\n",
    "classic_graphs = {}\n",
    "\n",
    "for gml_file in classic_gml_files:\n",
    "    input_path = os.path.join(classic_graph_folder, gml_file)\n",
    "    output_path = os.path.join(classic_cleaned_folder, gml_file)\n",
    "    clean_gml_file(input_path, output_path, gml_file)\n",
    "    try:\n",
    "        G = nx.read_gml(output_path)\n",
    "        if isinstance(G, nx.MultiDiGraph) or isinstance(G, nx.DiGraph):\n",
    "            G = nx.Graph(G)\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        for node, data in G.nodes(data=True):\n",
    "            if 'value' in data:\n",
    "                G.nodes[node]['label'] = data['value']\n",
    "\n",
    "        classic_graphs[gml_file] = G\n",
    "        print(f\"[LOADED] {gml_file} -> {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n",
    "    except nx.NetworkXError as e:\n",
    "        print(f\"[ERROR] Could not load {gml_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCZRHqiSl8oK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CiteSeer graph with 3264 nodes and 4536 edges.\n",
      "Loaded Cora graph with 2708 nodes and 5278 edges.\n",
      "Loaded PubMed graph with 19717 nodes and 44327 edges.\n"
     ]
    }
   ],
   "source": [
    "graph_folder = \"labeled_graph_data\"\n",
    "edges_file = os.path.join(graph_folder, \"citeseer_edges\")\n",
    "labels_file = os.path.join(graph_folder, \"citeseer_node_labels\")\n",
    "\n",
    "def load_citeseer_graph(edges_file, labels_file):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    with open(edges_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            src, dst, *_ = line.strip().split(\",\")\n",
    "            G.add_edge(int(src), int(dst))\n",
    "\n",
    "    labels = {}\n",
    "    with open(labels_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            node, label = line.strip().split(\",\")\n",
    "            node, label = int(node), int(label)\n",
    "            G.nodes[node][\"label\"] = label\n",
    "            labels[node] = label\n",
    "\n",
    "    return G, labels\n",
    "\n",
    "G_citeseer, citeseer_labels = load_citeseer_graph(edges_file, labels_file)\n",
    "print(f\"Loaded CiteSeer graph with {G_citeseer.number_of_nodes()} nodes and {G_citeseer.number_of_edges()} edges.\")\n",
    "\n",
    "edges_file = os.path.join(graph_folder, \"cora_cites\")\n",
    "content_file = os.path.join(graph_folder, \"cora_content\")\n",
    "\n",
    "def load_cora_graph(edges_file, content_file):\n",
    "    G = nx.Graph()\n",
    "    node_features = {}\n",
    "    node_labels = {}\n",
    "\n",
    "    with open(content_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\") \n",
    "            node_id = int(parts[0])\n",
    "            features = np.array([int(x) for x in parts[1:-1]]) \n",
    "            label = parts[-1]  \n",
    "\n",
    "            G.add_node(node_id, features=features, label=label)\n",
    "            node_features[node_id] = features\n",
    "            node_labels[node_id] = label\n",
    "\n",
    "    with open(edges_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            src, dst = map(int, line.strip().split(\"\\t\"))\n",
    "            G.add_edge(src, dst)\n",
    "\n",
    "    return G, node_features, node_labels\n",
    "\n",
    "G_cora, cora_features, cora_labels = load_cora_graph(edges_file, content_file)\n",
    "print(f\"Loaded Cora graph with {G_cora.number_of_nodes()} nodes and {G_cora.number_of_edges()} edges.\")\n",
    "\n",
    "\n",
    "edges_file = os.path.join(graph_folder, \"pubmed_edges\")\n",
    "labels_file = os.path.join(graph_folder, \"pubmed_node_labels\")\n",
    "\n",
    "def load_pubmed_graph(edges_file, labels_file):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    with open(edges_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            src, dst = map(int, line.strip().split(\",\"))\n",
    "            G.add_edge(src, dst)\n",
    "\n",
    "    labels = {}\n",
    "    with open(labels_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            node, label = line.strip().split(\",\")\n",
    "            node, label = int(node), int(label)\n",
    "            G.nodes[node][\"label\"] = label\n",
    "            labels[node] = label\n",
    "\n",
    "    return G, labels\n",
    "\n",
    "G_pubmed, pubmed_labels = load_pubmed_graph(edges_file, labels_file)\n",
    "print(f\"Loaded PubMed graph with {G_pubmed.number_of_nodes()} nodes and {G_pubmed.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVM073GUl8oL"
   },
   "source": [
    "### 2(a) Algorithmic Complexity [10%]\n",
    "Derive and report the complexity of the chosen algorithms (in your report, not here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cko9X8Xjl8oL"
   },
   "source": [
    "### 2(b) Qualitative Evaluation [20%]\n",
    "Visualize the obtained clusters using [Gephi](https://gephi.org) or any other graph visualization tool. Report the visualizations and comment your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wyBFvEgl8oL"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "import infomap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def spectral_partition(G, n_groups):\n",
    "    L = nx.laplacian_matrix(G).astype(float)\n",
    "    _, vecs = eigsh(L, k=n_groups, which='SM')\n",
    "    pred = KMeans(n_clusters=n_groups, n_init=10, random_state=0).fit_predict(vecs)\n",
    "    return dict(zip(G.nodes(), pred))\n",
    "\n",
    "def infomap_partition(G):\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {n: i for i, n in enumerate(node_list)}\n",
    "\n",
    "    im = infomap.Infomap(silent=True, two_level=True)\n",
    "    for u, v in G.edges():\n",
    "        im.add_link(node_to_idx[u], node_to_idx[v])\n",
    "    im.run()\n",
    "\n",
    "    partition = {}\n",
    "    for node in im.tree:\n",
    "        if node.is_leaf:\n",
    "            partition[node_list[node.node_id]] = node.module_id\n",
    "    return partition\n",
    "\n",
    "def louvain_partition(G):\n",
    "    return {n: ci for ci, comm in enumerate(louvain_communities(G)) for n in comm}\n",
    "\n",
    "graph_collection = {}\n",
    "\n",
    "for fname, g in classic_graphs.items():\n",
    "    tag = fname.split('.')[0]\n",
    "    gt = nx.get_node_attributes(g, 'label')\n",
    "    k = len(set(gt.values())) if gt else 3\n",
    "    graph_collection[tag] = {'G': g, 'gt': gt or None, 'k': max(k, 2)}\n",
    "\n",
    "graph_collection['citeseer'] = {'G': G_citeseer, 'gt': citeseer_labels, 'k': len(set(citeseer_labels.values()))}\n",
    "graph_collection['cora'] = {'G': G_cora, 'gt': cora_labels, 'k': len(set(cora_labels.values()))}\n",
    "graph_collection['pubmed'] = {'G': G_pubmed, 'gt': pubmed_labels, 'k': len(set(pubmed_labels.values()))}\n",
    "\n",
    "methods = [\n",
    "    ('Spectral', lambda g, k: spectral_partition(\n",
    "        g if nx.is_connected(g) else g.subgraph(max(nx.connected_components(g), key=len)).copy(), k)),\n",
    "    ('Infomap', lambda g, k: infomap_partition(g)),\n",
    "    ('Louvain', lambda g, k: louvain_partition(g)),\n",
    "]\n",
    "cluster_output = {}\n",
    "\n",
    "os.makedirs('gephi_export', exist_ok=True)\n",
    "\n",
    "for ds, info in graph_collection.items():\n",
    "    G = info['G']\n",
    "    n = G.number_of_nodes()\n",
    "    cluster_output[ds] = {}\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "    print(f\"\\n{'='*55}\\n{ds}: {n} nodes, {G.number_of_edges()} edges\\n\")\n",
    "\n",
    "    for mname, mfunc in methods:\n",
    "        part = mfunc(G, info['k'])\n",
    "        part = {nd: part.get(nd, -1) for nd in G.nodes()}\n",
    "        cluster_output[ds][mname] = part\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        node_cols = [part[nd] for nd in G.nodes()]\n",
    "        nx.draw(G, pos, node_color=node_cols, cmap=plt.cm.Set3,\n",
    "                node_size=max(5, 300 // (1 + n // 50)),\n",
    "                with_labels=n < 50, font_size=5,\n",
    "                edge_color='#cccccc', width=0.3, ax=ax)\n",
    "        ax.set_title(f'{ds} — {mname}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        Gx = nx.Graph()\n",
    "        Gx.add_nodes_from(G.nodes())\n",
    "        Gx.add_edges_from(G.edges())\n",
    "        deg = dict(G.degree())\n",
    "        deg_max = max(deg.values(), default=1)\n",
    "        for nd in Gx.nodes():\n",
    "            Gx.nodes[nd]['cluster'] = int(part.get(nd, -1))\n",
    "            Gx.nodes[nd]['size'] = round(5 + 20 * deg[nd] / deg_max, 2)\n",
    "\n",
    "        gml_path = f\"gephi_export/{ds}_{mname.lower().replace(' ', '_').replace('.', '')}.gml\"\n",
    "        nx.write_gml(Gx, gml_path)\n",
    "        print(f'  {mname} -> {gml_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My51aNIFl8oL"
   },
   "source": [
    "### 2(c) Quantitative Evaluation [20%]\n",
    "Evaluate the quality of the clusters using label independent (topology only) metrics for both sets of graphs and label dependent metrics for graphs with labels.\n",
    "\n",
    "*   \\tipo{topology based metrics: Modularity and Conductance}\n",
    "*   \\tipo{label dependent metrics: NMI and ARI (report for all datasets that have labels, this should be all but **strike**)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvDIcEnwl8oL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "def modularity_score(G, partition):\n",
    "    groups = defaultdict(set)\n",
    "    for nd, cid in partition.items():\n",
    "        groups[cid].add(nd)\n",
    "    return nx.community.modularity(G, groups.values())\n",
    "\n",
    "def avg_conductance(G, partition):\n",
    "    groups = defaultdict(set)\n",
    "    for nd, cid in partition.items():\n",
    "        groups[cid].add(nd)\n",
    "\n",
    "    total_degree = sum(deg for _, deg in G.degree())\n",
    "    vals = []\n",
    "    for S in groups.values():\n",
    "        if len(S) in (0, G.number_of_nodes()):\n",
    "            continue\n",
    "        cut = nx.cut_size(G, S)\n",
    "        vol = sum(deg for _, deg in G.degree(S))\n",
    "        vol_bar = total_degree - vol\n",
    "        if min(vol, vol_bar) > 0:\n",
    "            vals.append(cut / min(vol, vol_bar))\n",
    "    return float(np.mean(vals)) if vals else 0.0\n",
    "eval_rows = []\n",
    "\n",
    "for ds, info in graph_collection.items():\n",
    "    G = info['G']\n",
    "    gt = info['gt']\n",
    "\n",
    "    for mname, partition in cluster_output[ds].items():\n",
    "        mod = modularity_score(G, partition)\n",
    "        cond = avg_conductance(G, partition)\n",
    "\n",
    "        row = {'Dataset': ds, 'Algorithm': mname,\n",
    "               'Modularity': round(mod, 4), 'Conductance': round(cond, 4),\n",
    "               'NMI': '—', 'ARI': '—'}\n",
    "        if gt:\n",
    "            shared = [n for n in partition if n in gt]\n",
    "            if shared:\n",
    "                y_pred = [partition[n] for n in shared]\n",
    "                y_true = [gt[n] for n in shared]\n",
    "                row['NMI'] = round(normalized_mutual_info_score(y_true, y_pred), 4)\n",
    "                row['ARI'] = round(adjusted_rand_score(y_true, y_pred), 4)\n",
    "\n",
    "        eval_rows.append(row)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "print(eval_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZh8ikmnl8oL"
   },
   "source": [
    "## 3. Evaluation using synthetic datasets: [30%]\n",
    "1. Create a set of synthetic dataset using [LFR](https://networkx.github.io/documentation/stable/reference/generated/networkx.generators.community.LFR_benchmark_graph.html).  [10%]\n",
    "    *   The common practice is to sample for varying values of $\\mu$ which controls how well separated are the communities, i.e. generating synthetic graphs with $\\mu=.1$ to $\\mu=.9$, reporting average performance for 10 realizations at each difficulty level (90 total), see https://arxiv.org/abs/0805.4770, Fig 5 for example. N = 1000, or 5000 are common settings. For this experiments, you can use $\\mu=.5$, n=1000, tau1 = 3, tau2 = 1.5, average degree=5, min community=20.\n",
    "2. Evaluate the chosen algorithms (quantitatively using ARI/NMI) in the previous questions on this synthetic datasets and report your results [10%]\n",
    "3. Compute the Average Modularity and Conductance measures for each of the following sets of datasets (i) Real-Classic, (ii) Real-Node-Label and (iii) Synthetic datasets, compare them and report your observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVEtkQwHl8oL"
   },
   "outputs": [],
   "source": [
    "mu_range = np.round(np.linspace(0.1, 0.9, 9), 2)\n",
    "n_realizations = 10\n",
    "\n",
    "lfr_graphs = []\n",
    "for mu in mu_range:\n",
    "    for r in range(n_realizations):\n",
    "        try:\n",
    "            G_lfr = nx.generators.community.LFR_benchmark_graph(\n",
    "                n=1000, tau1=3, tau2=1.5, mu=mu,\n",
    "                average_degree=5, max_degree=100,\n",
    "                min_community=20, max_community=100,\n",
    "                seed=r * 100 + int(mu * 100),\n",
    "                tol=1e-3, max_iters=1000\n",
    "            )\n",
    "            gt = {nd: min(G_lfr.nodes[nd]['community']) for nd in G_lfr.nodes()}\n",
    "            lfr_graphs.append((mu, r, G_lfr, gt))\n",
    "            print(f\"  LFR mu={mu:.1f} run {r+1}/{n_realizations} OK\")\n",
    "        except Exception as exc:\n",
    "            print(f\"  LFR mu={mu:.1f} run {r+1}/{n_realizations} FAILED: {exc}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(lfr_graphs)} LFR graphs\")\n",
    "synth_rows = []\n",
    "\n",
    "for mu, r, G_lfr, gt in lfr_graphs:\n",
    "    n_comms = max(len(set(gt.values())), 2)\n",
    "    if nx.is_connected(G_lfr):\n",
    "        G_conn = G_lfr\n",
    "    else:\n",
    "        G_conn = G_lfr.subgraph(max(nx.connected_components(G_lfr), key=len)).copy()\n",
    "\n",
    "    algo_results = {\n",
    "        'Spectral': spectral_partition(G_conn, n_comms),\n",
    "        'Infomap': infomap_partition(G_lfr),\n",
    "        'Louvain': louvain_partition(G_lfr),\n",
    "    }\n",
    "\n",
    "    for aname, part in algo_results.items():\n",
    "        part = {nd: part.get(nd, -1) for nd in G_lfr.nodes()}\n",
    "\n",
    "        mod = modularity_score(G_lfr, part)\n",
    "        cond = avg_conductance(G_lfr, part)\n",
    "\n",
    "        nodes_with_gt = [n for n in part if n in gt]\n",
    "        y_pred = [part[n] for n in nodes_with_gt]\n",
    "        y_true = [gt[n] for n in nodes_with_gt]\n",
    "        nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "        ari = adjusted_rand_score(y_true, y_pred)\n",
    "\n",
    "        synth_rows.append({\n",
    "            'mu': mu, 'run': r, 'Algorithm': aname,\n",
    "            'Modularity': mod, 'Conductance': cond, 'NMI': nmi, 'ARI': ari\n",
    "        })\n",
    "\n",
    "    print(f\"  Evaluated mu={mu:.1f} run {r+1}\")\n",
    "\n",
    "synth_df = pd.DataFrame(synth_rows)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for ax, metric in zip(axes.ravel(), ['NMI', 'ARI', 'Modularity', 'Conductance']):\n",
    "    for algo in ['Spectral', 'Infomap', 'Louvain']:\n",
    "        subset = synth_df[synth_df['Algorithm'] == algo]\n",
    "        grouped = subset.groupby('mu')[metric]\n",
    "        mu_means = grouped.mean()\n",
    "        mu_stds = grouped.std()\n",
    "        ax.errorbar(mu_means.index, mu_means.values, yerr=mu_stds.values,\n",
    "                    label=algo, marker='o', capsize=4)\n",
    "    ax.set_xlabel('μ (mixing parameter)')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} vs μ')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSynthetic Dataset: Mean Performance per μ per Algorithm\\n\")\n",
    "pivot = synth_df.groupby(['Algorithm', 'mu'])[['NMI', 'ARI', 'Modularity', 'Conductance']].mean()\n",
    "print(pivot.round(4).to_string())\n",
    "\n",
    "print(\"\\nOverall Average across all μ values\\n\")\n",
    "overall = synth_df.groupby('Algorithm')[['NMI', 'ARI', 'Modularity', 'Conductance']].mean()\n",
    "print(overall.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_names = {fname.split('.')[0] for fname in classic_graphs.keys()}\n",
    "labeled_names = {'citeseer', 'cora', 'pubmed'}\n",
    "\n",
    "rc_mask = eval_df['Dataset'].isin(classic_names)\n",
    "rl_mask = eval_df['Dataset'].isin(labeled_names)\n",
    "numeric_cols = ['Modularity', 'Conductance']\n",
    "avg_rc = eval_df.loc[rc_mask, numeric_cols].apply(pd.to_numeric, errors='coerce').mean()\n",
    "avg_rl = eval_df.loc[rl_mask, numeric_cols].apply(pd.to_numeric, errors='coerce').mean()\n",
    "avg_syn = synth_df[numeric_cols].mean()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Real-Classic': avg_rc,\n",
    "    'Real-Node-Label': avg_rl,\n",
    "    'Synthetic (LFR)': avg_syn,\n",
    "}).T\n",
    "\n",
    "print(\"Average Modularity & Conductance by Dataset Category (all)\\n\")\n",
    "print(comparison_df.round(4).to_string())\n",
    "print(\"\\n\\nPer-Algorithm Breakdown by Category\\n\")\n",
    "\n",
    "for algo in ['Spectral', 'Infomap', 'Louvain']:\n",
    "    rc_algo = eval_df.loc[rc_mask & (eval_df['Algorithm'] == algo), numeric_cols].apply(pd.to_numeric, errors='coerce').mean()\n",
    "    rl_algo = eval_df.loc[rl_mask & (eval_df['Algorithm'] == algo), numeric_cols].apply(pd.to_numeric, errors='coerce').mean()\n",
    "    syn_algo = synth_df.loc[synth_df['Algorithm'] == algo, numeric_cols].mean()\n",
    "\n",
    "    tbl = pd.DataFrame({\n",
    "        'Real-Classic': rc_algo,\n",
    "        'Real-Node-Label': rl_algo,\n",
    "        'Synthetic (LFR)': syn_algo,\n",
    "    }).T\n",
    "\n",
    "    print(f\"--- {algo} ---\")\n",
    "    print(tbl.round(4).to_string())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ondsa6JUl8oL"
   },
   "source": [
    "## 4. [Bonus] Compare the results with an algorithm published/proposed in the last 4 years and report your observations [10%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcgmraMCl8oL"
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import leidenalg\n",
    "\n",
    "def leiden_partition(G):\n",
    "    \"\"\"Community detection via the Leiden algorithm.\"\"\"\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {n: i for i, n in enumerate(node_list)}\n",
    "    g_ig = ig.Graph(n=len(node_list), directed=False)\n",
    "    g_ig.add_edges([(node_to_idx[u], node_to_idx[v]) for u, v in G.edges()])\n",
    "    part = leidenalg.find_partition(g_ig, leidenalg.ModularityVertexPartition)\n",
    "    return {node_list[i]: m for i, m in enumerate(part.membership)}\n",
    "print(\"=== Leiden vs Spectral / Infomap / Louvain on Real-World Datasets ===\\n\")\n",
    "\n",
    "leiden_rows = []\n",
    "for ds, info in graph_collection.items():\n",
    "    G = info['G']\n",
    "    gt = info['gt']\n",
    "    part = leiden_partition(G)\n",
    "    part = {nd: part.get(nd, -1) for nd in G.nodes()}\n",
    "\n",
    "    mod = modularity_score(G, part)\n",
    "    cond = avg_conductance(G, part)\n",
    "    row = {'Dataset': ds, 'Algorithm': 'Leiden',\n",
    "           'Modularity': round(mod, 4), 'Conductance': round(cond, 4),\n",
    "           'NMI': '—', 'ARI': '—'}\n",
    "    if gt:\n",
    "        shared = [n for n in part if n in gt]\n",
    "        if shared:\n",
    "            y_pred = [part[n] for n in shared]\n",
    "            y_true = [gt[n] for n in shared]\n",
    "            row['NMI'] = round(normalized_mutual_info_score(y_true, y_pred), 4)\n",
    "            row['ARI'] = round(adjusted_rand_score(y_true, y_pred), 4)\n",
    "    leiden_rows.append(row)\n",
    "\n",
    "leiden_real_df = pd.DataFrame(leiden_rows)\n",
    "combined_real = pd.concat([eval_df, leiden_real_df], ignore_index=True)\n",
    "print(combined_real.to_string(index=False))\n",
    "print(\"\\n\\nLeiden on LFR Synthetic Graphs\\n\")\n",
    "\n",
    "leiden_synth_rows = []\n",
    "for mu, r, G_lfr, gt in lfr_graphs:\n",
    "    part = leiden_partition(G_lfr)\n",
    "    part = {nd: part.get(nd, -1) for nd in G_lfr.nodes()}\n",
    "    mod = modularity_score(G_lfr, part)\n",
    "    cond = avg_conductance(G_lfr, part)\n",
    "    nodes_with_gt = [n for n in part if n in gt]\n",
    "    y_pred = [part[n] for n in nodes_with_gt]\n",
    "    y_true = [gt[n] for n in nodes_with_gt]\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    leiden_synth_rows.append({\n",
    "        'mu': mu, 'run': r, 'Algorithm': 'Leiden',\n",
    "        'Modularity': mod, 'Conductance': cond, 'NMI': nmi, 'ARI': ari\n",
    "    })\n",
    "\n",
    "leiden_synth_df = pd.DataFrame(leiden_synth_rows)\n",
    "all_synth_df = pd.concat([synth_df, leiden_synth_df], ignore_index=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for ax, metric in zip(axes.ravel(), ['NMI', 'ARI', 'Modularity', 'Conductance']):\n",
    "    for algo in ['Spectral', 'Infomap', 'Louvain', 'Leiden']:\n",
    "        subset = all_synth_df[all_synth_df['Algorithm'] == algo]\n",
    "        grouped = subset.groupby('mu')[metric]\n",
    "        mu_means = grouped.mean()\n",
    "        mu_stds = grouped.std()\n",
    "        ax.errorbar(mu_means.index, mu_means.values, yerr=mu_stds.values,\n",
    "                    label=algo, marker='o', capsize=4)\n",
    "    ax.set_xlabel('μ (mixing parameter)')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} vs μ (incl. Leiden)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nSynthetic: Overall Average (4 algos)\\n\")\n",
    "summary = all_synth_df.groupby('Algorithm')[['NMI', 'ARI', 'Modularity', 'Conductance']].mean()\n",
    "print(summary.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMA_EBGLl8oM"
   },
   "source": [
    "## 5. [Bonus] Compare the results on 3 more real world datasets not included in the assignment and report your observations [10%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVlV5ihDl8oM"
   },
   "outputs": [],
   "source": [
    "import requests, gzip, io\n",
    "\n",
    "bonus_graphs = {}\n",
    "G_lesmis = nx.les_miserables_graph()\n",
    "bonus_graphs['les_miserables'] = {'G': G_lesmis, 'gt': None,\n",
    "                                   'k': 6} \n",
    "print(f\"Les Misérables: {G_lesmis.number_of_nodes()} nodes, {G_lesmis.number_of_edges()} edges (no ground-truth labels)\")\n",
    "try:\n",
    "    r1 = requests.get(\"https://snap.stanford.edu/data/email-Eu-core.txt.gz\", timeout=30)\n",
    "    with gzip.open(io.BytesIO(r1.content), 'rt') as f:\n",
    "        G_email = nx.parse_edgelist(\n",
    "            [line for line in f if not line.startswith('#')], nodetype=int)\n",
    "\n",
    "    r2 = requests.get(\"https://snap.stanford.edu/data/email-Eu-core-department-labels.txt.gz\", timeout=30)\n",
    "    email_labels = {}\n",
    "    with gzip.open(io.BytesIO(r2.content), 'rt') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                email_labels[int(parts[0])] = int(parts[1])\n",
    "    for n, l in email_labels.items():\n",
    "        if n in G_email:\n",
    "            G_email.nodes[n]['label'] = l\n",
    "    bonus_graphs['email_eu_core'] = {\n",
    "        'G': G_email, 'gt': email_labels,\n",
    "        'k': len(set(email_labels.values()))}\n",
    "    print(f\"Email-Eu-core: {G_email.number_of_nodes()} nodes, {G_email.number_of_edges()} edges, \"\n",
    "          f\"{len(set(email_labels.values()))} departments\")\n",
    "except Exception as e:\n",
    "    print(f\"Email-Eu-core download failed: {e}\")\n",
    "\n",
    "try:\n",
    "    r3 = requests.get(\"https://snap.stanford.edu/data/facebook_combined.txt.gz\", timeout=30)\n",
    "    with gzip.open(io.BytesIO(r3.content), 'rt') as f:\n",
    "        G_fb = nx.parse_edgelist(\n",
    "            [line for line in f if not line.startswith('#')], nodetype=int)\n",
    "    bonus_graphs['facebook'] = {'G': G_fb, 'gt': None, 'k': 10}\n",
    "    print(f\"Facebook: {G_fb.number_of_nodes()} nodes, {G_fb.number_of_edges()} edges (no ground-truth labels)\")\n",
    "except Exception as e:\n",
    "    print(f\"Facebook download failed: {e}\")\n",
    "\n",
    "all_algos = [\n",
    "    ('Spectral', lambda g, k: spectral_partition(\n",
    "        g if nx.is_connected(g) else g.subgraph(max(nx.connected_components(g), key=len)).copy(), k)),\n",
    "    ('Infomap', lambda g, k: infomap_partition(g)),\n",
    "    ('Louvain', lambda g, k: louvain_partition(g)),\n",
    "    ('Leiden', lambda g, k: leiden_partition(g)),\n",
    "]\n",
    "\n",
    "bonus_rows = []\n",
    "for ds, info in bonus_graphs.items():\n",
    "    G = info['G']\n",
    "    gt = info['gt']\n",
    "    print(f\"\\nRunning algorithms on {ds} ...\")\n",
    "\n",
    "    for aname, afunc in all_algos:\n",
    "        part = afunc(G, info['k'])\n",
    "        part = {nd: part.get(nd, -1) for nd in G.nodes()}\n",
    "\n",
    "        mod = modularity_score(G, part)\n",
    "        cond = avg_conductance(G, part)\n",
    "        row = {'Dataset': ds, 'Algorithm': aname,\n",
    "               'Modularity': round(mod, 4), 'Conductance': round(cond, 4),\n",
    "               'NMI': '—', 'ARI': '—'}\n",
    "        if gt:\n",
    "            shared = [n for n in part if n in gt]\n",
    "            if shared:\n",
    "                y_pred = [part[n] for n in shared]\n",
    "                y_true = [gt[n] for n in shared]\n",
    "                row['NMI'] = round(normalized_mutual_info_score(y_true, y_pred), 4)\n",
    "                row['ARI'] = round(adjusted_rand_score(y_true, y_pred), 4)\n",
    "        bonus_rows.append(row)\n",
    "        print(f\"  {aname} done\")\n",
    "\n",
    "bonus_df = pd.DataFrame(bonus_rows)\n",
    "print(\"\\n=== Bonus Datasets: Full Evaluation ===\\n\")\n",
    "print(bonus_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
