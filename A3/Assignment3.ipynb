{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33373b06",
   "metadata": {},
   "source": [
    "# Assignment 3 \u2014 COMP 511\n",
    "\n",
    "**Q1**: Node Classification \u2014 Label Propagation \u00b7 Node2Vec \u00b7 GCN  \n",
    "**Q2**: Link Prediction \u2014 Common Neighbours \u00b7 Jaccard \u00b7 GCN encoder-decoder  \n",
    "\n",
    "Datasets: Cora \u00b7 CiteSeer \u00b7 PubMed \u00b7 ogbn-proteins  \n",
    "All results: mean \u00b1 std over 10 independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Imports \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from node2vec import Node2Vec as GensimNode2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.data import Data as PyGData\n",
    "\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else\n",
    "                       'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Accelerator: {DEVICE}')\n",
    "\n",
    "N_RUNS = 10\n",
    "\n",
    "def fix_seed(s):\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Dataset Loading \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Planetoid datasets auto-downloaded by torch_geometric (same automated\n",
    "# approach as A2's gdown downloads). Extracted as scipy/numpy arrays\n",
    "# to match the format expected by our custom LP and GCN implementations.\n",
    "\n",
    "pyg_catalog = {}\n",
    "for tag in ['Cora', 'CiteSeer', 'PubMed']:\n",
    "    pyg_catalog[tag.lower()] = Planetoid(root=f'data/{tag}', name=tag,\n",
    "                                          transform=T.NormalizeFeatures())\n",
    "    d = pyg_catalog[tag.lower()][0]\n",
    "    print(f\"{tag:10s}: {d.num_nodes:6d} nodes  {d.num_edges:6d} edges  \"\n",
    "          f\"{pyg_catalog[tag.lower()].num_features:4d} feats  \"\n",
    "          f\"{pyg_catalog[tag.lower()].num_classes} classes  \"\n",
    "          f\"tr={d.train_mask.sum().item()} va={d.val_mask.sum().item()} te={d.test_mask.sum().item()}\")\n",
    "\n",
    "# ogbn-proteins: auto-downloaded by OGB\n",
    "prot_ds     = PygNodePropPredDataset(name='ogbn-proteins', root='data/ogbn-proteins')\n",
    "prot_splits = prot_ds.get_idx_split()\n",
    "prot_data   = prot_ds[0]\n",
    "\n",
    "# 8-dim node features: mean of incident edge attributes (OGB standard approach)\n",
    "_ei  = prot_data.edge_index\n",
    "_ea  = prot_data.edge_attr.float()\n",
    "_x   = torch.zeros(prot_data.num_nodes, _ea.size(1))\n",
    "_cnt = torch.zeros(prot_data.num_nodes, 1)\n",
    "_x.scatter_add_(0, _ei[0].unsqueeze(1).expand_as(_ea), _ea)\n",
    "_cnt.scatter_add_(0, _ei[0].unsqueeze(1), torch.ones(_ei.size(1), 1))\n",
    "_cnt[_cnt == 0] = 1\n",
    "prot_data.x = _x / _cnt\n",
    "\n",
    "print(f\"\\nogbn-proteins: {prot_data.num_nodes:,} nodes  {prot_data.num_edges:,} edges  \"\n",
    "      f\"x={tuple(prot_data.x.shape)}  y={tuple(prot_data.y.shape)}\")\n",
    "print(f\"  tr={len(prot_splits['train']):,}  va={len(prot_splits['valid']):,}  \"\n",
    "      f\"te={len(prot_splits['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Shared Utilities \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "def build_adj(edge_index: torch.Tensor, n_nodes: int) -> sp.csr_matrix:\n",
    "    \"\"\"Symmetric sparse adjacency from a PyG edge_index.\"\"\"\n",
    "    r, c  = edge_index.numpy()\n",
    "    rows  = np.concatenate([r, c])\n",
    "    cols  = np.concatenate([c, r])\n",
    "    mat   = sp.coo_matrix((np.ones(len(rows)), (rows, cols)), shape=(n_nodes, n_nodes)).tocsr()\n",
    "    mat.data = np.ones_like(mat.data)     # binarise duplicate entries\n",
    "    return mat\n",
    "\n",
    "\n",
    "def row_normalise(M: sp.spmatrix) -> sp.spmatrix:\n",
    "    \"\"\"Return D^{-1} A (row-stochastic form).\"\"\"\n",
    "    sums  = np.array(M.sum(axis=1)).flatten()\n",
    "    inv_d = np.where(sums > 0, 1.0 / sums, 0.0)\n",
    "    return sp.diags(inv_d).dot(M)\n",
    "\n",
    "\n",
    "def scipy_to_sparse_torch(M: sp.spmatrix) -> torch.Tensor:\n",
    "    \"\"\"Convert scipy sparse \u2192 torch sparse COO float tensor.\"\"\"\n",
    "    coo  = M.tocoo().astype(np.float32)\n",
    "    idx  = torch.from_numpy(np.vstack([coo.row, coo.col]).astype(np.int64))\n",
    "    vals = torch.from_numpy(coo.data)\n",
    "    return torch.sparse_coo_tensor(idx, vals, torch.Size(coo.shape))\n",
    "\n",
    "\n",
    "def extract_arrays(pyg_data, n_cls):\n",
    "    \"\"\"Pull a PyG Data object into scipy/numpy arrays (LP.py / gcn.py format).\"\"\"\n",
    "    adjacency = build_adj(pyg_data.edge_index, pyg_data.num_nodes)\n",
    "    feats     = pyg_data.x.numpy().astype(np.float32)\n",
    "    y_int     = pyg_data.y.numpy().flatten()\n",
    "    labels_oh = np.zeros((pyg_data.num_nodes, n_cls), dtype=np.float32)\n",
    "    labels_oh[np.arange(pyg_data.num_nodes), y_int] = 1.0\n",
    "    tr = pyg_data.train_mask.numpy().nonzero()[0].tolist()\n",
    "    va = pyg_data.val_mask.numpy().nonzero()[0].tolist()\n",
    "    te = pyg_data.test_mask.numpy().nonzero()[0].tolist()\n",
    "    return adjacency, feats, labels_oh, tr, va, te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_q1",
   "metadata": {},
   "source": [
    "---",
    "## Q1 \u2014 Node Classification",
    "",
    "| Algorithm | Features | Notes |",
    "|-----------|----------|-------|",
    "| **Label Propagation** | none (structure only) | deterministic \u2192 std = 0 |",
    "| **Node2Vec** | none (structure only) | gensim random-walk embeddings + LogReg |",
    "| **GCN** | node features (required) | custom 2-layer implementation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_lp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Label Propagation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Based on LP.py structure: iterative diffusion Y \u2190 \u03b1\u00b7S\u00b7Y + (1-\u03b1)\u00b7Y\u2080\n",
    "# with hard clamping of labelled nodes after every step.\n",
    "# No features used. Deterministic \u2192 all 10 runs identical, std = 0.\n",
    "\n",
    "def diffuse_labels(transition, seed_mat, anchor_nodes,\n",
    "                   decay=0.99, max_steps=1000, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Propagate label beliefs through the graph.\n",
    "    transition  : row-normalised adjacency (scipy sparse)\n",
    "    seed_mat    : (n, C) float, non-zero only at training rows\n",
    "    anchor_nodes: indices re-clamped to seed_mat each iteration\n",
    "    Returns     : converged belief matrix, per-step Frobenius residuals\n",
    "    \"\"\"\n",
    "    belief    = seed_mat.copy()\n",
    "    residuals = []\n",
    "    for _ in range(max_steps):\n",
    "        prev   = belief.copy()\n",
    "        belief = decay * transition.dot(belief) + (1.0 - decay) * seed_mat\n",
    "        belief[anchor_nodes, :] = seed_mat[anchor_nodes, :]   # clamp\n",
    "        delta  = np.linalg.norm(belief - prev, ord='fro')\n",
    "        residuals.append(delta)\n",
    "        if delta < eps:\n",
    "            break\n",
    "    return belief, residuals\n",
    "\n",
    "\n",
    "def run_lp(adj, labels_oh, tr_idx, va_idx, te_idx,\n",
    "           decay=0.99, max_steps=1000, n_runs=N_RUNS):\n",
    "    transition = row_normalise(adj)\n",
    "    y_hard     = np.argmax(labels_oh, axis=1)\n",
    "    seed_mat   = np.zeros_like(labels_oh)\n",
    "    seed_mat[tr_idx, :] = labels_oh[tr_idx, :]\n",
    "\n",
    "    beliefs, residuals = diffuse_labels(transition, seed_mat, tr_idx, decay, max_steps)\n",
    "    preds     = np.argmax(beliefs, axis=1)\n",
    "    test_acc  = float(np.mean(preds[te_idx] == y_hard[te_idx]))\n",
    "    scores    = [test_acc] * n_runs\n",
    "    print(f\"  LP  acc={test_acc:.4f}  (deterministic, std=0)\")\n",
    "    return scores, residuals\n",
    "\n",
    "\n",
    "def run_lp_proteins(data, splits, decay=0.85, max_steps=100, n_runs=N_RUNS):\n",
    "    \"\"\"Vectorised LP for ogbn-proteins (112 binary tasks, macro ROC-AUC).\"\"\"\n",
    "    adj        = build_adj(data.edge_index, data.num_nodes)\n",
    "    transition = row_normalise(adj)\n",
    "    y          = data.y.numpy().astype(float)\n",
    "    tr_idx     = splits['train'].numpy()\n",
    "    te_idx     = splits['test'].numpy()\n",
    "\n",
    "    seed_mat   = np.zeros_like(y)\n",
    "    seed_mat[tr_idx] = y[tr_idx]\n",
    "    beliefs, residuals = diffuse_labels(transition, seed_mat, tr_idx, decay, max_steps)\n",
    "\n",
    "    def macro_auc(idx):\n",
    "        yt = y[idx].astype(int);  yp = beliefs[idx]\n",
    "        vc = (yt.sum(0) > 0) & (yt.sum(0) < len(idx))\n",
    "        return roc_auc_score(yt[:, vc], yp[:, vc], average='macro') if vc.sum() else float('nan')\n",
    "\n",
    "    auc    = macro_auc(te_idx)\n",
    "    scores = [auc] * n_runs\n",
    "    print(f\"  LP  auc={auc:.4f}  (deterministic, std=0)\")\n",
    "    return scores, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_n2v",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Node2Vec \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Based on node_to_vector.py: gensim-based random walks \u2192 LogisticRegression.\n",
    "# No features used. Stochastic (different seeds each run) \u2192 non-zero variance.\n",
    "\n",
    "def embed_graph(nx_graph, embed_dim=64, walk_len=40, n_walks=10,\n",
    "                p=1, q=1, seed=0):\n",
    "    \"\"\"Train Node2Vec and return (n_nodes, embed_dim) embedding matrix.\"\"\"\n",
    "    walker   = GensimNode2Vec(nx_graph, dimensions=embed_dim,\n",
    "                               walk_length=walk_len, num_walks=n_walks,\n",
    "                               p=p, q=q, workers=1, seed=seed, quiet=True)\n",
    "    wv_model = walker.fit(window=10, min_count=1, batch_words=4)\n",
    "    node_ids = sorted(nx_graph.nodes())\n",
    "    return np.array([wv_model.wv[str(nid)] for nid in node_ids])\n",
    "\n",
    "\n",
    "def run_n2v_planetoid(data, n_cls, n_runs=N_RUNS, embed_dim=64, walk_len=40):\n",
    "    adj, _, labels_oh, tr_idx, va_idx, te_idx = extract_arrays(data, n_cls)\n",
    "    nx_graph    = nx.from_scipy_sparse_array(adj)\n",
    "    true_labels = np.argmax(labels_oh, axis=1)\n",
    "    test_accs   = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        emb = embed_graph(nx_graph, embed_dim=embed_dim,\n",
    "                          walk_len=walk_len, seed=run * 13)\n",
    "        clf = LogisticRegression(max_iter=300, random_state=run, C=1.0)\n",
    "        clf.fit(emb[tr_idx], true_labels[tr_idx])\n",
    "        acc = accuracy_score(true_labels[te_idx], clf.predict(emb[te_idx]))\n",
    "        test_accs.append(acc)\n",
    "        print(f\"  N2V run {run+1:02d}/{n_runs}: acc={acc:.4f}\")\n",
    "    return test_accs\n",
    "\n",
    "\n",
    "def run_n2v_proteins(data, splits, n_runs=N_RUNS,\n",
    "                     embed_dim=32, walk_len=10, n_walks=3):\n",
    "    \"\"\"Node2Vec for ogbn-proteins (multilabel ROC-AUC, reduced walk settings).\"\"\"\n",
    "    adj      = build_adj(data.edge_index, data.num_nodes)\n",
    "    nx_graph = nx.from_scipy_sparse_array(adj)\n",
    "    y        = data.y.numpy().astype(int)\n",
    "    tr_idx   = splits['train'].numpy()\n",
    "    te_idx   = splits['test'].numpy()\n",
    "    test_aucs = []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        emb = embed_graph(nx_graph, embed_dim=embed_dim,\n",
    "                          walk_len=walk_len, n_walks=n_walks, seed=run * 13)\n",
    "        clf = MultiOutputClassifier(\n",
    "            LogisticRegression(max_iter=200, random_state=run, C=0.5), n_jobs=-1)\n",
    "        clf.fit(emb[tr_idx], y[tr_idx])\n",
    "        probs = np.stack([e.predict_proba(emb[te_idx])[:, 1]\n",
    "                          for e in clf.estimators_], axis=1)\n",
    "        yt    = y[te_idx]\n",
    "        vc    = (yt.sum(0) > 0) & (yt.sum(0) < len(te_idx))\n",
    "        auc   = roc_auc_score(yt[:, vc], probs[:, vc], average='macro')\n",
    "        test_aucs.append(auc)\n",
    "        print(f\"  N2V proteins run {run+1:02d}/{n_runs}: auc={auc:.4f}\")\n",
    "    return test_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_gcn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Custom GCN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Based on gcn.py structure: custom layer using torch.spmm on pre-computed\n",
    "# sparse normalised adjacency.  Node features are used (required for GCN).\n",
    "\n",
    "class SpectralConvBlock(nn.Module):\n",
    "    \"\"\"Graph conv: dropout \u2192 sparse propagation \u2192 linear \u2192 ReLU.\"\"\"\n",
    "    def __init__(self, ch_in, ch_out, p_drop):\n",
    "        super().__init__()\n",
    "        self.proj   = nn.Linear(ch_in, ch_out)\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "    def forward(self, h, norm_adj):\n",
    "        h = F.dropout(h, p=self.p_drop, training=self.training)\n",
    "        h = torch.spmm(norm_adj, h)\n",
    "        return F.relu(self.proj(h))\n",
    "\n",
    "\n",
    "class NodeClassifierNet(nn.Module):\n",
    "    \"\"\"2-layer GCN for multi-class node classification (gcn.py layout).\"\"\"\n",
    "    def __init__(self, n_feat, n_hid, n_cls, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.layer_in  = SpectralConvBlock(n_feat, n_hid, p_drop)\n",
    "        self.layer_out = SpectralConvBlock(n_hid,  n_cls, p_drop)\n",
    "        self.p_drop    = p_drop\n",
    "\n",
    "    def forward(self, x, norm_adj):\n",
    "        h = self.layer_in(x, norm_adj)\n",
    "        h = F.dropout(h, p=self.p_drop, training=self.training)\n",
    "        h = torch.spmm(norm_adj, h)\n",
    "        return F.log_softmax(self.layer_out.proj(h), dim=1)\n",
    "\n",
    "\n",
    "def train_one_gcn(adj, feats, labels_oh, tr_idx, va_idx, te_idx,\n",
    "                  n_hid=16, epochs=200, lr=0.01, wd=5e-4,\n",
    "                  p_drop=0.5, patience=10, seed=0):\n",
    "    fix_seed(seed)\n",
    "    norm_adj = row_normalise(adj)\n",
    "    feat_t   = torch.FloatTensor(feats)\n",
    "    lbl_t    = torch.LongTensor(np.argmax(labels_oh, axis=1))\n",
    "    adj_t    = scipy_to_sparse_torch(norm_adj)\n",
    "    tr_t, va_t, te_t = (torch.LongTensor(x) for x in (tr_idx, va_idx, te_idx))\n",
    "\n",
    "    net   = NodeClassifierNet(feats.shape[1], n_hid, labels_oh.shape[1], p_drop)\n",
    "    opt   = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    best_val, pat_ctr, best_state = float('inf'), 0, None\n",
    "    tr_hist, va_hist = [], []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        net.train();  opt.zero_grad()\n",
    "        logits  = net(feat_t, adj_t)\n",
    "        tr_loss = F.nll_loss(logits[tr_t], lbl_t[tr_t])\n",
    "        tr_loss.backward();  opt.step()\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            logits  = net(feat_t, adj_t)\n",
    "            va_loss = F.nll_loss(logits[va_t], lbl_t[va_t]).item()\n",
    "        tr_hist.append(tr_loss.item());  va_hist.append(va_loss)\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val  = va_loss\n",
    "            best_state = {k: v.clone() for k, v in net.state_dict().items()}\n",
    "            pat_ctr   = 0\n",
    "        else:\n",
    "            pat_ctr  += 1\n",
    "        if pat_ctr >= patience:\n",
    "            break\n",
    "\n",
    "    net.load_state_dict(best_state);  net.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = net(feat_t, adj_t)[te_t].argmax(1)\n",
    "        acc  = (pred == lbl_t[te_t]).float().mean().item()\n",
    "    return acc, tr_hist, va_hist\n",
    "\n",
    "\n",
    "def run_gcn_planetoid(adj, feats, labels_oh, tr_idx, va_idx, te_idx,\n",
    "                      n_runs=N_RUNS, **kw):\n",
    "    test_accs, all_tr, all_va = [], [], []\n",
    "    for run in range(n_runs):\n",
    "        acc, tr_h, va_h = train_one_gcn(adj, feats, labels_oh,\n",
    "                                         tr_idx, va_idx, te_idx,\n",
    "                                         seed=run * 17, **kw)\n",
    "        test_accs.append(acc);  all_tr.append(tr_h);  all_va.append(va_h)\n",
    "        print(f\"  GCN run {run+1:02d}/{n_runs}: acc={acc:.4f}\")\n",
    "    return test_accs, all_tr, all_va\n",
    "\n",
    "\n",
    "class ProteinsGCN(nn.Module):\n",
    "    \"\"\"3-layer GCN with BatchNorm for ogbn-proteins multilabel task.\"\"\"\n",
    "    def __init__(self, n_feat, n_hid, n_cls=112, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.c1  = nn.Linear(n_feat, n_hid);  self.bn1 = nn.BatchNorm1d(n_hid)\n",
    "        self.c2  = nn.Linear(n_hid,  n_hid);  self.bn2 = nn.BatchNorm1d(n_hid)\n",
    "        self.c3  = nn.Linear(n_hid,  n_cls);  self.p   = p_drop\n",
    "\n",
    "    def forward(self, x, adj_sp):\n",
    "        h = F.relu(self.bn1(self.c1(torch.spmm(adj_sp, x))))\n",
    "        h = F.dropout(h, p=self.p, training=self.training)\n",
    "        h = F.relu(self.bn2(self.c2(torch.spmm(adj_sp, h))))\n",
    "        h = F.dropout(h, p=self.p, training=self.training)\n",
    "        return self.c3(torch.spmm(adj_sp, h))\n",
    "\n",
    "\n",
    "def run_gcn_proteins(data, splits, n_runs=N_RUNS, n_hid=256, epochs=100, lr=1e-2):\n",
    "    adj    = build_adj(data.edge_index, data.num_nodes)\n",
    "    adj_t  = scipy_to_sparse_torch(row_normalise(adj)).to(DEVICE)\n",
    "    feat_t = data.x.to(DEVICE)\n",
    "    y_t    = data.y.float().to(DEVICE)\n",
    "    tr_idx = splits['train'].to(DEVICE)\n",
    "    va_idx = splits['valid'].to(DEVICE)\n",
    "    te_idx = splits['test']\n",
    "    y_test = data.y[te_idx].numpy().astype(int)\n",
    "    test_aucs, all_tr, all_va = [], [], []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        fix_seed(run * 17)\n",
    "        net  = ProteinsGCN(feat_t.size(1), n_hid).to(DEVICE)\n",
    "        opt  = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        crit = nn.BCEWithLogitsLoss()\n",
    "        tr_h, va_h = [], []\n",
    "        for _ in range(epochs):\n",
    "            net.train();  opt.zero_grad()\n",
    "            out  = net(feat_t, adj_t)\n",
    "            loss = crit(out[tr_idx], y_t[tr_idx])\n",
    "            loss.backward();  opt.step()\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                out    = net(feat_t, adj_t)\n",
    "                va_l   = crit(out[va_idx], y_t[va_idx]).item()\n",
    "            tr_h.append(loss.item());  va_h.append(va_l)\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(net(feat_t, adj_t)[te_idx.to(DEVICE)]).cpu().numpy()\n",
    "        vc  = (y_test.sum(0) > 0) & (y_test.sum(0) < len(y_test))\n",
    "        auc = roc_auc_score(y_test[:, vc], probs[:, vc], average='macro')\n",
    "        test_aucs.append(auc);  all_tr.append(tr_h);  all_va.append(va_h)\n",
    "        print(f\"  GCN proteins run {run+1:02d}/{n_runs}: auc={auc:.4f}\")\n",
    "    return test_aucs, all_tr, all_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q1run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q1 Experiments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "q1_store = {}\n",
    "\n",
    "for tag in ['cora', 'citeseer', 'pubmed']:\n",
    "    ds    = pyg_catalog[tag]\n",
    "    data  = ds[0]\n",
    "    adj, feats, labels_oh, tr_idx, va_idx, te_idx = extract_arrays(data, ds.num_classes)\n",
    "    print(f\"\\n{'='*56}\\n{tag.upper()}  ({ds.num_classes} classes)\\n{'='*56}\")\n",
    "    q1_store[tag] = {}\n",
    "\n",
    "    print('\\n\u2500\u2500 Label Propagation \u2500\u2500')\n",
    "    sc, resid = run_lp(adj, labels_oh, tr_idx, va_idx, te_idx)\n",
    "    q1_store[tag]['LP'] = {'scores': sc, 'residuals': resid}\n",
    "\n",
    "    print('\\n\u2500\u2500 Node2Vec \u2500\u2500')\n",
    "    sc = run_n2v_planetoid(data, ds.num_classes)\n",
    "    q1_store[tag]['Node2Vec'] = {'scores': sc}\n",
    "    print(f\"  mean={np.mean(sc):.4f}  std={np.std(sc):.4f}\")\n",
    "\n",
    "    print('\\n\u2500\u2500 GCN \u2500\u2500')\n",
    "    sc, tr_h, va_h = run_gcn_planetoid(adj, feats, labels_oh, tr_idx, va_idx, te_idx)\n",
    "    q1_store[tag]['GCN'] = {'scores': sc, 'tr_hist': tr_h, 'va_hist': va_h}\n",
    "    print(f\"  mean={np.mean(sc):.4f}  std={np.std(sc):.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*56}\\nOGBN-PROTEINS  (ROC-AUC)\\n{'='*56}\")\n",
    "q1_store['ogbn-proteins'] = {}\n",
    "\n",
    "print('\\n\u2500\u2500 Label Propagation \u2500\u2500')\n",
    "sc, resid = run_lp_proteins(prot_data, prot_splits)\n",
    "q1_store['ogbn-proteins']['LP'] = {'scores': sc, 'residuals': resid}\n",
    "\n",
    "print('\\n\u2500\u2500 Node2Vec \u2500\u2500')\n",
    "sc = run_n2v_proteins(prot_data, prot_splits)\n",
    "q1_store['ogbn-proteins']['Node2Vec'] = {'scores': sc}\n",
    "print(f\"  mean={np.mean(sc):.4f}  std={np.std(sc):.4f}\")\n",
    "\n",
    "print('\\n\u2500\u2500 GCN \u2500\u2500')\n",
    "sc, tr_h, va_h = run_gcn_proteins(prot_data, prot_splits)\n",
    "q1_store['ogbn-proteins']['GCN'] = {'scores': sc, 'tr_hist': tr_h, 'va_hist': va_h}\n",
    "print(f\"  mean={np.mean(sc):.4f}  std={np.std(sc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q1curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q1 Training Curves \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# One 2-panel figure per dataset.\n",
    "#   Left : LP Frobenius-residual convergence (log-scale)\n",
    "#   Right: GCN train & val loss (mean \u00b1 std across 10 runs)\n",
    "# Node2Vec training is handled internally by gensim; no per-epoch loss exposed.\n",
    "\n",
    "def plot_q1_curves(tag, store, gcn_ylabel='NLL Loss'):\n",
    "    fig, (ax_lp, ax_gcn) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(f'{tag.upper()} \u2014 Q1 Training Curves', fontweight='bold')\n",
    "\n",
    "    resid = store[tag]['LP']['residuals']\n",
    "    ax_lp.semilogy(range(1, len(resid) + 1), resid, color='steelblue', lw=2)\n",
    "    ax_lp.set_xlabel('Iteration');  ax_lp.set_ylabel('Frobenius Residual (log)')\n",
    "    ax_lp.set_title('Label Propagation \u2014 Convergence');  ax_lp.grid(True, alpha=0.3)\n",
    "\n",
    "    max_ep = max(len(h) for h in store[tag]['GCN']['tr_hist'])\n",
    "    def pad_runs(lst):\n",
    "        return np.array([np.pad(h, (0, max_ep - len(h)), constant_values=h[-1]) for h in lst])\n",
    "    tr_arr = pad_runs(store[tag]['GCN']['tr_hist'])\n",
    "    va_arr = pad_runs(store[tag]['GCN']['va_hist'])\n",
    "    ep = range(1, max_ep + 1)\n",
    "    for arr, col, lbl in [(tr_arr, 'forestgreen', 'train'), (va_arr, 'crimson', 'val')]:\n",
    "        m, s = arr.mean(0), arr.std(0)\n",
    "        ax_gcn.plot(ep, m, color=col, lw=2, label=f'{lbl} loss')\n",
    "        ax_gcn.fill_between(ep, m - s, m + s, alpha=0.2, color=col)\n",
    "    ax_gcn.set_xlabel('Epoch');  ax_gcn.set_ylabel(gcn_ylabel)\n",
    "    ax_gcn.set_title('GCN \u2014 Train / Val Loss');  ax_gcn.legend(fontsize=9);  ax_gcn.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'q1_curves_{tag}.pdf', bbox_inches='tight');  plt.show()\n",
    "\n",
    "for tag in ['cora', 'citeseer', 'pubmed']:\n",
    "    plot_q1_curves(tag, q1_store)\n",
    "plot_q1_curves('ogbn-proteins', q1_store, gcn_ylabel='BCE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q1results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q1 Per-Dataset Tables \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "algos_q1 = ['LP', 'Node2Vec', 'GCN']\n",
    "for tag in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    metric = 'ROC-AUC' if tag == 'ogbn-proteins' else 'Accuracy'\n",
    "    rows = [{'Run': r+1, **{a: f\"{q1_store[tag][a]['scores'][r]:.4f}\" for a in algos_q1}}\n",
    "            for r in range(N_RUNS)]\n",
    "    df = pd.DataFrame(rows).set_index('Run')\n",
    "    df.loc['mean'] = {a: f\"{np.mean(q1_store[tag][a]['scores']):.4f}\" for a in algos_q1}\n",
    "    df.loc['std']  = {a: f\"{np.std(q1_store[tag][a]['scores']):.4f}\"  for a in algos_q1}\n",
    "    print(f\"\\n{'\u2500'*50}\\n {tag.upper()}  \u2014 Test {metric}\\n{'\u2500'*50}\")\n",
    "    print(df.to_string())\n",
    "\n",
    "# Summary pivot\n",
    "rows = []\n",
    "for tag in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    m = 'ROC-AUC' if tag == 'ogbn-proteins' else 'Accuracy'\n",
    "    for algo in algos_q1:\n",
    "        v = q1_store[tag][algo]['scores']\n",
    "        rows.append({'Dataset': tag, 'Algorithm': algo,\n",
    "                     'Result': f\"{np.mean(v):.4f} \\u00b1 {np.std(v):.4f}\"})\n",
    "pivot = pd.DataFrame(rows).pivot_table(\n",
    "    values='Result', index='Dataset', columns='Algorithm', aggfunc='first')[algos_q1]\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Q1 \u2014 Node Classification  (mean \\u00b1 std, 10 runs)\")\n",
    "print(\"=\"*65);  print(pivot.to_string())\n",
    "\n",
    "# Bar plot\n",
    "tags_q1  = ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']\n",
    "titles_q1 = ['Cora\\n(Accuracy)', 'CiteSeer\\n(Accuracy)',\n",
    "              'PubMed\\n(Accuracy)', 'ogbn-proteins\\n(ROC-AUC)']\n",
    "bar_cols  = ['steelblue', 'darkorange', 'seagreen']\n",
    "fig, axes = plt.subplots(1, 4, figsize=(17, 5))\n",
    "for ax, tag, title in zip(axes, tags_q1, titles_q1):\n",
    "    means = [np.mean(q1_store[tag][a]['scores']) for a in algos_q1]\n",
    "    stds  = [np.std(q1_store[tag][a]['scores'])  for a in algos_q1]\n",
    "    bars  = ax.bar(algos_q1, means, yerr=stds, capsize=7,\n",
    "                   color=bar_cols, alpha=0.82, edgecolor='k', lw=0.7)\n",
    "    ax.set_title(title, fontsize=10);  ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, min(1.15, max(means) + max(stds) + 0.15));  ax.grid(axis='y', alpha=0.3)\n",
    "    for bar, m, s in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, m+s+0.01, f'{m:.3f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "fig.suptitle('Q1 \u2014 Node Classification: Algorithm Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout();  plt.savefig('q1_comparison.pdf', bbox_inches='tight');  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_q2",
   "metadata": {},
   "source": [
    "---",
    "## Q2 \u2014 Link Prediction",
    "",
    "**Protocol**: for each run, drop 20% of edges uniformly at random as the held-out positive set.  ",
    "**Negative sampling**: sample an equal number of random non-edges; the same set is used by all algorithms within a run (consistent).  ",
    "**Metric**: AUC (mean \u00b1 std over 10 runs).",
    "",
    "| Algorithm | Features | Notes |",
    "|-----------|----------|-------|",
    "| **Common Neighbours** | none | structural heuristic |",
    "| **Jaccard Coefficient** | none | structural heuristic |",
    "| **GCN encoder-decoder** | node features | inner-product edge scorer |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q2prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q2 Edge Splitting & Negative Sampling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Based on Q2.py: randomly drop 20% of edges; sample equal negative edges.\n",
    "# Negative sampling strategy: uniform random non-edges (no self-loops,\n",
    "# not in the full edge set). Same negatives shared across all algorithms\n",
    "# within a run for fair comparison.\n",
    "\n",
    "def split_edges(edge_index: torch.Tensor, n_nodes: int,\n",
    "                held_out_frac=0.2, seed=0):\n",
    "    rng    = np.random.RandomState(seed)\n",
    "    edges  = edge_index.T.numpy()           # (E, 2)\n",
    "    n_test = int(len(edges) * held_out_frac)\n",
    "    perm   = rng.permutation(len(edges))\n",
    "    pos_test  = edges[perm[:n_test]]\n",
    "    train_arr = edges[perm[n_test:]]\n",
    "    train_ei  = torch.from_numpy(train_arr.T).long()\n",
    "\n",
    "    full_set = set(map(tuple, edges))\n",
    "    neg, tries = [], 0\n",
    "    while len(neg) < n_test and tries < n_test * 30:\n",
    "        u, v = rng.randint(0, n_nodes, 2)\n",
    "        if u != v and (u,v) not in full_set and (v,u) not in full_set:\n",
    "            neg.append([u, v]);  full_set.add((u, v))\n",
    "        tries += 1\n",
    "    return train_ei, pos_test, np.array(neg[:n_test])\n",
    "\n",
    "\n",
    "def make_nx(edge_arr, n_nodes):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n_nodes))\n",
    "    G.add_edges_from(map(tuple, edge_arr))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q2heur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Structural Heuristic Link Predictors \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Based on Q2.py logic. No features used.\n",
    "\n",
    "def count_shared_nbrs(train_nx, pairs):\n",
    "    \"\"\"Common-neighbour count for each (u, v) pair.\"\"\"\n",
    "    return np.array([\n",
    "        len(list(nx.common_neighbors(train_nx, int(u), int(v))))\n",
    "        if train_nx.has_node(int(u)) and train_nx.has_node(int(v)) else 0\n",
    "        for u, v in pairs\n",
    "    ], dtype=float)\n",
    "\n",
    "\n",
    "def jaccard_scores(train_nx, pairs):\n",
    "    \"\"\"Jaccard coefficient for each (u, v) pair.\"\"\"\n",
    "    out = []\n",
    "    for u, v in pairs:\n",
    "        u, v = int(u), int(v)\n",
    "        if train_nx.has_node(u) and train_nx.has_node(v):\n",
    "            out.append(next(nx.jaccard_coefficient(train_nx, [(u, v)]))[2])\n",
    "        else:\n",
    "            out.append(0.0)\n",
    "    return np.array(out)\n",
    "\n",
    "\n",
    "def eval_link_heuristic(score_fn, train_nx, pos_test, neg_test):\n",
    "    all_pairs  = np.concatenate([pos_test, neg_test], axis=0)\n",
    "    all_labels = np.concatenate([np.ones(len(pos_test)), np.zeros(len(neg_test))])\n",
    "    return roc_auc_score(all_labels, score_fn(train_nx, all_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q2gcn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 GCN Link Predictor (encoder-decoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Encoder: 2-layer GCN (same SpectralConvBlock) \u2192 node embeddings\n",
    "# Decoder: \u03c3(z_u \u00b7 z_v)  (inner-product + sigmoid)\n",
    "# Training: BCE on positive (train edges) + negative samples\n",
    "\n",
    "class LinkEncoderGCN(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_emb, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.enc1   = SpectralConvBlock(n_feat, n_hid, p_drop)\n",
    "        self.enc2   = nn.Linear(n_hid, n_emb)\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "    def encode(self, x, norm_adj):\n",
    "        h = self.enc1(x, norm_adj)\n",
    "        h = F.dropout(h, p=self.p_drop, training=self.training)\n",
    "        return self.enc2(h)\n",
    "\n",
    "    @staticmethod\n",
    "    def score_pairs(z, node_pairs):\n",
    "        src, dst = node_pairs[:, 0], node_pairs[:, 1]\n",
    "        return torch.sigmoid((z[src] * z[dst]).sum(dim=1))\n",
    "\n",
    "\n",
    "def train_link_gcn(adj_train, feats, pos_tr, neg_tr, pos_te, neg_te,\n",
    "                   n_hid=64, n_emb=32, epochs=100, lr=1e-2, p_drop=0.5, seed=0):\n",
    "    fix_seed(seed)\n",
    "    adj_t  = scipy_to_sparse_torch(row_normalise(adj_train))\n",
    "    feat_t = torch.FloatTensor(feats)\n",
    "\n",
    "    def make_batch(pos, neg):\n",
    "        pairs  = torch.LongTensor(np.concatenate([pos, neg]))\n",
    "        labels = torch.FloatTensor(np.concatenate([np.ones(len(pos)), np.zeros(len(neg))]))\n",
    "        return pairs, labels\n",
    "\n",
    "    tr_pairs, tr_lbl = make_batch(pos_tr, neg_tr)\n",
    "    va_pairs, va_lbl = make_batch(pos_te, neg_te)\n",
    "\n",
    "    net = LinkEncoderGCN(feats.shape[1], n_hid, n_emb, p_drop)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    tr_hist, va_hist = [], []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        net.train();  opt.zero_grad()\n",
    "        z    = net.encode(feat_t, adj_t)\n",
    "        loss = F.binary_cross_entropy(net.score_pairs(z, tr_pairs), tr_lbl)\n",
    "        loss.backward();  opt.step()\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            z      = net.encode(feat_t, adj_t)\n",
    "            va_loss = F.binary_cross_entropy(net.score_pairs(z, va_pairs), va_lbl).item()\n",
    "        tr_hist.append(loss.item());  va_hist.append(va_loss)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        z     = net.encode(feat_t, adj_t)\n",
    "        pairs = torch.LongTensor(np.concatenate([pos_te, neg_te]))\n",
    "        probs = net.score_pairs(z, pairs).numpy()\n",
    "    true = np.concatenate([np.ones(len(pos_te)), np.zeros(len(neg_te))])\n",
    "    return roc_auc_score(true, probs), tr_hist, va_hist\n",
    "\n",
    "\n",
    "def run_gcn_lp(data, n_runs=N_RUNS):\n",
    "    feats = data.x.numpy().astype(np.float32)\n",
    "    aucs, all_tr, all_va = [], [], []\n",
    "    for run in range(n_runs):\n",
    "        train_ei, pos_te, neg_te = split_edges(data.edge_index, data.num_nodes, seed=run)\n",
    "        adj_tr = build_adj(train_ei, data.num_nodes)\n",
    "        tr_pos = train_ei.T.numpy()\n",
    "        rng    = np.random.RandomState(run)\n",
    "        neg_tr = neg_te[rng.choice(len(neg_te), min(len(tr_pos), len(neg_te)), replace=False)]\n",
    "\n",
    "        auc, tr_h, va_h = train_link_gcn(adj_tr, feats, tr_pos, neg_tr, pos_te, neg_te, seed=run*17)\n",
    "        aucs.append(auc);  all_tr.append(tr_h);  all_va.append(va_h)\n",
    "        print(f\"  GCN-LP run {run+1:02d}/{n_runs}: auc={auc:.4f}\")\n",
    "    return aucs, all_tr, all_va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q2run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q2 Experiments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "q2_store = {}\n",
    "\n",
    "for tag in ['cora', 'citeseer', 'pubmed']:\n",
    "    data = pyg_catalog[tag][0]\n",
    "    print(f\"\\n{'='*56}\\n{tag.upper()} \u2014 Link Prediction\\n{'='*56}\")\n",
    "    q2_store[tag] = {a: {'aucs': []} for a in ['Common Nbrs', 'Jaccard', 'GCN']}\n",
    "    for k in ['GCN']:  q2_store[tag][k].update({'tr_hist': [], 'va_hist': []})\n",
    "\n",
    "    for run in range(N_RUNS):\n",
    "        train_ei, pos_te, neg_te = split_edges(data.edge_index, data.num_nodes, seed=run)\n",
    "        train_nx = make_nx(train_ei.T.numpy(), data.num_nodes)\n",
    "        q2_store[tag]['Common Nbrs']['aucs'].append(\n",
    "            eval_link_heuristic(count_shared_nbrs, train_nx, pos_te, neg_te))\n",
    "        q2_store[tag]['Jaccard']['aucs'].append(\n",
    "            eval_link_heuristic(jaccard_scores, train_nx, pos_te, neg_te))\n",
    "        cn  = q2_store[tag]['Common Nbrs']['aucs'][-1]\n",
    "        jac = q2_store[tag]['Jaccard']['aucs'][-1]\n",
    "        print(f\"  run {run+1:02d}/{N_RUNS}: CN={cn:.4f}  Jaccard={jac:.4f}\")\n",
    "\n",
    "    print('\\n\u2500\u2500 GCN link predictor \u2500\u2500')\n",
    "    aucs, tr_h, va_h = run_gcn_lp(data)\n",
    "    q2_store[tag]['GCN'].update({'aucs': aucs, 'tr_hist': tr_h, 'va_hist': va_h})\n",
    "    for algo in ['Common Nbrs', 'Jaccard', 'GCN']:\n",
    "        v = q2_store[tag][algo]['aucs']\n",
    "        print(f\"  {algo:<14} mean={np.mean(v):.4f}  std={np.std(v):.4f}\")\n",
    "\n",
    "# ogbn-proteins: subsampled to 5 000 nodes (heuristic tractability)\n",
    "print(f\"\\n{'='*56}\\nOGBN-PROTEINS \u2014 Link Prediction (5 000-node subgraph)\\n{'='*56}\")\n",
    "fix_seed(0)\n",
    "sub_n     = 5000\n",
    "sub_nodes = np.random.choice(prot_data.num_nodes, sub_n, replace=False)\n",
    "nmap      = {int(o): i for i, o in enumerate(sub_nodes)}\n",
    "ei_np     = prot_data.edge_index.T.numpy()\n",
    "mask      = np.isin(ei_np[:,0], sub_nodes) & np.isin(ei_np[:,1], sub_nodes)\n",
    "sub_e     = np.array([[nmap[int(u)], nmap[int(v)]] for u, v in ei_np[mask]])\n",
    "sub_ei    = torch.from_numpy(sub_e.T).long()\n",
    "sub_data  = PyGData(x=prot_data.x[sub_nodes], edge_index=sub_ei,\n",
    "                    y=prot_data.y[sub_nodes], num_nodes=sub_n)\n",
    "print(f\"Subgraph: {sub_n} nodes, {sub_ei.size(1)} edges\")\n",
    "\n",
    "q2_store['ogbn-proteins'] = {a: {'aucs': []} for a in ['Common Nbrs', 'Jaccard', 'GCN']}\n",
    "for k in ['GCN']:  q2_store['ogbn-proteins'][k].update({'tr_hist': [], 'va_hist': []})\n",
    "for run in range(N_RUNS):\n",
    "    train_ei, pos_te, neg_te = split_edges(sub_ei, sub_n, seed=run)\n",
    "    train_nx = make_nx(train_ei.T.numpy(), sub_n)\n",
    "    q2_store['ogbn-proteins']['Common Nbrs']['aucs'].append(\n",
    "        eval_link_heuristic(count_shared_nbrs, train_nx, pos_te, neg_te))\n",
    "    q2_store['ogbn-proteins']['Jaccard']['aucs'].append(\n",
    "        eval_link_heuristic(jaccard_scores, train_nx, pos_te, neg_te))\n",
    "    cn  = q2_store['ogbn-proteins']['Common Nbrs']['aucs'][-1]\n",
    "    jac = q2_store['ogbn-proteins']['Jaccard']['aucs'][-1]\n",
    "    print(f\"  run {run+1:02d}/{N_RUNS}: CN={cn:.4f}  Jaccard={jac:.4f}\")\n",
    "\n",
    "print('\\n\u2500\u2500 GCN link predictor \u2500\u2500')\n",
    "aucs, tr_h, va_h = run_gcn_lp(sub_data)\n",
    "q2_store['ogbn-proteins']['GCN'].update({'aucs': aucs, 'tr_hist': tr_h, 'va_hist': va_h})\n",
    "for algo in ['Common Nbrs', 'Jaccard', 'GCN']:\n",
    "    v = q2_store['ogbn-proteins'][algo]['aucs']\n",
    "    print(f\"  {algo:<14} mean={np.mean(v):.4f}  std={np.std(v):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q2curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q2 GCN Training Curves \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# One figure per dataset: GCN link-predictor train/val BCE loss.\n",
    "# Common Neighbours and Jaccard have no iterative training phase.\n",
    "\n",
    "def plot_q2_curves(tag, store):\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    fig.suptitle(f'{tag.upper()} \u2014 Q2 GCN Link Predictor: Train/Val Loss',\n",
    "                 fontweight='bold')\n",
    "    max_ep = max(len(h) for h in store[tag]['GCN']['tr_hist'])\n",
    "    def pad(lst):\n",
    "        return np.array([np.pad(h, (0, max_ep-len(h)), constant_values=h[-1]) for h in lst])\n",
    "    tr_arr = pad(store[tag]['GCN']['tr_hist'])\n",
    "    va_arr = pad(store[tag]['GCN']['va_hist'])\n",
    "    ep = range(1, max_ep + 1)\n",
    "    for arr, col, lbl in [(tr_arr, 'teal', 'train'), (va_arr, 'tomato', 'val')]:\n",
    "        m, s = arr.mean(0), arr.std(0)\n",
    "        ax.plot(ep, m, color=col, lw=2, label=f'{lbl} loss')\n",
    "        ax.fill_between(ep, m-s, m+s, alpha=0.2, color=col)\n",
    "    ax.set_xlabel('Epoch');  ax.set_ylabel('BCE Loss')\n",
    "    ax.legend(fontsize=9);   ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'q2_gcn_curves_{tag}.pdf', bbox_inches='tight');  plt.show()\n",
    "\n",
    "for tag in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    plot_q2_curves(tag, q2_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_q2results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Q2 Per-Dataset Tables \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "algos_q2 = ['Common Nbrs', 'Jaccard', 'GCN']\n",
    "for tag in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    rows = [{'Run': r+1, **{a: f\"{q2_store[tag][a]['aucs'][r]:.4f}\" for a in algos_q2}}\n",
    "            for r in range(N_RUNS)]\n",
    "    df = pd.DataFrame(rows).set_index('Run')\n",
    "    df.loc['mean'] = {a: f\"{np.mean(q2_store[tag][a]['aucs']):.4f}\" for a in algos_q2}\n",
    "    df.loc['std']  = {a: f\"{np.std(q2_store[tag][a]['aucs']):.4f}\"  for a in algos_q2}\n",
    "    print(f\"\\n{'\u2500'*50}\\n {tag.upper()}  \u2014 Test AUC\\n{'\u2500'*50}\")\n",
    "    print(df.to_string())\n",
    "\n",
    "rows = []\n",
    "for tag in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    for algo in algos_q2:\n",
    "        v = q2_store[tag][algo]['aucs']\n",
    "        rows.append({'Dataset': tag, 'Algorithm': algo,\n",
    "                     'AUC': f\"{np.mean(v):.4f} \\u00b1 {np.std(v):.4f}\"})\n",
    "pivot = pd.DataFrame(rows).pivot_table(\n",
    "    values='AUC', index='Dataset', columns='Algorithm', aggfunc='first')[algos_q2]\n",
    "print(\"\\n\"+\"=\"*65)\n",
    "print(\"Q2 \u2014 Link Prediction  (mean \\u00b1 std, 10 runs)\")\n",
    "print(\"=\"*65);  print(pivot.to_string())\n",
    "\n",
    "tags_q2   = ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']\n",
    "titles_q2 = ['Cora\\n(AUC)', 'CiteSeer\\n(AUC)', 'PubMed\\n(AUC)', 'ogbn-proteins\\n(AUC)']\n",
    "bar_cols  = ['steelblue', 'darkorange', 'seagreen']\n",
    "fig, axes = plt.subplots(1, 4, figsize=(17, 5))\n",
    "for ax, tag, title in zip(axes, tags_q2, titles_q2):\n",
    "    means = [np.mean(q2_store[tag][a]['aucs']) for a in algos_q2]\n",
    "    stds  = [np.std(q2_store[tag][a]['aucs'])  for a in algos_q2]\n",
    "    bars  = ax.bar(algos_q2, means, yerr=stds, capsize=7,\n",
    "                   color=bar_cols, alpha=0.82, edgecolor='k', lw=0.7)\n",
    "    ax.set_title(title, fontsize=10);  ax.set_ylabel('AUC')\n",
    "    ax.set_ylim(0, min(1.15, max(means)+max(stds)+0.15));  ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticks(range(len(algos_q2)))\n",
    "    ax.set_xticklabels(['Com.Nbrs', 'Jaccard', 'GCN'], fontsize=8)\n",
    "    for bar, m, s in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x()+bar.get_width()/2, m+s+0.01, f'{m:.3f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "fig.suptitle('Q2 \u2014 Link Prediction: Algorithm Comparison', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout();  plt.savefig('q2_comparison.pdf', bbox_inches='tight');  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (A3 venv)",
   "language": "python",
   "name": "a3_venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}