{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "33373b06",
      "metadata": {},
      "source": [
        "# Assignment 3 - COMP 511"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1000001",
      "metadata": {},
      "source": [
        "## Q1 — Node Classification [50%]\n",
        "\n",
        "Three algorithms compared on Cora, CiteSeer, PubMed, and ogbn-arxiv (all measured by accuracy):\n",
        "\n",
        "| Algorithm | Features used | Notes |\n",
        "|-----------|--------------|-------|\n",
        "| **Label Propagation** | None (structure only) | Deterministic — std = 0 across runs |\n",
        "| **Node2Vec** | None (structure only) | Embeddings → Logistic Regression |\n",
        "| **GCN** | Node features (required) | Kipf & Welling (2017) |\n",
        "\n",
        "**Splits**: Planetoid protocol for Cora/CiteSeer/PubMed (`torch_geometric.datasets.Planetoid`), official OGB split for ogbn-arxiv.\n",
        "\n",
        "**Results reported**: mean ± std over 10 independent runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a1000002",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCN device: mps  |  Node2Vec/LP device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ── Imports ──────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "\n",
        "# PyTorch ≥2.6 defaults weights_only=True in torch.load, which breaks OGB/PyG\n",
        "# dataset caching (they pickle custom Data classes). Restore the old default.\n",
        "# Store the true original on the module so re-running this cell is always safe.\n",
        "if not hasattr(torch, '_original_load'):\n",
        "    torch._original_load = torch.load\n",
        "def _patched_load(*args, **kwargs):\n",
        "    kwargs.setdefault('weights_only', False)\n",
        "    return torch._original_load(*args, **kwargs)\n",
        "torch.load = _patched_load\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, Node2Vec\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "# Device: prefer MPS (Apple Silicon) > CUDA > CPU\n",
        "if torch.backends.mps.is_available():\n",
        "    GCN_DEVICE = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    GCN_DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    GCN_DEVICE = torch.device('cpu')\n",
        "CPU = torch.device('cpu')\n",
        "print(f\"GCN device: {GCN_DEVICE}  |  Node2Vec/LP device: cpu\")\n",
        "\n",
        "N_RUNS = 10\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1000003",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cora      :   2708 nodes   10556 edges  1433 features  7 classes  train=140 val=500 test=1000\n",
            "CiteSeer  :   3327 nodes    9104 edges  3703 features  6 classes  train=120 val=500 test=1000\n",
            "PubMed    :  19717 nodes   88648 edges   500 features  3 classes  train=60 val=500 test=1000\n",
            "\n",
            "ogbn-arxiv: 169,343 nodes  1,166,243 edges  x=(169343, 128)  y classes=40\n",
            "  train=90,941  valid=29,799  test=48,603\n"
          ]
        }
      ],
      "source": [
        "# ── Dataset Loading ───────────────────────────────────────────────────────────\n",
        "#\n",
        "# Planetoid (Cora / CiteSeer / PubMed): auto-downloaded by torch_geometric.\n",
        "# Uses the Planetoid split protocol as required by the assignment.\n",
        "#\n",
        "# ogbn-arxiv: auto-downloaded by OGB. Uses official year-based split.\n",
        "\n",
        "planetoid_datasets = {}\n",
        "for name in ['Cora', 'CiteSeer', 'PubMed']:\n",
        "    ds = Planetoid(root=f'data/{name}', name=name, transform=T.NormalizeFeatures())\n",
        "    planetoid_datasets[name.lower()] = ds\n",
        "    d = ds[0]\n",
        "    print(f\"{name:10s}: {d.num_nodes:6d} nodes  {d.num_edges:6d} edges  \"\n",
        "          f\"{ds.num_features:4d} features  {ds.num_classes} classes  \"\n",
        "          f\"train={d.train_mask.sum().item()} val={d.val_mask.sum().item()} test={d.test_mask.sum().item()}\")\n",
        "\n",
        "# ogbn-arxiv — multiclass (40 CS subject areas), 128-dim node features\n",
        "dataset_arxiv = PygNodePropPredDataset(name='ogbn-arxiv', root='data/ogbn-arxiv')\n",
        "split_idx     = dataset_arxiv.get_idx_split()\n",
        "data_arxiv    = dataset_arxiv[0]\n",
        "n_arxiv_classes = int(data_arxiv.y.max()) + 1\n",
        "\n",
        "print(f\"\\nogbn-arxiv: {data_arxiv.num_nodes:,} nodes  {data_arxiv.num_edges:,} edges  \"\n",
        "      f\"x={tuple(data_arxiv.x.shape)}  y classes={n_arxiv_classes}\")\n",
        "print(f\"  train={len(split_idx['train']):,}  valid={len(split_idx['valid']):,}  \"\n",
        "      f\"test={len(split_idx['test']):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1000004",
      "metadata": {},
      "source": [
        "### Algorithm 1 — Label Propagation\n",
        "\n",
        "Classic semi-supervised LP (Zhu et al., 2003).  \n",
        "No features are used — purely structure-based.  \n",
        "Update rule: **Y ← α D⁻¹A Y + (1−α) Y₀**, iterated until convergence.  \n",
        "Because the algorithm is deterministic given the graph and fixed splits, all 10 runs return identical results (std = 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1000005",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_norm_adj(edge_index: torch.Tensor, n_nodes: int):\n",
        "    \"\"\"Row-normalised adjacency D⁻¹A (scipy CSR, symmetric).\"\"\"\n",
        "    r, c = edge_index.numpy()\n",
        "    rows = np.concatenate([r, c])\n",
        "    cols = np.concatenate([c, r])\n",
        "    adj  = csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(n_nodes, n_nodes))\n",
        "    adj.data[:] = 1.0                          # binarise duplicate entries\n",
        "    deg  = np.array(adj.sum(axis=1)).flatten()\n",
        "    deg[deg == 0] = 1\n",
        "    return diags(1.0 / deg) @ adj              # row-stochastic\n",
        "\n",
        "\n",
        "def label_propagate(A_norm, Y0: np.ndarray, alpha=0.85, max_iter=200, tol=1e-6):\n",
        "    \"\"\"Iterate Y ← α A_norm Y + (1-α) Y0 until convergence. Returns (Y, trace).\"\"\"\n",
        "    Y, trace = Y0.copy(), []\n",
        "    for _ in range(max_iter):\n",
        "        Y_new = alpha * (A_norm @ Y) + (1.0 - alpha) * Y0\n",
        "        delta = float(np.abs(Y_new - Y).max())\n",
        "        trace.append(delta)\n",
        "        Y = Y_new\n",
        "        if delta < tol:\n",
        "            break\n",
        "    return Y, trace\n",
        "\n",
        "\n",
        "def run_lp_planetoid(data, n_classes, alpha=0.85, max_iter=200, n_runs=N_RUNS):\n",
        "    \"\"\"LP for Planetoid datasets. Returns (test_accs, val_accs, lp_trace).\"\"\"\n",
        "    A_norm = build_norm_adj(data.edge_index, data.num_nodes)\n",
        "    labels = data.y.numpy().flatten()\n",
        "    train_idx = data.train_mask.numpy().nonzero()[0]\n",
        "    val_idx   = data.val_mask.numpy().nonzero()[0]\n",
        "    test_idx  = data.test_mask.numpy().nonzero()[0]\n",
        "\n",
        "    Y0 = np.zeros((data.num_nodes, n_classes))\n",
        "    Y0[train_idx, labels[train_idx]] = 1.0\n",
        "\n",
        "    Y, trace = label_propagate(A_norm, Y0, alpha=alpha, max_iter=max_iter)\n",
        "    pred = Y.argmax(axis=1)\n",
        "\n",
        "    test_acc = float((pred[test_idx] == labels[test_idx]).mean())\n",
        "    val_acc  = float((pred[val_idx]  == labels[val_idx]).mean())\n",
        "    # LP is deterministic → replicate result across all runs\n",
        "    return [test_acc] * n_runs, [val_acc] * n_runs, trace\n",
        "\n",
        "\n",
        "def run_lp_ogb(data, split_idx, n_classes, alpha=0.85, max_iter=200, n_runs=N_RUNS):\n",
        "    \"\"\"LP for OGB datasets (multiclass). Metric: accuracy.\"\"\"\n",
        "    A_norm    = build_norm_adj(data.edge_index, data.num_nodes)\n",
        "    labels    = data.y.numpy().flatten()\n",
        "    train_idx = split_idx['train'].numpy()\n",
        "    valid_idx = split_idx['valid'].numpy()\n",
        "    test_idx  = split_idx['test'].numpy()\n",
        "\n",
        "    Y0 = np.zeros((data.num_nodes, n_classes))\n",
        "    Y0[train_idx, labels[train_idx]] = 1.0\n",
        "\n",
        "    Y, trace = label_propagate(A_norm, Y0, alpha=alpha, max_iter=max_iter)\n",
        "    pred = Y.argmax(axis=1)\n",
        "\n",
        "    test_acc = float((pred[test_idx] == labels[test_idx]).mean())\n",
        "    val_acc  = float((pred[valid_idx] == labels[valid_idx]).mean())\n",
        "    return [test_acc] * n_runs, [val_acc] * n_runs, trace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1000006",
      "metadata": {},
      "source": [
        "### Algorithm 2 — Node2Vec\n",
        "\n",
        "Random-walk-based node embedding (Grover & Leskovec, 2016).  \n",
        "No features are used — purely structure-based.  \n",
        "Embeddings are trained via the skip-gram objective, then a Logistic Regression classifier is fitted on the training nodes and evaluated on test nodes.  \n",
        "Stochastic across runs (different seeds → different walk samples and optimisation trajectories) → non-zero variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a1000007",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_node2vec_model(\n",
        "    edge_index, n_nodes,\n",
        "    embedding_dim=128, walk_length=20, context_size=10,\n",
        "    walks_per_node=10, p=1.0, q=1.0,\n",
        "    n_epochs=50, lr=0.01, batch_size=256, seed=0\n",
        "):\n",
        "    \"\"\"Train Node2Vec skip-gram model. Returns (embeddings, epoch_losses).\"\"\"\n",
        "    set_seed(seed)\n",
        "    model = Node2Vec(\n",
        "        edge_index,\n",
        "        embedding_dim=embedding_dim,\n",
        "        walk_length=walk_length,\n",
        "        context_size=context_size,\n",
        "        walks_per_node=walks_per_node,\n",
        "        p=p, q=q,\n",
        "        num_negative_samples=1,\n",
        "        sparse=True,\n",
        "        num_nodes=n_nodes,\n",
        "    ).to(CPU)\n",
        "\n",
        "    loader    = model.loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    optimizer = torch.optim.SparseAdam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for pos_rw, neg_rw in loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.loss(pos_rw, neg_rw)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total += loss.item()\n",
        "        losses.append(total / len(loader))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model().cpu().numpy()\n",
        "    return z, losses\n",
        "\n",
        "\n",
        "def run_node2vec_planetoid(data, n_classes, n_epochs=50, n_runs=N_RUNS):\n",
        "    \"\"\"Node2Vec for Planetoid datasets. Returns (test_accs, val_accs, all_losses).\"\"\"\n",
        "    train_m = data.train_mask.numpy()\n",
        "    val_m   = data.val_mask.numpy()\n",
        "    test_m  = data.test_mask.numpy()\n",
        "    labels  = data.y.numpy().flatten()\n",
        "\n",
        "    test_accs, val_accs, all_losses = [], [], []\n",
        "    for run in range(n_runs):\n",
        "        z, losses = train_node2vec_model(\n",
        "            data.edge_index, data.num_nodes,\n",
        "            embedding_dim=128, walk_length=20, context_size=10,\n",
        "            walks_per_node=10, n_epochs=n_epochs, lr=0.01, seed=run * 7\n",
        "        )\n",
        "        clf = LogisticRegression(max_iter=1000, random_state=run, C=1.0)\n",
        "        clf.fit(z[train_m], labels[train_m])\n",
        "        test_accs.append(clf.score(z[test_m], labels[test_m]))\n",
        "        val_accs.append(clf.score(z[val_m],  labels[val_m]))\n",
        "        all_losses.append(losses)\n",
        "        print(f\"  Node2Vec run {run+1:02d}/{n_runs}: test={test_accs[-1]:.4f}\")\n",
        "    return test_accs, val_accs, all_losses\n",
        "\n",
        "\n",
        "def run_node2vec_ogb(data, split_idx, n_epochs=10, n_runs=N_RUNS):\n",
        "    \"\"\"Node2Vec for OGB datasets (multiclass). Metric: accuracy.\"\"\"\n",
        "    labels    = data.y.numpy().flatten()\n",
        "    train_idx = split_idx['train'].numpy()\n",
        "    valid_idx = split_idx['valid'].numpy()\n",
        "    test_idx  = split_idx['test'].numpy()\n",
        "\n",
        "    test_accs, val_accs, all_losses = [], [], []\n",
        "    for run in range(n_runs):\n",
        "        z, losses = train_node2vec_model(\n",
        "            data.edge_index, data.num_nodes,\n",
        "            embedding_dim=128, walk_length=20, context_size=10,\n",
        "            walks_per_node=5, n_epochs=n_epochs, lr=0.01,\n",
        "            batch_size=512, seed=run * 7\n",
        "        )\n",
        "        clf = LogisticRegression(max_iter=500, random_state=run, C=1.0)\n",
        "        clf.fit(z[train_idx], labels[train_idx])\n",
        "        test_accs.append(clf.score(z[test_idx], labels[test_idx]))\n",
        "        val_accs.append(clf.score(z[valid_idx], labels[valid_idx]))\n",
        "        all_losses.append(losses)\n",
        "        print(f\"  Node2Vec ogb run {run+1:02d}/{n_runs}: test={test_accs[-1]:.4f}\")\n",
        "    return test_accs, val_accs, all_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1000008",
      "metadata": {},
      "source": [
        "### Algorithm 3 — Graph Convolutional Network (GCN)\n",
        "\n",
        "Standard 2-layer GCN (Kipf & Welling, 2017) for Planetoid datasets.  \n",
        "3-layer GCN with BatchNorm for ogbn-arxiv (larger graph benefits from deeper model).  \n",
        "**Node features are used** (required for GCN per the assignment).  \n",
        "Stochastic across runs (random weight initialisation + stochastic optimisation) → non-zero variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a1000009",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"2-layer GCN for multi-class node classification.\"\"\"\n",
        "    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1   = GCNConv(in_ch, hidden_ch)\n",
        "        self.conv2   = GCNConv(hidden_ch, out_ch)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "\n",
        "class GCN3(torch.nn.Module):\n",
        "    \"\"\"3-layer GCN with BatchNorm for larger graphs (e.g. ogbn-arxiv).\"\"\"\n",
        "    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_ch, hidden_ch)\n",
        "        self.bn1   = torch.nn.BatchNorm1d(hidden_ch)\n",
        "        self.conv2 = GCNConv(hidden_ch, hidden_ch)\n",
        "        self.bn2   = torch.nn.BatchNorm1d(hidden_ch)\n",
        "        self.conv3 = GCNConv(hidden_ch, out_ch)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.bn1(self.conv1(x, edge_index)).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.bn2(self.conv2(x, edge_index)).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv3(x, edge_index)\n",
        "\n",
        "\n",
        "def run_gcn_planetoid(data, n_classes, hidden=256, n_epochs=200,\n",
        "                      lr=1e-2, wd=5e-4, n_runs=N_RUNS):\n",
        "    \"\"\"Train 2-layer GCN on a Planetoid dataset. Returns (test_accs, histories).\"\"\"\n",
        "    data_dev = data.to(GCN_DEVICE)\n",
        "    all_test, all_hist = [], []\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        set_seed(run * 13)\n",
        "        model = GCN(data.num_node_features, hidden, n_classes).to(GCN_DEVICE)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        crit  = torch.nn.CrossEntropyLoss()\n",
        "        hist  = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            loss = crit(out[data_dev.train_mask], data_dev.y[data_dev.train_mask].squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out      = model(data_dev.x, data_dev.edge_index)\n",
        "                val_loss = crit(out[data_dev.val_mask],\n",
        "                                data_dev.y[data_dev.val_mask].squeeze()).item()\n",
        "                val_pred = out[data_dev.val_mask].argmax(1)\n",
        "                val_acc  = (val_pred == data_dev.y[data_dev.val_mask].squeeze()\n",
        "                            ).float().mean().item()\n",
        "            hist['train_loss'].append(loss.item())\n",
        "            hist['val_loss'].append(val_loss)\n",
        "            hist['val_acc'].append(val_acc)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            pred = out[data_dev.test_mask].argmax(1)\n",
        "            test_acc = (pred == data_dev.y[data_dev.test_mask].squeeze()\n",
        "                        ).float().mean().item()\n",
        "        all_test.append(test_acc)\n",
        "        all_hist.append(hist)\n",
        "        print(f\"  GCN run {run+1:02d}/{n_runs}: test={test_acc:.4f}\")\n",
        "\n",
        "    data_dev.to(CPU)\n",
        "    return all_test, all_hist\n",
        "\n",
        "\n",
        "def run_gcn_ogb(data, split_idx, n_classes, hidden=256, n_epochs=200,\n",
        "                lr=1e-2, wd=5e-4, n_runs=N_RUNS):\n",
        "    \"\"\"Train 3-layer GCN on an OGB dataset (multiclass). Metric: accuracy.\"\"\"\n",
        "    data_dev  = data.to(GCN_DEVICE)\n",
        "    train_idx = split_idx['train'].to(GCN_DEVICE)\n",
        "    valid_idx = split_idx['valid'].to(GCN_DEVICE)\n",
        "    test_idx  = split_idx['test'].to(GCN_DEVICE)\n",
        "    labels    = data.y.squeeze().to(GCN_DEVICE)\n",
        "    all_test, all_hist = [], []\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        set_seed(run * 13)\n",
        "        model = GCN3(data.x.size(1), hidden, n_classes).to(GCN_DEVICE)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        crit  = torch.nn.CrossEntropyLoss()\n",
        "        hist  = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            loss = crit(out[train_idx], labels[train_idx])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out      = model(data_dev.x, data_dev.edge_index)\n",
        "                val_loss = crit(out[valid_idx], labels[valid_idx]).item()\n",
        "                val_pred = out[valid_idx].argmax(1)\n",
        "                val_acc  = (val_pred == labels[valid_idx]).float().mean().item()\n",
        "            hist['train_loss'].append(loss.item())\n",
        "            hist['val_loss'].append(val_loss)\n",
        "            hist['val_acc'].append(val_acc)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            pred = out[test_idx].argmax(1)\n",
        "            test_acc = (pred == labels[test_idx]).float().mean().item()\n",
        "        all_test.append(test_acc)\n",
        "        all_hist.append(hist)\n",
        "        print(f\"  GCN ogb run {run+1:02d}/{n_runs}: test={test_acc:.4f}\")\n",
        "\n",
        "    data_dev.to(CPU)\n",
        "    return all_test, all_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1000010",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CORA  (7 classes)\n",
            "============================================================\n",
            "\n",
            "── Label Propagation ──\n",
            "  LP  test acc : 0.6940 ± 0.0000\n",
            "\n",
            "── Node2Vec ──\n"
          ]
        }
      ],
      "source": [
        "# ── Run All Experiments ───────────────────────────────────────────────────────\n",
        "#\n",
        "# results[dataset][algorithm] = {\n",
        "#   'test'    : list of N_RUNS test scores,\n",
        "#   'val'     : list of N_RUNS val  scores,   (LP / Node2Vec)\n",
        "#   'history' : list of N_RUNS history dicts, (Node2Vec, GCN)\n",
        "#   'lp_trace': convergence trace,            (LP only)\n",
        "# }\n",
        "\n",
        "results = {}\n",
        "\n",
        "# ── Planetoid ─────────────────────────────────────────────────────────────────\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed']:\n",
        "    ds        = planetoid_datasets[ds_name]\n",
        "    data      = ds[0]\n",
        "    n_classes = ds.num_classes\n",
        "    print(f\"\\n{'='*60}\\n{ds_name.upper()}  ({n_classes} classes)\\n{'='*60}\")\n",
        "    results[ds_name] = {}\n",
        "\n",
        "    # Label Propagation\n",
        "    print(\"\\n── Label Propagation ──\")\n",
        "    test, val, trace = run_lp_planetoid(data, n_classes)\n",
        "    results[ds_name]['LP'] = {'test': test, 'val': val, 'lp_trace': trace}\n",
        "    print(f\"  LP  test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "    # Node2Vec\n",
        "    print(\"\\n── Node2Vec ──\")\n",
        "    test, val, hists = run_node2vec_planetoid(data, n_classes, n_epochs=50)\n",
        "    results[ds_name]['Node2Vec'] = {'test': test, 'val': val, 'history': hists}\n",
        "    print(f\"  N2V test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "    # GCN\n",
        "    print(\"\\n── GCN ──\")\n",
        "    test, hists = run_gcn_planetoid(data, n_classes, n_epochs=200)\n",
        "    results[ds_name]['GCN'] = {'test': test, 'history': hists}\n",
        "    print(f\"  GCN test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "# ── ogbn-arxiv ────────────────────────────────────────────────────────────────\n",
        "print(f\"\\n{'='*60}\\nOGBN-ARXIV  ({n_arxiv_classes} classes, metric: Accuracy)\\n{'='*60}\")\n",
        "results['ogbn-arxiv'] = {}\n",
        "\n",
        "print(\"\\n── Label Propagation ──\")\n",
        "test, val, trace = run_lp_ogb(data_arxiv, split_idx, n_arxiv_classes)\n",
        "results['ogbn-arxiv']['LP'] = {'test': test, 'val': val, 'lp_trace': trace}\n",
        "print(f\"  LP  test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "print(\"\\n── Node2Vec ──\")\n",
        "test, val, hists = run_node2vec_ogb(data_arxiv, split_idx, n_epochs=10)\n",
        "results['ogbn-arxiv']['Node2Vec'] = {'test': test, 'val': val, 'history': hists}\n",
        "print(f\"  N2V test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "print(\"\\n── GCN ──\")\n",
        "test, hists = run_gcn_ogb(data_arxiv, split_idx, n_arxiv_classes, n_epochs=200)\n",
        "results['ogbn-arxiv']['GCN'] = {'test': test, 'history': hists}\n",
        "print(f\"  GCN test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1000011",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Training Curves ───────────────────────────────────────────────────────────\n",
        "# One figure per dataset, three panels:\n",
        "#   Left  : LP convergence (max label change per iteration, log-scale)\n",
        "#   Centre: Node2Vec training loss (skip-gram, mean ± std across runs)\n",
        "#   Right : GCN train loss and validation loss (mean ± std across runs)\n",
        "\n",
        "def plot_training_curves(ds_name, res, y_label_gcn='Cross-Entropy Loss'):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    fig.suptitle(f'{ds_name.upper()} — Training Curves', fontsize=13, fontweight='bold')\n",
        "\n",
        "    # ── LP convergence ────────────────────────────────────────────────────────\n",
        "    ax    = axes[0]\n",
        "    trace = res['LP']['lp_trace']\n",
        "    ax.semilogy(range(1, len(trace) + 1), trace, color='steelblue', lw=2)\n",
        "    ax.set_xlabel('Iteration')\n",
        "    ax.set_ylabel('Max Label Change (log scale)')\n",
        "    ax.set_title('Label Propagation — Convergence')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ── Node2Vec skip-gram loss ───────────────────────────────────────────────\n",
        "    ax     = axes[1]\n",
        "    losses = np.array(res['Node2Vec']['history'])   # (n_runs, n_epochs)\n",
        "    mean_l = losses.mean(0)\n",
        "    std_l  = losses.std(0)\n",
        "    ep     = range(1, len(mean_l) + 1)\n",
        "    ax.plot(ep, mean_l, color='darkorange', lw=2, label='train loss (mean)')\n",
        "    ax.fill_between(ep, mean_l - std_l, mean_l + std_l, alpha=0.25, color='darkorange',\n",
        "                    label='±1 std')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Skip-gram Loss')\n",
        "    ax.set_title('Node2Vec — Training Loss')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ── GCN train + val loss ──────────────────────────────────────────────────\n",
        "    ax  = axes[2]\n",
        "    tl  = np.array([h['train_loss'] for h in res['GCN']['history']])\n",
        "    vl  = np.array([h['val_loss']   for h in res['GCN']['history']])\n",
        "    ep  = range(1, tl.shape[1] + 1)\n",
        "    ax.plot(ep, tl.mean(0), color='forestgreen', lw=2, label='train loss')\n",
        "    ax.fill_between(ep, tl.mean(0) - tl.std(0), tl.mean(0) + tl.std(0),\n",
        "                    alpha=0.2, color='forestgreen')\n",
        "    ax.plot(ep, vl.mean(0), color='crimson', lw=2, label='val loss')\n",
        "    ax.fill_between(ep, vl.mean(0) - vl.std(0), vl.mean(0) + vl.std(0),\n",
        "                    alpha=0.2, color='crimson')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel(y_label_gcn)\n",
        "    ax.set_title('GCN — Train / Val Loss')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_curves_{ds_name}.pdf', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed']:\n",
        "    plot_training_curves(ds_name, results[ds_name])\n",
        "\n",
        "plot_training_curves('ogbn-arxiv', results['ogbn-arxiv'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1000012",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Per-Dataset Result Tables ─────────────────────────────────────────────────\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    metric = 'Accuracy'\n",
        "    rows   = []\n",
        "    for run in range(N_RUNS):\n",
        "        row = {'Run': run + 1}\n",
        "        for algo in ['LP', 'Node2Vec', 'GCN']:\n",
        "            row[algo] = f\"{results[ds_name][algo]['test'][run]:.4f}\"\n",
        "        rows.append(row)\n",
        "\n",
        "    df_per = pd.DataFrame(rows).set_index('Run')\n",
        "    # Append mean and std rows\n",
        "    mean_row = {algo: f\"{np.mean(results[ds_name][algo]['test']):.4f}\"\n",
        "                for algo in ['LP', 'Node2Vec', 'GCN']}\n",
        "    std_row  = {algo: f\"{np.std(results[ds_name][algo]['test']):.4f}\"\n",
        "                for algo in ['LP', 'Node2Vec', 'GCN']}\n",
        "    df_per.loc['mean'] = mean_row\n",
        "    df_per.loc['std']  = std_row\n",
        "\n",
        "    print(f\"\\n{'─'*55}\")\n",
        "    print(f\" {ds_name.upper()}  |  Test {metric} per run\")\n",
        "    print(f\"{'─'*55}\")\n",
        "    print(df_per.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1000013",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Summary Table (all datasets × all algorithms) ─────────────────────────────\n",
        "\n",
        "summary_rows = []\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    for algo in ['LP', 'Node2Vec', 'GCN']:\n",
        "        vals = results[ds_name][algo]['test']\n",
        "        summary_rows.append({\n",
        "            'Dataset': ds_name, 'Algorithm': algo, 'Metric': 'Accuracy',\n",
        "            'Mean': np.mean(vals), 'Std': np.std(vals),\n",
        "            'Result': f\"{np.mean(vals):.4f} ± {np.std(vals):.4f}\"\n",
        "        })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows)\n",
        "pivot = df_summary.pivot_table(values='Result', index='Dataset',\n",
        "                                columns='Algorithm', aggfunc='first')[['LP', 'Node2Vec', 'GCN']]\n",
        "pivot.index.name = 'Dataset'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Q1 — Node Classification: Test Accuracy (mean ± std, 10 runs)\")\n",
        "print(\"=\"*70)\n",
        "print(pivot.to_string())\n",
        "\n",
        "# ── Grouped Bar Plot ──────────────────────────────────────────────────────────\n",
        "ds_list = ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']\n",
        "titles  = ['Cora\\n(Accuracy)', 'CiteSeer\\n(Accuracy)',\n",
        "           'PubMed\\n(Accuracy)', 'ogbn-arxiv\\n(Accuracy)']\n",
        "algos   = ['LP', 'Node2Vec', 'GCN']\n",
        "colors  = ['steelblue', 'darkorange', 'forestgreen']\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(17, 5), sharey=False)\n",
        "for ax, ds_name, title in zip(axes, ds_list, titles):\n",
        "    means = [np.mean(results[ds_name][a]['test']) for a in algos]\n",
        "    stds  = [np.std(results[ds_name][a]['test'])  for a in algos]\n",
        "    bars  = ax.bar(algos, means, yerr=stds, capsize=7,\n",
        "                   color=colors, alpha=0.82, edgecolor='black', lw=0.7)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_ylim(0, min(1.12, max(means) + max(stds) + 0.15))\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    for bar, m, s in zip(bars, means, stds):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2,\n",
        "                m + s + 0.01, f'{m:.3f}',\n",
        "                ha='center', va='bottom', fontsize=8.5)\n",
        "\n",
        "fig.suptitle('Q1 — Node Classification: Algorithm Comparison across Datasets',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('q1_comparison_barplot.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
