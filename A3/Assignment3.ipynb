{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33373b06",
   "metadata": {},
   "source": [
    "# Assignment 3 - COMP 511"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "## Q1 \u2014 Node Classification [50%]\n",
    "\n",
    "Three algorithms compared on Cora, CiteSeer, PubMed (accuracy) and ogbn-proteins (ROC-AUC):\n",
    "\n",
    "| Algorithm | Features used | Notes |\n",
    "|-----------|--------------|-------|\n",
    "| **Label Propagation** | None (structure only) | Deterministic \u2014 std = 0 across runs |\n",
    "| **Node2Vec** | None (structure only) | Embeddings \u2192 Logistic Regression |\n",
    "| **GCN** | Node features (required) | Kipf & Welling (2017) |\n",
    "\n",
    "**Splits**: Planetoid protocol for Cora/CiteSeer/PubMed (`torch_geometric.datasets.Planetoid`), official OGB split for ogbn-proteins.\n",
    "\n",
    "**Results reported**: mean \u00b1 std over 10 independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Imports \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, Node2Vec\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "\n",
    "# Device: prefer MPS (Apple Silicon) > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    GCN_DEVICE = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    GCN_DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    GCN_DEVICE = torch.device('cpu')\n",
    "CPU = torch.device('cpu')\n",
    "print(f\"GCN device: {GCN_DEVICE}  |  Node2Vec/LP device: cpu\")\n",
    "\n",
    "N_RUNS = 10\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Dataset Loading \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# Planetoid (Cora / CiteSeer / PubMed): auto-downloaded by torch_geometric,\n",
    "# identical in spirit to A2's gdown downloads \u2014 everything is automated.\n",
    "# Uses the Planetoid split protocol as required by the assignment.\n",
    "#\n",
    "# ogbn-proteins: auto-downloaded by OGB.\n",
    "\n",
    "planetoid_datasets = {}\n",
    "for name in ['Cora', 'CiteSeer', 'PubMed']:\n",
    "    ds = Planetoid(root=f'data/{name}', name=name, transform=T.NormalizeFeatures())\n",
    "    planetoid_datasets[name.lower()] = ds\n",
    "    d = ds[0]\n",
    "    print(f\"{name:10s}: {d.num_nodes:6d} nodes  {d.num_edges:6d} edges  \"\n",
    "          f\"{ds.num_features:4d} features  {ds.num_classes} classes  \"\n",
    "          f\"train={d.train_mask.sum().item()} val={d.val_mask.sum().item()} test={d.test_mask.sum().item()}\")\n",
    "\n",
    "# ogbn-proteins\n",
    "dataset_proteins = PygNodePropPredDataset(name='ogbn-proteins', root='data/ogbn-proteins')\n",
    "split_idx        = dataset_proteins.get_idx_split()\n",
    "data_proteins    = dataset_proteins[0]\n",
    "\n",
    "# ogbn-proteins has no native node features.\n",
    "# Standard approach (per OGB docs): build node features by averaging\n",
    "# the edge features of each node's incident edges (gives 8-dim vectors).\n",
    "row       = data_proteins.edge_index[0]\n",
    "edge_attr = data_proteins.edge_attr.float()   # (E, 8)\n",
    "x_prot    = torch.zeros(data_proteins.num_nodes, edge_attr.size(1))\n",
    "cnt       = torch.zeros(data_proteins.num_nodes, 1)\n",
    "x_prot.scatter_add_(0, row.unsqueeze(1).expand_as(edge_attr), edge_attr)\n",
    "cnt.scatter_add_(0, row.unsqueeze(1), torch.ones(len(row), 1))\n",
    "cnt[cnt == 0] = 1\n",
    "data_proteins.x = x_prot / cnt\n",
    "\n",
    "print(f\"\\nogbn-proteins: {data_proteins.num_nodes:,} nodes  {data_proteins.num_edges:,} edges  \"\n",
    "      f\"x={tuple(data_proteins.x.shape)}  y={tuple(data_proteins.y.shape)}\")\n",
    "print(f\"  train={len(split_idx['train']):,}  valid={len(split_idx['valid']):,}  \"\n",
    "      f\"test={len(split_idx['test']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000004",
   "metadata": {},
   "source": [
    "### Algorithm 1 \u2014 Label Propagation\n",
    "\n",
    "Classic semi-supervised LP (Zhu et al., 2003).  \n",
    "No features are used \u2014 purely structure-based.  \n",
    "Update rule: **Y \u2190 \u03b1 D\u207b\u00b9A Y + (1\u2212\u03b1) Y\u2080**, iterated until convergence.  \n",
    "Because the algorithm is deterministic given the graph and fixed splits, all 10 runs return identical results (std = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_norm_adj(edge_index: torch.Tensor, n_nodes: int):\n",
    "    \"\"\"Row-normalised adjacency D\u207b\u00b9A (scipy CSR, symmetric).\"\"\"\n",
    "    r, c = edge_index.numpy()\n",
    "    rows = np.concatenate([r, c])\n",
    "    cols = np.concatenate([c, r])\n",
    "    adj  = csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(n_nodes, n_nodes))\n",
    "    adj.data[:] = 1.0                          # binarise duplicate entries\n",
    "    deg  = np.array(adj.sum(axis=1)).flatten()\n",
    "    deg[deg == 0] = 1\n",
    "    return diags(1.0 / deg) @ adj              # row-stochastic\n",
    "\n",
    "\n",
    "def label_propagate(A_norm, Y0: np.ndarray, alpha=0.85, max_iter=200, tol=1e-6):\n",
    "    \"\"\"Iterate Y \u2190 \u03b1 A_norm Y + (1-\u03b1) Y0 until convergence. Returns (Y, trace).\"\"\"\n",
    "    Y, trace = Y0.copy(), []\n",
    "    for _ in range(max_iter):\n",
    "        Y_new = alpha * (A_norm @ Y) + (1.0 - alpha) * Y0\n",
    "        delta = float(np.abs(Y_new - Y).max())\n",
    "        trace.append(delta)\n",
    "        Y = Y_new\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return Y, trace\n",
    "\n",
    "\n",
    "def run_lp_planetoid(data, n_classes, alpha=0.85, max_iter=200, n_runs=N_RUNS):\n",
    "    \"\"\"LP for Planetoid datasets. Returns (test_accs, val_accs, lp_trace).\"\"\"\n",
    "    A_norm = build_norm_adj(data.edge_index, data.num_nodes)\n",
    "    labels = data.y.numpy().flatten()\n",
    "    train_idx = data.train_mask.numpy().nonzero()[0]\n",
    "    val_idx   = data.val_mask.numpy().nonzero()[0]\n",
    "    test_idx  = data.test_mask.numpy().nonzero()[0]\n",
    "\n",
    "    Y0 = np.zeros((data.num_nodes, n_classes))\n",
    "    Y0[train_idx, labels[train_idx]] = 1.0\n",
    "\n",
    "    Y, trace = label_propagate(A_norm, Y0, alpha=alpha, max_iter=max_iter)\n",
    "    pred = Y.argmax(axis=1)\n",
    "\n",
    "    test_acc = float((pred[test_idx] == labels[test_idx]).mean())\n",
    "    val_acc  = float((pred[val_idx]  == labels[val_idx]).mean())\n",
    "    # LP is deterministic \u2192 replicate result across all runs\n",
    "    return [test_acc] * n_runs, [val_acc] * n_runs, trace\n",
    "\n",
    "\n",
    "def run_lp_proteins(data, split_idx, alpha=0.85, max_iter=100, n_runs=N_RUNS):\n",
    "    \"\"\"Vectorised LP for ogbn-proteins (112 binary tasks). Metric: macro ROC-AUC.\"\"\"\n",
    "    A_norm    = build_norm_adj(data.edge_index, data.num_nodes)\n",
    "    y         = data.y.numpy().astype(float)    # (n, 112)\n",
    "    train_idx = split_idx['train'].numpy()\n",
    "    valid_idx = split_idx['valid'].numpy()\n",
    "    test_idx  = split_idx['test'].numpy()\n",
    "\n",
    "    Y0 = np.zeros_like(y)\n",
    "    Y0[train_idx] = y[train_idx]\n",
    "\n",
    "    Y, trace = label_propagate(A_norm, Y0, alpha=alpha, max_iter=max_iter)\n",
    "\n",
    "    def auc_for(idx):\n",
    "        yt = y[idx].astype(int)\n",
    "        yp = Y[idx]\n",
    "        vc = (yt.sum(0) > 0) & (yt.sum(0) < len(idx))\n",
    "        return roc_auc_score(yt[:, vc], yp[:, vc], average='macro') if vc.sum() > 0 else float('nan')\n",
    "\n",
    "    test_auc = auc_for(test_idx)\n",
    "    val_auc  = auc_for(valid_idx)\n",
    "    return [test_auc] * n_runs, [val_auc] * n_runs, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000006",
   "metadata": {},
   "source": [
    "### Algorithm 2 \u2014 Node2Vec\n",
    "\n",
    "Random-walk-based node embedding (Grover & Leskovec, 2016).  \n",
    "No features are used \u2014 purely structure-based.  \n",
    "Embeddings are trained via the skip-gram objective, then a Logistic Regression classifier is fitted on the training nodes and evaluated on test nodes.  \n",
    "Stochastic across runs (different seeds \u2192 different walk samples and optimisation trajectories) \u2192 non-zero variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node2vec_model(\n",
    "    edge_index, n_nodes,\n",
    "    embedding_dim=128, walk_length=20, context_size=10,\n",
    "    walks_per_node=10, p=1.0, q=1.0,\n",
    "    n_epochs=50, lr=0.01, batch_size=256, seed=0\n",
    "):\n",
    "    \"\"\"Train Node2Vec skip-gram model. Returns (embeddings, epoch_losses).\"\"\"\n",
    "    set_seed(seed)\n",
    "    model = Node2Vec(\n",
    "        edge_index,\n",
    "        embedding_dim=embedding_dim,\n",
    "        walk_length=walk_length,\n",
    "        context_size=context_size,\n",
    "        walks_per_node=walks_per_node,\n",
    "        p=p, q=q,\n",
    "        num_negative_samples=1,\n",
    "        sparse=True,\n",
    "        num_nodes=n_nodes,\n",
    "    ).to(CPU)\n",
    "\n",
    "    loader    = model.loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    optimizer = torch.optim.SparseAdam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw, neg_rw)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += loss.item()\n",
    "        losses.append(total / len(loader))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model().cpu().numpy()\n",
    "    return z, losses\n",
    "\n",
    "\n",
    "def run_node2vec_planetoid(data, n_classes, n_epochs=50, n_runs=N_RUNS):\n",
    "    \"\"\"Node2Vec for Planetoid datasets. Returns (test_accs, val_accs, all_losses).\"\"\"\n",
    "    train_m = data.train_mask.numpy()\n",
    "    val_m   = data.val_mask.numpy()\n",
    "    test_m  = data.test_mask.numpy()\n",
    "    labels  = data.y.numpy().flatten()\n",
    "\n",
    "    test_accs, val_accs, all_losses = [], [], []\n",
    "    for run in range(n_runs):\n",
    "        z, losses = train_node2vec_model(\n",
    "            data.edge_index, data.num_nodes,\n",
    "            embedding_dim=128, walk_length=20, context_size=10,\n",
    "            walks_per_node=10, n_epochs=n_epochs, lr=0.01, seed=run * 7\n",
    "        )\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=run, C=1.0)\n",
    "        clf.fit(z[train_m], labels[train_m])\n",
    "        test_accs.append(clf.score(z[test_m], labels[test_m]))\n",
    "        val_accs.append(clf.score(z[val_m],  labels[val_m]))\n",
    "        all_losses.append(losses)\n",
    "        print(f\"  Node2Vec run {run+1:02d}/{n_runs}: test={test_accs[-1]:.4f}\")\n",
    "    return test_accs, val_accs, all_losses\n",
    "\n",
    "\n",
    "def run_node2vec_proteins(data, split_idx, n_epochs=5, n_runs=N_RUNS):\n",
    "    \"\"\"Node2Vec for ogbn-proteins (large graph \u2014 reduced settings). Metric: ROC-AUC.\"\"\"\n",
    "    y         = data.y.numpy().astype(int)   # (n, 112)\n",
    "    train_idx = split_idx['train'].numpy()\n",
    "    valid_idx = split_idx['valid'].numpy()\n",
    "    test_idx  = split_idx['test'].numpy()\n",
    "\n",
    "    test_aucs, val_aucs, all_losses = [], [], []\n",
    "    for run in range(n_runs):\n",
    "        z, losses = train_node2vec_model(\n",
    "            data.edge_index, data.num_nodes,\n",
    "            embedding_dim=64, walk_length=10, context_size=5,\n",
    "            walks_per_node=2, n_epochs=n_epochs, lr=0.02,\n",
    "            batch_size=512, seed=run * 7\n",
    "        )\n",
    "        # Multilabel: one LR per class\n",
    "        clf = MultiOutputClassifier(\n",
    "            LogisticRegression(max_iter=200, random_state=run, C=0.5), n_jobs=-1\n",
    "        )\n",
    "        clf.fit(z[train_idx], y[train_idx])\n",
    "\n",
    "        def auc_for(idx, z_sub):\n",
    "            probs = np.stack([e.predict_proba(z_sub)[:, 1]\n",
    "                              for e in clf.estimators_], axis=1)\n",
    "            yt = y[idx]\n",
    "            vc = (yt.sum(0) > 0) & (yt.sum(0) < len(idx))\n",
    "            return roc_auc_score(yt[:, vc], probs[:, vc], average='macro')\n",
    "\n",
    "        test_aucs.append(auc_for(test_idx, z[test_idx]))\n",
    "        val_aucs.append(auc_for(valid_idx, z[valid_idx]))\n",
    "        all_losses.append(losses)\n",
    "        print(f\"  Node2Vec proteins run {run+1:02d}/{n_runs}: test_auc={test_aucs[-1]:.4f}\")\n",
    "    return test_aucs, val_aucs, all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000008",
   "metadata": {},
   "source": [
    "### Algorithm 3 \u2014 Graph Convolutional Network (GCN)\n",
    "\n",
    "Standard 2-layer GCN (Kipf & Welling, 2017) for Planetoid datasets.  \n",
    "3-layer GCN with BatchNorm for ogbn-proteins (deeper model needed for harder multilabel task).  \n",
    "**Node features are used** (required for GCN per the assignment).  \n",
    "Stochastic across runs (random weight initialisation + stochastic optimisation) \u2192 non-zero variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"2-layer GCN for multi-class node classification.\"\"\"\n",
    "    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1   = GCNConv(in_ch, hidden_ch)\n",
    "        self.conv2   = GCNConv(hidden_ch, out_ch)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "class GCNProteins(torch.nn.Module):\n",
    "    \"\"\"3-layer GCN with BatchNorm for multilabel binary classification (ogbn-proteins).\"\"\"\n",
    "    def __init__(self, in_ch, hidden_ch, out_ch=112, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_ch, hidden_ch)\n",
    "        self.bn1   = torch.nn.BatchNorm1d(hidden_ch)\n",
    "        self.conv2 = GCNConv(hidden_ch, hidden_ch)\n",
    "        self.bn2   = torch.nn.BatchNorm1d(hidden_ch)\n",
    "        self.conv3 = GCNConv(hidden_ch, out_ch)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.bn1(self.conv1(x, edge_index)).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.bn2(self.conv2(x, edge_index)).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv3(x, edge_index)\n",
    "\n",
    "\n",
    "def run_gcn_planetoid(data, n_classes, hidden=256, n_epochs=200,\n",
    "                      lr=1e-2, wd=5e-4, n_runs=N_RUNS):\n",
    "    \"\"\"Train 2-layer GCN on a Planetoid dataset. Returns (test_accs, histories).\"\"\"\n",
    "    data_dev = data.to(GCN_DEVICE)\n",
    "    all_test, all_hist = [], []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        set_seed(run * 13)\n",
    "        model = GCN(data.num_node_features, hidden, n_classes).to(GCN_DEVICE)\n",
    "        opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        crit  = torch.nn.CrossEntropyLoss()\n",
    "        hist  = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out  = model(data_dev.x, data_dev.edge_index)\n",
    "            loss = crit(out[data_dev.train_mask], data_dev.y[data_dev.train_mask].squeeze())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out      = model(data_dev.x, data_dev.edge_index)\n",
    "                val_loss = crit(out[data_dev.val_mask],\n",
    "                                data_dev.y[data_dev.val_mask].squeeze()).item()\n",
    "                val_pred = out[data_dev.val_mask].argmax(1)\n",
    "                val_acc  = (val_pred == data_dev.y[data_dev.val_mask].squeeze()\n",
    "                            ).float().mean().item()\n",
    "            hist['train_loss'].append(loss.item())\n",
    "            hist['val_loss'].append(val_loss)\n",
    "            hist['val_acc'].append(val_acc)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out  = model(data_dev.x, data_dev.edge_index)\n",
    "            pred = out[data_dev.test_mask].argmax(1)\n",
    "            test_acc = (pred == data_dev.y[data_dev.test_mask].squeeze()\n",
    "                        ).float().mean().item()\n",
    "        all_test.append(test_acc)\n",
    "        all_hist.append(hist)\n",
    "        print(f\"  GCN run {run+1:02d}/{n_runs}: test={test_acc:.4f}\")\n",
    "\n",
    "    data_dev.to(CPU)\n",
    "    return all_test, all_hist\n",
    "\n",
    "\n",
    "def run_gcn_proteins(data, split_idx, hidden=256, n_epochs=100, lr=1e-2, n_runs=N_RUNS):\n",
    "    \"\"\"Train 3-layer GCN on ogbn-proteins. Metric: macro ROC-AUC.\"\"\"\n",
    "    data_dev  = data.to(GCN_DEVICE)\n",
    "    y_dev     = data.y.float().to(GCN_DEVICE)\n",
    "    train_idx = split_idx['train'].to(GCN_DEVICE)\n",
    "    valid_idx = split_idx['valid'].to(GCN_DEVICE)\n",
    "    test_idx  = split_idx['test'].to(GCN_DEVICE)\n",
    "    all_test, all_hist = [], []\n",
    "\n",
    "    for run in range(n_runs):\n",
    "        set_seed(run * 13)\n",
    "        model = GCNProteins(data.x.size(1), hidden, out_ch=112).to(GCN_DEVICE)\n",
    "        opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        crit  = torch.nn.BCEWithLogitsLoss()\n",
    "        hist  = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out  = model(data_dev.x, data_dev.edge_index)\n",
    "            loss = crit(out[train_idx], y_dev[train_idx])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out      = model(data_dev.x, data_dev.edge_index)\n",
    "                val_loss = crit(out[valid_idx], y_dev[valid_idx]).item()\n",
    "            hist['train_loss'].append(loss.item())\n",
    "            hist['val_loss'].append(val_loss)\n",
    "\n",
    "        # Final evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data_dev.x, data_dev.edge_index)\n",
    "        probs  = torch.sigmoid(out[test_idx]).cpu().numpy()\n",
    "        y_test = data.y[split_idx['test'].cpu()].numpy().astype(int)\n",
    "        vc     = (y_test.sum(0) > 0) & (y_test.sum(0) < len(y_test))\n",
    "        test_auc = roc_auc_score(y_test[:, vc], probs[:, vc], average='macro')\n",
    "        all_test.append(test_auc)\n",
    "        all_hist.append(hist)\n",
    "        print(f\"  GCN proteins run {run+1:02d}/{n_runs}: test_auc={test_auc:.4f}\")\n",
    "\n",
    "    data_dev.to(CPU)\n",
    "    return all_test, all_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Run All Experiments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "#\n",
    "# results[dataset][algorithm] = {\n",
    "#   'test'    : list of N_RUNS test scores,\n",
    "#   'val'     : list of N_RUNS val  scores,   (LP / Node2Vec)\n",
    "#   'history' : list of N_RUNS history dicts, (Node2Vec, GCN)\n",
    "#   'lp_trace': convergence trace,            (LP only)\n",
    "# }\n",
    "\n",
    "results = {}\n",
    "\n",
    "# \u2500\u2500 Planetoid \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "for ds_name in ['cora', 'citeseer', 'pubmed']:\n",
    "    ds        = planetoid_datasets[ds_name]\n",
    "    data      = ds[0]\n",
    "    n_classes = ds.num_classes\n",
    "    print(f\"\\n{'='*60}\\n{ds_name.upper()}  ({n_classes} classes)\\n{'='*60}\")\n",
    "    results[ds_name] = {}\n",
    "\n",
    "    # Label Propagation\n",
    "    print(\"\\n\u2500\u2500 Label Propagation \u2500\u2500\")\n",
    "    test, val, trace = run_lp_planetoid(data, n_classes)\n",
    "    results[ds_name]['LP'] = {'test': test, 'val': val, 'lp_trace': trace}\n",
    "    print(f\"  LP  test acc : {np.mean(test):.4f} \u00b1 {np.std(test):.4f}\")\n",
    "\n",
    "    # Node2Vec\n",
    "    print(\"\\n\u2500\u2500 Node2Vec \u2500\u2500\")\n",
    "    test, val, hists = run_node2vec_planetoid(data, n_classes, n_epochs=50)\n",
    "    results[ds_name]['Node2Vec'] = {'test': test, 'val': val, 'history': hists}\n",
    "    print(f\"  N2V test acc : {np.mean(test):.4f} \u00b1 {np.std(test):.4f}\")\n",
    "\n",
    "    # GCN\n",
    "    print(\"\\n\u2500\u2500 GCN \u2500\u2500\")\n",
    "    test, hists = run_gcn_planetoid(data, n_classes, n_epochs=200)\n",
    "    results[ds_name]['GCN'] = {'test': test, 'history': hists}\n",
    "    print(f\"  GCN test acc : {np.mean(test):.4f} \u00b1 {np.std(test):.4f}\")\n",
    "\n",
    "# \u2500\u2500 ogbn-proteins \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(f\"\\n{'='*60}\\nOGBN-PROTEINS  (metric: macro ROC-AUC)\\n{'='*60}\")\n",
    "results['ogbn-proteins'] = {}\n",
    "\n",
    "print(\"\\n\u2500\u2500 Label Propagation \u2500\u2500\")\n",
    "test, val, trace = run_lp_proteins(data_proteins, split_idx)\n",
    "results['ogbn-proteins']['LP'] = {'test': test, 'val': val, 'lp_trace': trace}\n",
    "print(f\"  LP  test AUC : {np.mean(test):.4f} \u00b1 {np.std(test):.4f}\")\n",
    "\n",
    "print(\"\\n\u2500\u2500 Node2Vec \u2500\u2500\")\n",
    "test, val, hists = run_node2vec_proteins(data_proteins, split_idx, n_epochs=5)\n",
    "results['ogbn-proteins']['Node2Vec'] = {'test': test, 'val': val, 'history': hists}\n",
    "print(f\"  N2V test AUC : {np.mean(test):.4f} \u00b1 {np.std(test):.4f}\")\n",
    "\n",
    "print(\"\\n\u2500\u2500 GCN \u2500\u2500\")\n",
    "test, hists = run_gcn_proteins(data_proteins, split_idx, n_epochs=100)\n",
    "results['ogbn-proteins']['GCN'] = {'test': test, 'history': hists}\n",
    "print(f\"  GCN test AUC : {np.mean(test):.4f} \u00b1 {np.std(test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Training Curves \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# One figure per dataset, three panels:\n",
    "#   Left  : LP convergence (max label change per iteration, log-scale)\n",
    "#   Centre: Node2Vec training loss (skip-gram, mean \u00b1 std across runs)\n",
    "#   Right : GCN train loss and validation loss (mean \u00b1 std across runs)\n",
    "\n",
    "def plot_training_curves(ds_name, res, y_label_gcn='Cross-Entropy Loss'):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(f'{ds_name.upper()} \u2014 Training Curves', fontsize=13, fontweight='bold')\n",
    "\n",
    "    # \u2500\u2500 LP convergence \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    ax    = axes[0]\n",
    "    trace = res['LP']['lp_trace']\n",
    "    ax.semilogy(range(1, len(trace) + 1), trace, color='steelblue', lw=2)\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Max Label Change (log scale)')\n",
    "    ax.set_title('Label Propagation \u2014 Convergence')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # \u2500\u2500 Node2Vec skip-gram loss \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    ax     = axes[1]\n",
    "    losses = np.array(res['Node2Vec']['history'])   # (n_runs, n_epochs)\n",
    "    mean_l = losses.mean(0)\n",
    "    std_l  = losses.std(0)\n",
    "    ep     = range(1, len(mean_l) + 1)\n",
    "    ax.plot(ep, mean_l, color='darkorange', lw=2, label='train loss (mean)')\n",
    "    ax.fill_between(ep, mean_l - std_l, mean_l + std_l, alpha=0.25, color='darkorange',\n",
    "                    label='\u00b11 std')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Skip-gram Loss')\n",
    "    ax.set_title('Node2Vec \u2014 Training Loss')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # \u2500\u2500 GCN train + val loss \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    ax  = axes[2]\n",
    "    tl  = np.array([h['train_loss'] for h in res['GCN']['history']])\n",
    "    vl  = np.array([h['val_loss']   for h in res['GCN']['history']])\n",
    "    ep  = range(1, tl.shape[1] + 1)\n",
    "    ax.plot(ep, tl.mean(0), color='forestgreen', lw=2, label='train loss')\n",
    "    ax.fill_between(ep, tl.mean(0) - tl.std(0), tl.mean(0) + tl.std(0),\n",
    "                    alpha=0.2, color='forestgreen')\n",
    "    ax.plot(ep, vl.mean(0), color='crimson', lw=2, label='val loss')\n",
    "    ax.fill_between(ep, vl.mean(0) - vl.std(0), vl.mean(0) + vl.std(0),\n",
    "                    alpha=0.2, color='crimson')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(y_label_gcn)\n",
    "    ax.set_title('GCN \u2014 Train / Val Loss')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_curves_{ds_name}.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for ds_name in ['cora', 'citeseer', 'pubmed']:\n",
    "    plot_training_curves(ds_name, results[ds_name])\n",
    "\n",
    "plot_training_curves('ogbn-proteins', results['ogbn-proteins'], y_label_gcn='BCE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Per-Dataset Result Tables \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    metric = 'ROC-AUC' if ds_name == 'ogbn-proteins' else 'Accuracy'\n",
    "    rows   = []\n",
    "    for run in range(N_RUNS):\n",
    "        row = {'Run': run + 1}\n",
    "        for algo in ['LP', 'Node2Vec', 'GCN']:\n",
    "            row[algo] = f\"{results[ds_name][algo]['test'][run]:.4f}\"\n",
    "        rows.append(row)\n",
    "\n",
    "    df_per = pd.DataFrame(rows).set_index('Run')\n",
    "    # Append mean and std rows\n",
    "    mean_row = {algo: f\"{np.mean(results[ds_name][algo]['test']):.4f}\"\n",
    "                for algo in ['LP', 'Node2Vec', 'GCN']}\n",
    "    std_row  = {algo: f\"{np.std(results[ds_name][algo]['test']):.4f}\"\n",
    "                for algo in ['LP', 'Node2Vec', 'GCN']}\n",
    "    df_per.loc['mean'] = mean_row\n",
    "    df_per.loc['std']  = std_row\n",
    "\n",
    "    print(f\"\\n{'\u2500'*55}\")\n",
    "    print(f\" {ds_name.upper()}  |  Test {metric} per run\")\n",
    "    print(f\"{'\u2500'*55}\")\n",
    "    print(df_per.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Summary Table (all datasets \u00d7 all algorithms) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "summary_rows = []\n",
    "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']:\n",
    "    metric = 'ROC-AUC' if ds_name == 'ogbn-proteins' else 'Accuracy'\n",
    "    for algo in ['LP', 'Node2Vec', 'GCN']:\n",
    "        vals = results[ds_name][algo]['test']\n",
    "        summary_rows.append({\n",
    "            'Dataset': ds_name, 'Algorithm': algo, 'Metric': metric,\n",
    "            'Mean': np.mean(vals), 'Std': np.std(vals),\n",
    "            'Result': f\"{np.mean(vals):.4f} \u00b1 {np.std(vals):.4f}\"\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "pivot = df_summary.pivot_table(values='Result', index='Dataset',\n",
    "                                columns='Algorithm', aggfunc='first')[['LP', 'Node2Vec', 'GCN']]\n",
    "pivot.index.name = 'Dataset'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Q1 \u2014 Node Classification: Test Performance (mean \u00b1 std, 10 runs)\")\n",
    "print(\"Metric: Accuracy for Cora/CiteSeer/PubMed; ROC-AUC for ogbn-proteins\")\n",
    "print(\"=\"*70)\n",
    "print(pivot.to_string())\n",
    "\n",
    "# \u2500\u2500 Grouped Bar Plot \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "ds_list = ['cora', 'citeseer', 'pubmed', 'ogbn-proteins']\n",
    "titles  = ['Cora\\n(Accuracy)', 'CiteSeer\\n(Accuracy)',\n",
    "           'PubMed\\n(Accuracy)', 'ogbn-proteins\\n(ROC-AUC)']\n",
    "algos   = ['LP', 'Node2Vec', 'GCN']\n",
    "colors  = ['steelblue', 'darkorange', 'forestgreen']\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(17, 5), sharey=False)\n",
    "for ax, ds_name, title in zip(axes, ds_list, titles):\n",
    "    means = [np.mean(results[ds_name][a]['test']) for a in algos]\n",
    "    stds  = [np.std(results[ds_name][a]['test'])  for a in algos]\n",
    "    bars  = ax.bar(algos, means, yerr=stds, capsize=7,\n",
    "                   color=colors, alpha=0.82, edgecolor='black', lw=0.7)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_ylim(0, min(1.12, max(means) + max(stds) + 0.15))\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    for bar, m, s in zip(bars, means, stds):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "                m + s + 0.01, f'{m:.3f}',\n",
    "                ha='center', va='bottom', fontsize=8.5)\n",
    "\n",
    "fig.suptitle('Q1 \u2014 Node Classification: Algorithm Comparison across Datasets',\n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('q1_comparison_barplot.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (A3 venv)",
   "language": "python",
   "name": "a3_venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}