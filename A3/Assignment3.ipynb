{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 3 - COMP 511"
      ],
      "id": "33373b06"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q1 — Node Classification [50%]\n",
        "\n",
        "Three algorithms compared on Cora, CiteSeer, PubMed, and ogbn-arxiv (all measured by accuracy):\n",
        "\n",
        "| Algorithm | Features used | Notes |\n",
        "|-----------|--------------|-------|\n",
        "| **Label Propagation** | None (structure only) | Deterministic — std = 0 across runs |\n",
        "| **Node2Vec** | None (structure only) | Embeddings → Logistic Regression |\n",
        "| **GCN** | Node features (required) | Kipf & Welling (2017) |\n",
        "\n",
        "**Splits**: Planetoid protocol for Cora/CiteSeer/PubMed (`torch_geometric.datasets.Planetoid`), official OGB split for ogbn-arxiv.\n",
        "\n",
        "**Results reported**: mean ± std over 10 independent runs."
      ],
      "id": "a1000001"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Imports ──────────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "\n",
        "# PyTorch ≥2.6 defaults weights_only=True in torch.load, which breaks OGB/PyG\n",
        "# dataset caching (they pickle custom Data classes). Restore the old default.\n",
        "# Store the true original on the module so re-running this cell is always safe.\n",
        "if not hasattr(torch, '_original_load'):\n",
        "    torch._original_load = torch.load\n",
        "def _patched_load(*args, **kwargs):\n",
        "    kwargs.setdefault('weights_only', False)\n",
        "    return torch._original_load(*args, **kwargs)\n",
        "torch.load = _patched_load\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from scipy.sparse import csr_matrix, diags\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, Node2Vec\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "# Device: prefer MPS (Apple Silicon) > CUDA > CPU\n",
        "if torch.backends.mps.is_available():\n",
        "    GCN_DEVICE = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    GCN_DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    GCN_DEVICE = torch.device('cpu')\n",
        "CPU = torch.device('cpu')\n",
        "print(f\"GCN device: {GCN_DEVICE}  |  Node2Vec/LP device: cpu\")\n",
        "\n",
        "N_RUNS = 10\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GCN device: mps  |  Node2Vec/LP device: cpu\n"
          ]
        }
      ],
      "id": "a1000002"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Dataset Loading ───────────────────────────────────────────────────────────\n",
        "#\n",
        "# Planetoid (Cora / CiteSeer / PubMed): auto-downloaded by torch_geometric.\n",
        "# Uses the Planetoid split protocol as required by the assignment.\n",
        "#\n",
        "# ogbn-arxiv: auto-downloaded by OGB. Uses official year-based split.\n",
        "\n",
        "planetoid_datasets = {}\n",
        "for name in ['Cora', 'CiteSeer', 'PubMed']:\n",
        "    ds = Planetoid(root=f'data/{name}', name=name, transform=T.NormalizeFeatures())\n",
        "    planetoid_datasets[name.lower()] = ds\n",
        "    d = ds[0]\n",
        "    print(f\"{name:10s}: {d.num_nodes:6d} nodes  {d.num_edges:6d} edges  \"\n",
        "          f\"{ds.num_features:4d} features  {ds.num_classes} classes  \"\n",
        "          f\"train={d.train_mask.sum().item()} val={d.val_mask.sum().item()} test={d.test_mask.sum().item()}\")\n",
        "\n",
        "# ogbn-arxiv — multiclass (40 CS subject areas), 128-dim node features\n",
        "dataset_arxiv = PygNodePropPredDataset(name='ogbn-arxiv', root='data/ogbn-arxiv')\n",
        "split_idx     = dataset_arxiv.get_idx_split()\n",
        "data_arxiv    = dataset_arxiv[0]\n",
        "n_arxiv_classes = int(data_arxiv.y.max()) + 1\n",
        "\n",
        "print(f\"\\nogbn-arxiv: {data_arxiv.num_nodes:,} nodes  {data_arxiv.num_edges:,} edges  \"\n",
        "      f\"x={tuple(data_arxiv.x.shape)}  y classes={n_arxiv_classes}\")\n",
        "print(f\"  train={len(split_idx['train']):,}  valid={len(split_idx['valid']):,}  \"\n",
        "      f\"test={len(split_idx['test']):,}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cora      :   2708 nodes   10556 edges  1433 features  7 classes  train=140 val=500 test=1000\n",
            "CiteSeer  :   3327 nodes    9104 edges  3703 features  6 classes  train=120 val=500 test=1000\n",
            "PubMed    :  19717 nodes   88648 edges   500 features  3 classes  train=60 val=500 test=1000\n",
            "\n",
            "ogbn-arxiv: 169,343 nodes  1,166,243 edges  x=(169343, 128)  y classes=40\n",
            "  train=90,941  valid=29,799  test=48,603\n"
          ]
        }
      ],
      "id": "a1000003"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm 1 — Label Propagation\n",
        "\n",
        "Classic semi-supervised LP (Zhu et al., 2003).  \n",
        "No features are used — purely structure-based.  \n",
        "Update rule: **Y ← α D⁻¹A Y + (1−α) Y₀**, iterated until convergence.  \n",
        "Because the algorithm is deterministic given the graph and fixed splits, all 10 runs return identical results (std = 0)."
      ],
      "id": "a1000004"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_norm_adj(edge_index: torch.Tensor, n_nodes: int):\n",
        "    \"\"\"Row-normalised adjacency D⁻¹A (scipy CSR, symmetric).\"\"\"\n",
        "    r, c = edge_index.numpy()\n",
        "    rows = np.concatenate([r, c])\n",
        "    cols = np.concatenate([c, r])\n",
        "    adj  = csr_matrix((np.ones(len(rows)), (rows, cols)), shape=(n_nodes, n_nodes))\n",
        "    adj.data[:] = 1.0                          # binarise duplicate entries\n",
        "    deg  = np.array(adj.sum(axis=1)).flatten()\n",
        "    deg[deg == 0] = 1\n",
        "    return diags(1.0 / deg) @ adj              # row-stochastic\n",
        "\n",
        "\n",
        "def label_propagate(A_norm, Y0: np.ndarray, alpha=0.85, max_iter=200, tol=1e-6):\n",
        "    \"\"\"Iterate Y ← α A_norm Y + (1-α) Y0 until convergence. Returns (Y, trace).\"\"\"\n",
        "    Y, trace = Y0.copy(), []\n",
        "    for _ in range(max_iter):\n",
        "        Y_new = alpha * (A_norm @ Y) + (1.0 - alpha) * Y0\n",
        "        delta = float(np.abs(Y_new - Y).max())\n",
        "        trace.append(delta)\n",
        "        Y = Y_new\n",
        "        if delta < tol:\n",
        "            break\n",
        "    return Y, trace\n",
        "\n",
        "\n",
        "def run_lp_planetoid(data, n_classes, alpha=0.85, max_iter=200, n_runs=N_RUNS):\n",
        "    \"\"\"LP for Planetoid datasets. Returns (test_accs, val_accs, lp_trace).\"\"\"\n",
        "    A_norm = build_norm_adj(data.edge_index, data.num_nodes)\n",
        "    labels = data.y.numpy().flatten()\n",
        "    train_idx = data.train_mask.numpy().nonzero()[0]\n",
        "    val_idx   = data.val_mask.numpy().nonzero()[0]\n",
        "    test_idx  = data.test_mask.numpy().nonzero()[0]\n",
        "\n",
        "    Y0 = np.zeros((data.num_nodes, n_classes))\n",
        "    Y0[train_idx, labels[train_idx]] = 1.0\n",
        "\n",
        "    Y, trace = label_propagate(A_norm, Y0, alpha=alpha, max_iter=max_iter)\n",
        "    pred = Y.argmax(axis=1)\n",
        "\n",
        "    test_acc = float((pred[test_idx] == labels[test_idx]).mean())\n",
        "    val_acc  = float((pred[val_idx]  == labels[val_idx]).mean())\n",
        "    # LP is deterministic → replicate result across all runs\n",
        "    return [test_acc] * n_runs, [val_acc] * n_runs, trace\n",
        "\n",
        "\n",
        "def run_lp_ogb(data, split_idx, n_classes, alpha=0.85, max_iter=200, n_runs=N_RUNS):\n",
        "    \"\"\"LP for OGB datasets (multiclass). Metric: accuracy.\"\"\"\n",
        "    A_norm    = build_norm_adj(data.edge_index, data.num_nodes)\n",
        "    labels    = data.y.numpy().flatten()\n",
        "    train_idx = split_idx['train'].numpy()\n",
        "    valid_idx = split_idx['valid'].numpy()\n",
        "    test_idx  = split_idx['test'].numpy()\n",
        "\n",
        "    Y0 = np.zeros((data.num_nodes, n_classes))\n",
        "    Y0[train_idx, labels[train_idx]] = 1.0\n",
        "\n",
        "    Y, trace = label_propagate(A_norm, Y0, alpha=alpha, max_iter=max_iter)\n",
        "    pred = Y.argmax(axis=1)\n",
        "\n",
        "    test_acc = float((pred[test_idx] == labels[test_idx]).mean())\n",
        "    val_acc  = float((pred[valid_idx] == labels[valid_idx]).mean())\n",
        "    return [test_acc] * n_runs, [val_acc] * n_runs, trace"
      ],
      "execution_count": 21,
      "outputs": [],
      "id": "a1000005"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm 2 — Node2Vec\n",
        "\n",
        "Random-walk-based node embedding (Grover & Leskovec, 2016).  \n",
        "No features are used — purely structure-based.  \n",
        "Embeddings are trained via the skip-gram objective, then a Logistic Regression classifier is fitted on the training nodes and evaluated on test nodes.  \n",
        "Stochastic across runs (different seeds → different walk samples and optimisation trajectories) → non-zero variance."
      ],
      "id": "a1000006"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_node2vec_model(\n",
        "    edge_index, n_nodes,\n",
        "    embedding_dim=128, walk_length=20, context_size=10,\n",
        "    walks_per_node=10, p=1.0, q=1.0,\n",
        "    n_epochs=50, lr=0.01, batch_size=256, seed=0\n",
        "):\n",
        "    \"\"\"Train Node2Vec skip-gram model. Returns (embeddings, epoch_losses).\"\"\"\n",
        "    set_seed(seed)\n",
        "    model = Node2Vec(\n",
        "        edge_index,\n",
        "        embedding_dim=embedding_dim,\n",
        "        walk_length=walk_length,\n",
        "        context_size=context_size,\n",
        "        walks_per_node=walks_per_node,\n",
        "        p=p, q=q,\n",
        "        num_negative_samples=1,\n",
        "        sparse=True,\n",
        "        num_nodes=n_nodes,\n",
        "    ).to(CPU)\n",
        "\n",
        "    loader    = model.loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    optimizer = torch.optim.SparseAdam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        for pos_rw, neg_rw in loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.loss(pos_rw, neg_rw)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total += loss.item()\n",
        "        losses.append(total / len(loader))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model().cpu().numpy()\n",
        "    return z, losses\n",
        "\n",
        "\n",
        "def run_node2vec_planetoid(data, n_classes, n_epochs=50, n_runs=N_RUNS):\n",
        "    \"\"\"Node2Vec for Planetoid datasets. Returns (test_accs, val_accs, all_losses).\"\"\"\n",
        "    train_m = data.train_mask.numpy()\n",
        "    val_m   = data.val_mask.numpy()\n",
        "    test_m  = data.test_mask.numpy()\n",
        "    labels  = data.y.numpy().flatten()\n",
        "\n",
        "    test_accs, val_accs, all_losses = [], [], []\n",
        "    for run in range(n_runs):\n",
        "        z, losses = train_node2vec_model(\n",
        "            data.edge_index, data.num_nodes,\n",
        "            embedding_dim=128, walk_length=20, context_size=10,\n",
        "            walks_per_node=10, n_epochs=n_epochs, lr=0.01, seed=run * 7\n",
        "        )\n",
        "        clf = LogisticRegression(max_iter=1000, random_state=run, C=1.0)\n",
        "        clf.fit(z[train_m], labels[train_m])\n",
        "        test_accs.append(clf.score(z[test_m], labels[test_m]))\n",
        "        val_accs.append(clf.score(z[val_m],  labels[val_m]))\n",
        "        all_losses.append(losses)\n",
        "        print(f\"  Node2Vec run {run+1:02d}/{n_runs}: test={test_accs[-1]:.4f}\")\n",
        "    return test_accs, val_accs, all_losses\n",
        "\n",
        "\n",
        "def run_node2vec_ogb(data, split_idx, n_epochs=10, n_runs=N_RUNS):\n",
        "    \"\"\"Node2Vec for OGB datasets (multiclass). Metric: accuracy.\"\"\"\n",
        "    labels    = data.y.numpy().flatten()\n",
        "    train_idx = split_idx['train'].numpy()\n",
        "    valid_idx = split_idx['valid'].numpy()\n",
        "    test_idx  = split_idx['test'].numpy()\n",
        "\n",
        "    test_accs, val_accs, all_losses = [], [], []\n",
        "    for run in range(n_runs):\n",
        "        z, losses = train_node2vec_model(\n",
        "            data.edge_index, data.num_nodes,\n",
        "            embedding_dim=128, walk_length=20, context_size=10,\n",
        "            walks_per_node=5, n_epochs=n_epochs, lr=0.01,\n",
        "            batch_size=512, seed=run * 7\n",
        "        )\n",
        "        clf = LogisticRegression(max_iter=500, random_state=run, C=1.0)\n",
        "        clf.fit(z[train_idx], labels[train_idx])\n",
        "        test_accs.append(clf.score(z[test_idx], labels[test_idx]))\n",
        "        val_accs.append(clf.score(z[valid_idx], labels[valid_idx]))\n",
        "        all_losses.append(losses)\n",
        "        print(f\"  Node2Vec ogb run {run+1:02d}/{n_runs}: test={test_accs[-1]:.4f}\")\n",
        "    return test_accs, val_accs, all_losses"
      ],
      "execution_count": 22,
      "outputs": [],
      "id": "a1000007"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm 3 — Graph Convolutional Network (GCN)\n",
        "\n",
        "Standard 2-layer GCN (Kipf & Welling, 2017) for Planetoid datasets.  \n",
        "3-layer GCN with BatchNorm for ogbn-arxiv (larger graph benefits from deeper model).  \n",
        "**Node features are used** (required for GCN per the assignment).  \n",
        "Stochastic across runs (random weight initialisation + stochastic optimisation) → non-zero variance."
      ],
      "id": "a1000008"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"2-layer GCN for multi-class node classification.\"\"\"\n",
        "    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1   = GCNConv(in_ch, hidden_ch)\n",
        "        self.conv2   = GCNConv(hidden_ch, out_ch)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "\n",
        "class GCN3(torch.nn.Module):\n",
        "    \"\"\"3-layer GCN with BatchNorm for larger graphs (e.g. ogbn-arxiv).\"\"\"\n",
        "    def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_ch, hidden_ch)\n",
        "        self.bn1   = torch.nn.BatchNorm1d(hidden_ch)\n",
        "        self.conv2 = GCNConv(hidden_ch, hidden_ch)\n",
        "        self.bn2   = torch.nn.BatchNorm1d(hidden_ch)\n",
        "        self.conv3 = GCNConv(hidden_ch, out_ch)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.bn1(self.conv1(x, edge_index)).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.bn2(self.conv2(x, edge_index)).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv3(x, edge_index)\n",
        "\n",
        "\n",
        "def run_gcn_planetoid(data, n_classes, hidden=256, n_epochs=200,\n",
        "                      lr=1e-2, wd=5e-4, n_runs=N_RUNS):\n",
        "    \"\"\"Train 2-layer GCN on a Planetoid dataset. Returns (test_accs, histories).\"\"\"\n",
        "    data_dev = data.to(GCN_DEVICE)\n",
        "    all_test, all_hist = [], []\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        set_seed(run * 13)\n",
        "        model = GCN(data.num_node_features, hidden, n_classes).to(GCN_DEVICE)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        crit  = torch.nn.CrossEntropyLoss()\n",
        "        hist  = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            loss = crit(out[data_dev.train_mask], data_dev.y[data_dev.train_mask].squeeze())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out      = model(data_dev.x, data_dev.edge_index)\n",
        "                val_loss = crit(out[data_dev.val_mask],\n",
        "                                data_dev.y[data_dev.val_mask].squeeze()).item()\n",
        "                val_pred = out[data_dev.val_mask].argmax(1)\n",
        "                val_acc  = (val_pred == data_dev.y[data_dev.val_mask].squeeze()\n",
        "                            ).float().mean().item()\n",
        "            hist['train_loss'].append(loss.item())\n",
        "            hist['val_loss'].append(val_loss)\n",
        "            hist['val_acc'].append(val_acc)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            pred = out[data_dev.test_mask].argmax(1)\n",
        "            test_acc = (pred == data_dev.y[data_dev.test_mask].squeeze()\n",
        "                        ).float().mean().item()\n",
        "        all_test.append(test_acc)\n",
        "        all_hist.append(hist)\n",
        "        print(f\"  GCN run {run+1:02d}/{n_runs}: test={test_acc:.4f}\")\n",
        "\n",
        "    data_dev.to(CPU)\n",
        "    return all_test, all_hist\n",
        "\n",
        "\n",
        "def run_gcn_ogb(data, split_idx, n_classes, hidden=256, n_epochs=200,\n",
        "                lr=1e-2, wd=5e-4, n_runs=N_RUNS):\n",
        "    \"\"\"Train 3-layer GCN on an OGB dataset (multiclass). Metric: accuracy.\"\"\"\n",
        "    data_dev  = data.to(GCN_DEVICE)\n",
        "    train_idx = split_idx['train'].to(GCN_DEVICE)\n",
        "    valid_idx = split_idx['valid'].to(GCN_DEVICE)\n",
        "    test_idx  = split_idx['test'].to(GCN_DEVICE)\n",
        "    labels    = data.y.squeeze().to(GCN_DEVICE)\n",
        "    all_test, all_hist = [], []\n",
        "\n",
        "    for run in range(n_runs):\n",
        "        set_seed(run * 13)\n",
        "        model = GCN3(data.x.size(1), hidden, n_classes).to(GCN_DEVICE)\n",
        "        opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        crit  = torch.nn.CrossEntropyLoss()\n",
        "        hist  = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()\n",
        "            opt.zero_grad()\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            loss = crit(out[train_idx], labels[train_idx])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                out      = model(data_dev.x, data_dev.edge_index)\n",
        "                val_loss = crit(out[valid_idx], labels[valid_idx]).item()\n",
        "                val_pred = out[valid_idx].argmax(1)\n",
        "                val_acc  = (val_pred == labels[valid_idx]).float().mean().item()\n",
        "            hist['train_loss'].append(loss.item())\n",
        "            hist['val_loss'].append(val_loss)\n",
        "            hist['val_acc'].append(val_acc)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out  = model(data_dev.x, data_dev.edge_index)\n",
        "            pred = out[test_idx].argmax(1)\n",
        "            test_acc = (pred == labels[test_idx]).float().mean().item()\n",
        "        all_test.append(test_acc)\n",
        "        all_hist.append(hist)\n",
        "        print(f\"  GCN ogb run {run+1:02d}/{n_runs}: test={test_acc:.4f}\")\n",
        "\n",
        "    data_dev.to(CPU)\n",
        "    return all_test, all_hist"
      ],
      "execution_count": 23,
      "outputs": [],
      "id": "a1000009"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Run All Experiments ───────────────────────────────────────────────────────\n",
        "#\n",
        "# results[dataset][algorithm] = {\n",
        "#   'test'    : list of N_RUNS test scores,\n",
        "#   'val'     : list of N_RUNS val  scores,   (LP / Node2Vec)\n",
        "#   'history' : list of N_RUNS history dicts, (Node2Vec, GCN)\n",
        "#   'lp_trace': convergence trace,            (LP only)\n",
        "# }\n",
        "\n",
        "results = {}\n",
        "\n",
        "# ── Planetoid ─────────────────────────────────────────────────────────────────\n",
        "for ds_name in ['pubmed']:\n",
        "    ds        = planetoid_datasets[ds_name]\n",
        "    data      = ds[0]\n",
        "    n_classes = ds.num_classes\n",
        "    print(f\"\\n{'='*60}\\n{ds_name.upper()}  ({n_classes} classes)\\n{'='*60}\")\n",
        "    results[ds_name] = {}\n",
        "\n",
        "    # Label Propagation\n",
        "    print(\"\\n── Label Propagation ──\")\n",
        "    test, val, trace = run_lp_planetoid(data, n_classes)\n",
        "    results[ds_name]['LP'] = {'test': test, 'val': val, 'lp_trace': trace}\n",
        "    print(f\"  LP  test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "    # Node2Vec\n",
        "    print(\"\\n── Node2Vec ──\")\n",
        "    test, val, hists = run_node2vec_planetoid(data, n_classes, n_epochs=50)\n",
        "    results[ds_name]['Node2Vec'] = {'test': test, 'val': val, 'history': hists}\n",
        "    print(f\"  N2V test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "    # GCN\n",
        "    print(\"\\n── GCN ──\")\n",
        "    test, hists = run_gcn_planetoid(data, n_classes, n_epochs=200)\n",
        "    results[ds_name]['GCN'] = {'test': test, 'history': hists}\n",
        "    print(f\"  GCN test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "# ── ogbn-arxiv ────────────────────────────────────────────────────────────────\n",
        "print(f\"\\n{'='*60}\\nOGBN-ARXIV  ({n_arxiv_classes} classes, metric: Accuracy)\\n{'='*60}\")\n",
        "results['ogbn-arxiv'] = {}\n",
        "\n",
        "print(\"\\n── Label Propagation ──\")\n",
        "test, val, trace = run_lp_ogb(data_arxiv, split_idx, n_arxiv_classes)\n",
        "results['ogbn-arxiv']['LP'] = {'test': test, 'val': val, 'lp_trace': trace}\n",
        "print(f\"  LP  test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "print(\"\\n── Node2Vec ──\")\n",
        "test, val, hists = run_node2vec_ogb(data_arxiv, split_idx, n_epochs=10)\n",
        "results['ogbn-arxiv']['Node2Vec'] = {'test': test, 'val': val, 'history': hists}\n",
        "print(f\"  N2V test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")\n",
        "\n",
        "print(\"\\n── GCN ──\")\n",
        "test, hists = run_gcn_ogb(data_arxiv, split_idx, n_arxiv_classes, n_epochs=200)\n",
        "results['ogbn-arxiv']['GCN'] = {'test': test, 'history': hists}\n",
        "print(f\"  GCN test acc : {np.mean(test):.4f} ± {np.std(test):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PUBMED  (3 classes)\n",
            "============================================================\n",
            "\n",
            "── Label Propagation ──\n",
            "  LP  test acc : 0.6990 ± 0.0000\n",
            "\n",
            "── Node2Vec ──\n"
          ]
        }
      ],
      "id": "a1000010"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Training Curves ───────────────────────────────────────────────────────────\n",
        "# One figure per dataset, three panels:\n",
        "#   Left  : LP convergence (max label change per iteration, log-scale)\n",
        "#   Centre: Node2Vec training loss (skip-gram, mean ± std across runs)\n",
        "#   Right : GCN train loss and validation loss (mean ± std across runs)\n",
        "\n",
        "def plot_training_curves(ds_name, res, y_label_gcn='Cross-Entropy Loss'):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "    fig.suptitle(f'{ds_name.upper()} — Training Curves', fontsize=13, fontweight='bold')\n",
        "\n",
        "    # ── LP convergence ────────────────────────────────────────────────────────\n",
        "    ax    = axes[0]\n",
        "    trace = res['LP']['lp_trace']\n",
        "    ax.semilogy(range(1, len(trace) + 1), trace, color='steelblue', lw=2)\n",
        "    ax.set_xlabel('Iteration')\n",
        "    ax.set_ylabel('Max Label Change (log scale)')\n",
        "    ax.set_title('Label Propagation — Convergence')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ── Node2Vec skip-gram loss ───────────────────────────────────────────────\n",
        "    ax     = axes[1]\n",
        "    losses = np.array(res['Node2Vec']['history'])   # (n_runs, n_epochs)\n",
        "    mean_l = losses.mean(0)\n",
        "    std_l  = losses.std(0)\n",
        "    ep     = range(1, len(mean_l) + 1)\n",
        "    ax.plot(ep, mean_l, color='darkorange', lw=2, label='train loss (mean)')\n",
        "    ax.fill_between(ep, mean_l - std_l, mean_l + std_l, alpha=0.25, color='darkorange',\n",
        "                    label='±1 std')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Skip-gram Loss')\n",
        "    ax.set_title('Node2Vec — Training Loss')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # ── GCN train + val loss ──────────────────────────────────────────────────\n",
        "    ax  = axes[2]\n",
        "    tl  = np.array([h['train_loss'] for h in res['GCN']['history']])\n",
        "    vl  = np.array([h['val_loss']   for h in res['GCN']['history']])\n",
        "    ep  = range(1, tl.shape[1] + 1)\n",
        "    ax.plot(ep, tl.mean(0), color='forestgreen', lw=2, label='train loss')\n",
        "    ax.fill_between(ep, tl.mean(0) - tl.std(0), tl.mean(0) + tl.std(0),\n",
        "                    alpha=0.2, color='forestgreen')\n",
        "    ax.plot(ep, vl.mean(0), color='crimson', lw=2, label='val loss')\n",
        "    ax.fill_between(ep, vl.mean(0) - vl.std(0), vl.mean(0) + vl.std(0),\n",
        "                    alpha=0.2, color='crimson')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel(y_label_gcn)\n",
        "    ax.set_title('GCN — Train / Val Loss')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_curves_{ds_name}.pdf', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed']:\n",
        "    plot_training_curves(ds_name, results[ds_name])\n",
        "\n",
        "plot_training_curves('ogbn-arxiv', results['ogbn-arxiv'])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a1000011"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Per-Dataset Result Tables ─────────────────────────────────────────────────\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    metric = 'Accuracy'\n",
        "    rows   = []\n",
        "    for run in range(N_RUNS):\n",
        "        row = {'Run': run + 1}\n",
        "        for algo in ['LP', 'Node2Vec', 'GCN']:\n",
        "            row[algo] = f\"{results[ds_name][algo]['test'][run]:.4f}\"\n",
        "        rows.append(row)\n",
        "\n",
        "    df_per = pd.DataFrame(rows).set_index('Run')\n",
        "    # Append mean and std rows\n",
        "    mean_row = {algo: f\"{np.mean(results[ds_name][algo]['test']):.4f}\"\n",
        "                for algo in ['LP', 'Node2Vec', 'GCN']}\n",
        "    std_row  = {algo: f\"{np.std(results[ds_name][algo]['test']):.4f}\"\n",
        "                for algo in ['LP', 'Node2Vec', 'GCN']}\n",
        "    df_per.loc['mean'] = mean_row\n",
        "    df_per.loc['std']  = std_row\n",
        "\n",
        "    print(f\"\\n{'─'*55}\")\n",
        "    print(f\" {ds_name.upper()}  |  Test {metric} per run\")\n",
        "    print(f\"{'─'*55}\")\n",
        "    print(df_per.to_string())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a1000012"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Summary Table (all datasets × all algorithms) ─────────────────────────────\n",
        "\n",
        "summary_rows = []\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    for algo in ['LP', 'Node2Vec', 'GCN']:\n",
        "        vals = results[ds_name][algo]['test']\n",
        "        summary_rows.append({\n",
        "            'Dataset': ds_name, 'Algorithm': algo, 'Metric': 'Accuracy',\n",
        "            'Mean': np.mean(vals), 'Std': np.std(vals),\n",
        "            'Result': f\"{np.mean(vals):.4f} ± {np.std(vals):.4f}\"\n",
        "        })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows)\n",
        "pivot = df_summary.pivot_table(values='Result', index='Dataset',\n",
        "                                columns='Algorithm', aggfunc='first')[['LP', 'Node2Vec', 'GCN']]\n",
        "pivot.index.name = 'Dataset'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Q1 — Node Classification: Test Accuracy (mean ± std, 10 runs)\")\n",
        "print(\"=\"*70)\n",
        "print(pivot.to_string())\n",
        "\n",
        "# ── Grouped Bar Plot ──────────────────────────────────────────────────────────\n",
        "ds_list = ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']\n",
        "titles  = ['Cora\\n(Accuracy)', 'CiteSeer\\n(Accuracy)',\n",
        "           'PubMed\\n(Accuracy)', 'ogbn-arxiv\\n(Accuracy)']\n",
        "algos   = ['LP', 'Node2Vec', 'GCN']\n",
        "colors  = ['steelblue', 'darkorange', 'forestgreen']\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(17, 5), sharey=False)\n",
        "for ax, ds_name, title in zip(axes, ds_list, titles):\n",
        "    means = [np.mean(results[ds_name][a]['test']) for a in algos]\n",
        "    stds  = [np.std(results[ds_name][a]['test'])  for a in algos]\n",
        "    bars  = ax.bar(algos, means, yerr=stds, capsize=7,\n",
        "                   color=colors, alpha=0.82, edgecolor='black', lw=0.7)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_ylim(0, min(1.12, max(means) + max(stds) + 0.15))\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    for bar, m, s in zip(bars, means, stds):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2,\n",
        "                m + s + 0.01, f'{m:.3f}',\n",
        "                ha='center', va='bottom', fontsize=8.5)\n",
        "\n",
        "fig.suptitle('Q1 — Node Classification: Algorithm Comparison across Datasets',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('q1_comparison_barplot.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a1000013"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q2 — Link Prediction [50%]\n",
        "\n",
        "Three algorithms compared on Cora, CiteSeer, PubMed, and ogbn-arxiv. Metric: **AUC**.\n",
        "\n",
        "| Algorithm | Features used | Notes |\n",
        "|-----------|--------------|-------|\n",
        "| **Adamic-Adar** | None (topology only) | Non-parametric baseline — deterministic given the split |\n",
        "| **Node2Vec** | None (structure only) | Dot-product scoring on learned embeddings |\n",
        "| **GCN** | Node features (required) | GCN encoder + dot-product decoder, trained with BCE |\n",
        "\n",
        "**Edge splits**: For each of the 10 runs, **20% of edges** are dropped as the test set, 10% as validation, 70% for training.  \n",
        "**Negative sampling**: 1:1 ratio (equal number of non-edges as positive edges in each split). The **same split and negatives** are used across all three algorithms within each run, ensuring a fair comparison.  \n",
        "**Results reported**: AUC mean ± std over 10 independent runs."
      ],
      "id": "5ffda340"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Q2 Setup: Edge Splitting, Dataset Prep, Algorithm Implementations ─────────\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "# Prepare undirected copies of each dataset for link prediction\n",
        "lp_data = {}\n",
        "for name in ['cora', 'citeseer', 'pubmed']:\n",
        "    lp_data[name] = planetoid_datasets[name][0].clone()\n",
        "\n",
        "arxiv_lp = data_arxiv.clone()\n",
        "arxiv_lp.edge_index = to_undirected(data_arxiv.edge_index)\n",
        "lp_data['ogbn-arxiv'] = arxiv_lp\n",
        "\n",
        "for name, d in lp_data.items():\n",
        "    print(f\"{name:12s}: {d.num_nodes:>7,} nodes  {d.edge_index.size(1):>10,} edges\")\n",
        "\n",
        "\n",
        "def create_link_splits(data, val_ratio=0.1, test_ratio=0.2, seed=0):\n",
        "    \"\"\"20% test, 10% val, 70% train edges. Negative sampling 1:1.\"\"\"\n",
        "    set_seed(seed)\n",
        "    transform = RandomLinkSplit(\n",
        "        num_val=val_ratio,\n",
        "        num_test=test_ratio,\n",
        "        is_undirected=True,\n",
        "        add_negative_train_samples=True,\n",
        "        neg_sampling_ratio=1.0,\n",
        "    )\n",
        "    return transform(data.clone())\n",
        "\n",
        "\n",
        "# ── Algorithm 1: Adamic-Adar (non-parametric, topology only) ─────────────────\n",
        "\n",
        "def aa_scores(adj, src, dst):\n",
        "    \"\"\"Vectorized Adamic-Adar index for given (src, dst) pairs.\"\"\"\n",
        "    inv_log_deg = 1.0 / np.log(np.array(adj.sum(1)).flatten().clip(min=2))\n",
        "    A_scaled = adj @ diags(inv_log_deg)\n",
        "    chunk = 50_000\n",
        "    scores = np.zeros(len(src))\n",
        "    for i in range(0, len(src), chunk):\n",
        "        j = min(i + chunk, len(src))\n",
        "        scores[i:j] = np.array(\n",
        "            A_scaled[src[i:j]].multiply(adj[dst[i:j]]).sum(1)\n",
        "        ).flatten()\n",
        "    return scores\n",
        "\n",
        "\n",
        "def run_aa(train_data, val_data, test_data):\n",
        "    \"\"\"Adamic-Adar link prediction. Returns (test_auc, val_auc).\"\"\"\n",
        "    ei = train_data.edge_index.numpy()\n",
        "    n = train_data.num_nodes\n",
        "    adj = csr_matrix((np.ones(ei.shape[1]), (ei[0], ei[1])), shape=(n, n))\n",
        "    adj.data[:] = 1.0\n",
        "\n",
        "    te = test_data.edge_label_index.numpy()\n",
        "    test_auc = roc_auc_score(test_data.edge_label.numpy(),\n",
        "                              aa_scores(adj, te[0], te[1]))\n",
        "    ve = val_data.edge_label_index.numpy()\n",
        "    val_auc = roc_auc_score(val_data.edge_label.numpy(),\n",
        "                             aa_scores(adj, ve[0], ve[1]))\n",
        "    return test_auc, val_auc\n",
        "\n",
        "\n",
        "# ── Algorithm 2: Node2Vec + dot-product (structure only) ─────────────────────\n",
        "\n",
        "def run_n2v_lp(train_data, val_data, test_data,\n",
        "               n_epochs=50, walks_per_node=10, seed=0):\n",
        "    \"\"\"Node2Vec link prediction via dot-product scoring.\n",
        "    Returns (test_auc, val_auc, epoch_losses).\"\"\"\n",
        "    z, losses = train_node2vec_model(\n",
        "        train_data.edge_index, train_data.num_nodes,\n",
        "        embedding_dim=128, walk_length=20, context_size=10,\n",
        "        walks_per_node=walks_per_node, n_epochs=n_epochs, lr=0.01,\n",
        "        batch_size=512, seed=seed\n",
        "    )\n",
        "\n",
        "    def score(eli):\n",
        "        s, d = eli[0].numpy(), eli[1].numpy()\n",
        "        return (z[s] * z[d]).sum(1)\n",
        "\n",
        "    test_auc = roc_auc_score(test_data.edge_label.numpy(),\n",
        "                              score(test_data.edge_label_index))\n",
        "    val_auc = roc_auc_score(val_data.edge_label.numpy(),\n",
        "                             score(val_data.edge_label_index))\n",
        "    return test_auc, val_auc, losses\n",
        "\n",
        "\n",
        "# ── Algorithm 3: GCN encoder + dot-product decoder (uses node features) ──────\n",
        "\n",
        "class GCNLinkPred(torch.nn.Module):\n",
        "    \"\"\"2-layer GCN encoder + dot-product decoder for link prediction.\"\"\"\n",
        "    def __init__(self, in_ch, hidden=128, out_ch=64, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_ch, hidden)\n",
        "        self.conv2 = GCNConv(hidden, out_ch)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_label_index):\n",
        "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(-1)\n",
        "\n",
        "\n",
        "def run_gcn_lp(train_data, val_data, test_data,\n",
        "               n_epochs=200, lr=0.01, seed=0):\n",
        "    \"\"\"GCN link prediction. Returns (test_auc, val_auc, hist).\"\"\"\n",
        "    set_seed(seed)\n",
        "    dev = GCN_DEVICE\n",
        "    tr = train_data.to(dev)\n",
        "    tr_eli = tr.edge_label_index\n",
        "    tr_el = tr.edge_label.float()\n",
        "    val_eli = val_data.edge_label_index.to(dev)\n",
        "    val_el_np = val_data.edge_label.numpy()\n",
        "    val_el_dev = val_data.edge_label.float().to(dev)\n",
        "\n",
        "    model = GCNLinkPred(train_data.x.size(1)).to(dev)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    crit = torch.nn.BCEWithLogitsLoss()\n",
        "    hist = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "        z = model.encode(tr.x, tr.edge_index)\n",
        "        loss = crit(model.decode(z, tr_eli), tr_el)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            z = model.encode(tr.x, tr.edge_index)\n",
        "            v_out = model.decode(z, val_eli)\n",
        "            v_loss = crit(v_out, val_el_dev).item()\n",
        "            v_auc = roc_auc_score(val_el_np, v_out.sigmoid().cpu().numpy())\n",
        "        hist['train_loss'].append(loss.item())\n",
        "        hist['val_loss'].append(v_loss)\n",
        "        hist['val_auc'].append(v_auc)\n",
        "\n",
        "    model.eval()\n",
        "    test_eli = test_data.edge_label_index.to(dev)\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(tr.x, tr.edge_index)\n",
        "        t_out = model.decode(z, test_eli)\n",
        "        t_auc = roc_auc_score(test_data.edge_label.numpy(),\n",
        "                               t_out.sigmoid().cpu().numpy())\n",
        "    tr.to(CPU)\n",
        "    return t_auc, hist['val_auc'][-1], hist\n",
        "\n",
        "print(\"Q2 utilities and algorithms defined.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a0417dd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Q2: Run All Link Prediction Experiments ───────────────────────────────────\n",
        "\n",
        "lp_results = {}\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    data = lp_data[ds_name]\n",
        "    is_large = (ds_name == 'ogbn-arxiv')\n",
        "    n2v_epochs = 10 if is_large else 50\n",
        "    n2v_wpn = 5 if is_large else 10\n",
        "    gcn_epochs = 200\n",
        "\n",
        "    print(f\"\\n{'='*60}\\n{ds_name.upper()} — Link Prediction\\n{'='*60}\")\n",
        "\n",
        "    aa_t, aa_v = [], []\n",
        "    n2v_t, n2v_v, n2v_h = [], [], []\n",
        "    gcn_t, gcn_v, gcn_h = [], [], []\n",
        "\n",
        "    for run in range(N_RUNS):\n",
        "        train_d, val_d, test_d = create_link_splits(data, seed=run)\n",
        "\n",
        "        # Adamic-Adar\n",
        "        ta, va = run_aa(train_d, val_d, test_d)\n",
        "        aa_t.append(ta); aa_v.append(va)\n",
        "\n",
        "        # Node2Vec\n",
        "        ta, va, losses = run_n2v_lp(train_d, val_d, test_d,\n",
        "                                     n_epochs=n2v_epochs,\n",
        "                                     walks_per_node=n2v_wpn, seed=run * 7)\n",
        "        n2v_t.append(ta); n2v_v.append(va); n2v_h.append(losses)\n",
        "\n",
        "        # GCN\n",
        "        ta, va, hist = run_gcn_lp(train_d, val_d, test_d,\n",
        "                                   n_epochs=gcn_epochs, seed=run * 13)\n",
        "        gcn_t.append(ta); gcn_v.append(va); gcn_h.append(hist)\n",
        "\n",
        "        print(f\"  Run {run+1:02d}/{N_RUNS}: \"\n",
        "              f\"AA={aa_t[-1]:.4f}  N2V={n2v_t[-1]:.4f}  GCN={gcn_t[-1]:.4f}\")\n",
        "\n",
        "    lp_results[ds_name] = {\n",
        "        'AA':       {'test': aa_t, 'val': aa_v},\n",
        "        'Node2Vec': {'test': n2v_t, 'val': n2v_v, 'history': n2v_h},\n",
        "        'GCN':      {'test': gcn_t, 'val': gcn_v, 'history': gcn_h},\n",
        "    }\n",
        "\n",
        "    print(f\"\\n  Summary:\")\n",
        "    for algo in ['AA', 'Node2Vec', 'GCN']:\n",
        "        t = lp_results[ds_name][algo]['test']\n",
        "        print(f\"    {algo:8s} AUC: {np.mean(t):.4f} ± {np.std(t):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a15c86fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Q2: Training Curves ──────────────────────────────────────────────────────\n",
        "# Two panels per dataset:\n",
        "#   Left  : Node2Vec skip-gram loss (mean ± std across runs)\n",
        "#   Right : GCN train BCE loss + val BCE loss (mean ± std across runs)\n",
        "# Adamic-Adar is non-parametric (no training), analogous to LP in Q1.\n",
        "\n",
        "def plot_lp_training_curves(ds_name, res):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    fig.suptitle(f'{ds_name.upper()} — Link Prediction Training Curves',\n",
        "                 fontsize=13, fontweight='bold')\n",
        "\n",
        "    # Node2Vec skip-gram loss\n",
        "    ax = axes[0]\n",
        "    losses = np.array(res['Node2Vec']['history'])\n",
        "    m, s = losses.mean(0), losses.std(0)\n",
        "    ep = range(1, len(m) + 1)\n",
        "    ax.plot(ep, m, color='darkorange', lw=2, label='train loss (mean)')\n",
        "    ax.fill_between(ep, m - s, m + s, alpha=0.25, color='darkorange',\n",
        "                    label='±1 std')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Skip-gram Loss')\n",
        "    ax.set_title('Node2Vec — Training Loss')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # GCN train + val loss\n",
        "    ax = axes[1]\n",
        "    tl = np.array([h['train_loss'] for h in res['GCN']['history']])\n",
        "    vl = np.array([h['val_loss'] for h in res['GCN']['history']])\n",
        "    ep = range(1, tl.shape[1] + 1)\n",
        "    ax.plot(ep, tl.mean(0), color='forestgreen', lw=2, label='train loss')\n",
        "    ax.fill_between(ep, tl.mean(0) - tl.std(0), tl.mean(0) + tl.std(0),\n",
        "                    alpha=0.2, color='forestgreen')\n",
        "    ax.plot(ep, vl.mean(0), color='crimson', lw=2, label='val loss')\n",
        "    ax.fill_between(ep, vl.mean(0) - vl.std(0), vl.mean(0) + vl.std(0),\n",
        "                    alpha=0.2, color='crimson')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('BCE Loss')\n",
        "    ax.set_title('GCN — Train / Val Loss')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'lp_training_curves_{ds_name}.pdf', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    plot_lp_training_curves(ds_name, lp_results[ds_name])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "136c62ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Q2: Per-Dataset Result Tables ─────────────────────────────────────────────\n",
        "\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    rows = []\n",
        "    for run in range(N_RUNS):\n",
        "        row = {'Run': run + 1}\n",
        "        for algo in ['AA', 'Node2Vec', 'GCN']:\n",
        "            row[algo] = f\"{lp_results[ds_name][algo]['test'][run]:.4f}\"\n",
        "        rows.append(row)\n",
        "\n",
        "    df_per = pd.DataFrame(rows).set_index('Run')\n",
        "    mean_row = {a: f\"{np.mean(lp_results[ds_name][a]['test']):.4f}\"\n",
        "                for a in ['AA', 'Node2Vec', 'GCN']}\n",
        "    std_row  = {a: f\"{np.std(lp_results[ds_name][a]['test']):.4f}\"\n",
        "                for a in ['AA', 'Node2Vec', 'GCN']}\n",
        "    df_per.loc['mean'] = mean_row\n",
        "    df_per.loc['std']  = std_row\n",
        "\n",
        "    print(f\"\\n{'─'*55}\")\n",
        "    print(f\" {ds_name.upper()}  |  Link Prediction AUC per run\")\n",
        "    print(f\"{'─'*55}\")\n",
        "    print(df_per.to_string())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9da49a64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Q2: Summary Table + Grouped Bar Plot ─────────────────────────────────────\n",
        "\n",
        "summary_lp = []\n",
        "for ds_name in ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']:\n",
        "    for algo in ['AA', 'Node2Vec', 'GCN']:\n",
        "        vals = lp_results[ds_name][algo]['test']\n",
        "        summary_lp.append({\n",
        "            'Dataset': ds_name, 'Algorithm': algo,\n",
        "            'Mean': np.mean(vals), 'Std': np.std(vals),\n",
        "            'Result': f\"{np.mean(vals):.4f} ± {np.std(vals):.4f}\"\n",
        "        })\n",
        "\n",
        "df_lp = pd.DataFrame(summary_lp)\n",
        "pivot_lp = df_lp.pivot_table(values='Result', index='Dataset',\n",
        "                              columns='Algorithm', aggfunc='first')[['AA', 'Node2Vec', 'GCN']]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Q2 — Link Prediction: Test AUC (mean ± std, 10 runs)\")\n",
        "print(\"=\" * 70)\n",
        "print(pivot_lp.to_string())\n",
        "\n",
        "# Grouped bar plot\n",
        "ds_list = ['cora', 'citeseer', 'pubmed', 'ogbn-arxiv']\n",
        "titles  = ['Cora\\n(AUC)', 'CiteSeer\\n(AUC)',\n",
        "           'PubMed\\n(AUC)', 'ogbn-arxiv\\n(AUC)']\n",
        "algos   = ['AA', 'Node2Vec', 'GCN']\n",
        "colors  = ['steelblue', 'darkorange', 'forestgreen']\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(17, 5), sharey=False)\n",
        "for ax, ds_name, title in zip(axes, ds_list, titles):\n",
        "    means = [np.mean(lp_results[ds_name][a]['test']) for a in algos]\n",
        "    stds  = [np.std(lp_results[ds_name][a]['test'])  for a in algos]\n",
        "    bars  = ax.bar(algos, means, yerr=stds, capsize=7,\n",
        "                   color=colors, alpha=0.82, edgecolor='black', lw=0.7)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_ylim(0, min(1.12, max(means) + max(stds) + 0.15))\n",
        "    ax.set_ylabel('AUC')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    for bar, m, s in zip(bars, means, stds):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2,\n",
        "                m + s + 0.01, f'{m:.3f}',\n",
        "                ha='center', va='bottom', fontsize=8.5)\n",
        "\n",
        "fig.suptitle('Q2 — Link Prediction: Algorithm Comparison across Datasets',\n",
        "             fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('q2_comparison_barplot.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "id": "58563b5d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}